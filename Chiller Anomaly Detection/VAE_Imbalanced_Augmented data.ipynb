{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9c0f6ff-b272-444b-9690-3af3285e2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-24 21:03:13,069] A new study created in memory with name: no-name-2523d367-af9e-4ca2-b937-112caa63a43a\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_11230/3685452488.py:81: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:14,345] Trial 0 finished with value: 0.8475634455680847 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 32, 'latent_dim': 8, 'learning_rate': 0.009513095938512645, 'batch_size': 48}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:15,471] Trial 1 finished with value: 0.9280211329460144 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 16, 'latent_dim': 6, 'learning_rate': 0.0002790749916953631, 'batch_size': 64}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:16,548] Trial 2 finished with value: 0.9541277885437012 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 48, 'latent_dim': 8, 'learning_rate': 0.0003241322951655266, 'batch_size': 48}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:17,661] Trial 3 finished with value: 0.8914218544960022 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 16, 'latent_dim': 4, 'learning_rate': 0.0002687108283430115, 'batch_size': 64}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:18,737] Trial 4 finished with value: 0.9642489552497864 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 32, 'latent_dim': 16, 'learning_rate': 0.0008906287879057009, 'batch_size': 48}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:19,942] Trial 5 finished with value: 0.9378938674926758 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 48, 'latent_dim': 2, 'learning_rate': 0.0014956452912085268, 'batch_size': 32}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:21,189] Trial 6 finished with value: 0.9777410626411438 and parameters: {'hidden_dim1': 96, 'hidden_dim2': 32, 'latent_dim': 16, 'learning_rate': 0.00037880683845452096, 'batch_size': 32}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:22,314] Trial 7 finished with value: 0.9107281565666199 and parameters: {'hidden_dim1': 96, 'hidden_dim2': 16, 'latent_dim': 2, 'learning_rate': 0.0006639352300006742, 'batch_size': 64}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:23,518] Trial 8 finished with value: 0.9294431805610657 and parameters: {'hidden_dim1': 48, 'hidden_dim2': 48, 'latent_dim': 16, 'learning_rate': 0.005700383990988637, 'batch_size': 16}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:24,583] Trial 9 finished with value: 0.9152917265892029 and parameters: {'hidden_dim1': 48, 'hidden_dim2': 32, 'latent_dim': 10, 'learning_rate': 0.0010767972694103462, 'batch_size': 48}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:25,797] Trial 10 finished with value: 0.8666830658912659 and parameters: {'hidden_dim1': 64, 'hidden_dim2': 64, 'latent_dim': 12, 'learning_rate': 0.00917271155132068, 'batch_size': 16}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:27,024] Trial 11 finished with value: 0.9040239453315735 and parameters: {'hidden_dim1': 64, 'hidden_dim2': 64, 'latent_dim': 12, 'learning_rate': 0.009605090840727082, 'batch_size': 16}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:28,245] Trial 12 finished with value: 0.9261019229888916 and parameters: {'hidden_dim1': 80, 'hidden_dim2': 64, 'latent_dim': 12, 'learning_rate': 0.003726421763039083, 'batch_size': 32}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:29,470] Trial 13 finished with value: 0.8964600563049316 and parameters: {'hidden_dim1': 32, 'hidden_dim2': 64, 'latent_dim': 12, 'learning_rate': 0.0028233589385422853, 'batch_size': 16}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:30,703] Trial 14 finished with value: 0.9660286903381348 and parameters: {'hidden_dim1': 80, 'hidden_dim2': 48, 'latent_dim': 8, 'learning_rate': 0.009590070383524489, 'batch_size': 32}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:31,809] Trial 15 finished with value: 1.2135159969329834 and parameters: {'hidden_dim1': 64, 'hidden_dim2': 32, 'latent_dim': 10, 'learning_rate': 0.00011077491281377927, 'batch_size': 48}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:33,065] Trial 16 finished with value: 0.899310827255249 and parameters: {'hidden_dim1': 64, 'hidden_dim2': 64, 'latent_dim': 6, 'learning_rate': 0.0023003518316561185, 'batch_size': 16}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:34,290] Trial 17 finished with value: 0.9096018671989441 and parameters: {'hidden_dim1': 96, 'hidden_dim2': 48, 'latent_dim': 14, 'learning_rate': 0.005122241053530249, 'batch_size': 32}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:35,371] Trial 18 finished with value: 0.938765823841095 and parameters: {'hidden_dim1': 32, 'hidden_dim2': 32, 'latent_dim': 6, 'learning_rate': 0.006022035714942313, 'batch_size': 48}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:36,475] Trial 19 finished with value: 0.9771959185600281 and parameters: {'hidden_dim1': 48, 'hidden_dim2': 16, 'latent_dim': 14, 'learning_rate': 0.0020062541382008547, 'batch_size': 64}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:37,759] Trial 20 finished with value: 0.869439423084259 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 64, 'latent_dim': 10, 'learning_rate': 0.007728401792246964, 'batch_size': 16}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:39,002] Trial 21 finished with value: 0.9520604014396667 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 64, 'latent_dim': 10, 'learning_rate': 0.009540528575515675, 'batch_size': 16}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:41,180] Trial 22 finished with value: 0.917900562286377 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 64, 'latent_dim': 8, 'learning_rate': 0.004113456473687175, 'batch_size': 16}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:42,554] Trial 23 finished with value: 0.9421461224555969 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 48, 'latent_dim': 12, 'learning_rate': 0.00686314371783973, 'batch_size': 32}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:43,822] Trial 24 finished with value: 0.9298824667930603 and parameters: {'hidden_dim1': 96, 'hidden_dim2': 64, 'latent_dim': 10, 'learning_rate': 0.0035507145805663224, 'batch_size': 16}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:45,033] Trial 25 finished with value: 0.8961908221244812 and parameters: {'hidden_dim1': 80, 'hidden_dim2': 48, 'latent_dim': 14, 'learning_rate': 0.007426732833522721, 'batch_size': 32}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:46,171] Trial 26 finished with value: 0.9955671429634094 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 32, 'latent_dim': 8, 'learning_rate': 0.0044502710567751414, 'batch_size': 48}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:47,416] Trial 27 finished with value: 0.8741448521614075 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 4, 'learning_rate': 0.007330858742921815, 'batch_size': 16}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:48,750] Trial 28 finished with value: 0.9245042204856873 and parameters: {'hidden_dim1': 80, 'hidden_dim2': 48, 'latent_dim': 10, 'learning_rate': 0.0024894457394702527, 'batch_size': 32}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:49,927] Trial 29 finished with value: 0.9538351893424988 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 16, 'latent_dim': 6, 'learning_rate': 0.0032571402767238213, 'batch_size': 64}. Best is trial 0 with value: 0.8475634455680847.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:51,233] Trial 30 finished with value: 0.8411245346069336 and parameters: {'hidden_dim1': 64, 'hidden_dim2': 64, 'latent_dim': 12, 'learning_rate': 0.0015985606288019547, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:52,465] Trial 31 finished with value: 0.9532699584960938 and parameters: {'hidden_dim1': 64, 'hidden_dim2': 64, 'latent_dim': 12, 'learning_rate': 0.0017084624514171547, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:53,715] Trial 32 finished with value: 0.9859251976013184 and parameters: {'hidden_dim1': 48, 'hidden_dim2': 64, 'latent_dim': 14, 'learning_rate': 0.0005374114325734028, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:54,957] Trial 33 finished with value: 0.9722962975502014 and parameters: {'hidden_dim1': 64, 'hidden_dim2': 64, 'latent_dim': 8, 'learning_rate': 0.00012547948825630868, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:56,237] Trial 34 finished with value: 0.9451231360435486 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 10, 'learning_rate': 0.0077143368942379775, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:57,366] Trial 35 finished with value: 0.8983704447746277 and parameters: {'hidden_dim1': 64, 'hidden_dim2': 48, 'latent_dim': 12, 'learning_rate': 0.004933815206880421, 'batch_size': 48}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:58,650] Trial 36 finished with value: 0.9286222457885742 and parameters: {'hidden_dim1': 80, 'hidden_dim2': 32, 'latent_dim': 4, 'learning_rate': 0.001372821670684482, 'batch_size': 32}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:03:59,741] Trial 37 finished with value: 0.905684232711792 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 48, 'latent_dim': 8, 'learning_rate': 0.0011349268717698996, 'batch_size': 48}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:01,106] Trial 38 finished with value: 0.9826580882072449 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 14, 'learning_rate': 0.000263361747049675, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:02,256] Trial 39 finished with value: 1.0391860008239746 and parameters: {'hidden_dim1': 48, 'hidden_dim2': 32, 'latent_dim': 10, 'learning_rate': 0.00019738095500504242, 'batch_size': 48}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:03,496] Trial 40 finished with value: 0.9554392695426941 and parameters: {'hidden_dim1': 96, 'hidden_dim2': 16, 'latent_dim': 12, 'learning_rate': 0.0006508123463943098, 'batch_size': 32}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:04,773] Trial 41 finished with value: 0.9234738349914551 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 4, 'learning_rate': 0.0070418720412929415, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:06,006] Trial 42 finished with value: 0.9248924851417542 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 2, 'learning_rate': 0.008309430452178263, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:07,269] Trial 43 finished with value: 0.9078928828239441 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 64, 'latent_dim': 4, 'learning_rate': 0.005836381580742628, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:08,499] Trial 44 finished with value: 0.8923112750053406 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 6, 'learning_rate': 0.005982464050532864, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:09,710] Trial 45 finished with value: 0.9251389503479004 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 48, 'latent_dim': 2, 'learning_rate': 0.008836650183120635, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:10,882] Trial 46 finished with value: 0.9396519064903259 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 16, 'learning_rate': 0.004171481849838242, 'batch_size': 64}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:12,138] Trial 47 finished with value: 0.8818455338478088 and parameters: {'hidden_dim1': 64, 'hidden_dim2': 64, 'latent_dim': 12, 'learning_rate': 0.009852312047667962, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:13,367] Trial 48 finished with value: 0.8721374869346619 and parameters: {'hidden_dim1': 80, 'hidden_dim2': 64, 'latent_dim': 10, 'learning_rate': 0.0032457078291648522, 'batch_size': 32}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:14,644] Trial 49 finished with value: 0.9718943238258362 and parameters: {'hidden_dim1': 80, 'hidden_dim2': 48, 'latent_dim': 10, 'learning_rate': 0.002882098471112574, 'batch_size': 32}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:15,857] Trial 50 finished with value: 0.9448909163475037 and parameters: {'hidden_dim1': 96, 'hidden_dim2': 32, 'latent_dim': 8, 'learning_rate': 0.000838132221160709, 'batch_size': 32}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:17,114] Trial 51 finished with value: 0.9201757311820984 and parameters: {'hidden_dim1': 64, 'hidden_dim2': 64, 'latent_dim': 10, 'learning_rate': 0.006531872012407273, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:18,205] Trial 52 finished with value: 0.9645711779594421 and parameters: {'hidden_dim1': 80, 'hidden_dim2': 64, 'latent_dim': 12, 'learning_rate': 0.005441865530703682, 'batch_size': 48}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:20,167] Trial 53 finished with value: 0.9174081683158875 and parameters: {'hidden_dim1': 48, 'hidden_dim2': 64, 'latent_dim': 10, 'learning_rate': 0.0018941763091620844, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:21,397] Trial 54 finished with value: 0.8617334365844727 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 64, 'latent_dim': 6, 'learning_rate': 0.00818116930604764, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:22,612] Trial 55 finished with value: 0.9296183586120605 and parameters: {'hidden_dim1': 96, 'hidden_dim2': 64, 'latent_dim': 8, 'learning_rate': 0.004932892571492159, 'batch_size': 32}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:23,824] Trial 56 finished with value: 0.9211726188659668 and parameters: {'hidden_dim1': 80, 'hidden_dim2': 64, 'latent_dim': 6, 'learning_rate': 0.0013572623329783165, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:24,955] Trial 57 finished with value: 0.9180544018745422 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 32, 'latent_dim': 6, 'learning_rate': 0.00835381287874705, 'batch_size': 48}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:26,190] Trial 58 finished with value: 0.9026709198951721 and parameters: {'hidden_dim1': 64, 'hidden_dim2': 64, 'latent_dim': 8, 'learning_rate': 0.009851958974693644, 'batch_size': 16}. Best is trial 30 with value: 0.8411245346069336.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:27,520] Trial 59 finished with value: 0.8298470973968506 and parameters: {'hidden_dim1': 96, 'hidden_dim2': 48, 'latent_dim': 10, 'learning_rate': 0.003191455810504189, 'batch_size': 32}. Best is trial 59 with value: 0.8298470973968506.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:28,831] Trial 60 finished with value: 0.9467089176177979 and parameters: {'hidden_dim1': 96, 'hidden_dim2': 48, 'latent_dim': 12, 'learning_rate': 0.0023346766448881437, 'batch_size': 32}. Best is trial 59 with value: 0.8298470973968506.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:30,109] Trial 61 finished with value: 0.9033377170562744 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 64, 'latent_dim': 10, 'learning_rate': 0.0033011533378483086, 'batch_size': 32}. Best is trial 59 with value: 0.8298470973968506.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:31,316] Trial 62 finished with value: 0.9293006062507629 and parameters: {'hidden_dim1': 96, 'hidden_dim2': 32, 'latent_dim': 10, 'learning_rate': 0.0039341427054806885, 'batch_size': 32}. Best is trial 59 with value: 0.8298470973968506.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:32,567] Trial 63 finished with value: 0.9418466687202454 and parameters: {'hidden_dim1': 96, 'hidden_dim2': 48, 'latent_dim': 14, 'learning_rate': 0.0028649490064213638, 'batch_size': 16}. Best is trial 59 with value: 0.8298470973968506.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:33,687] Trial 64 finished with value: 0.9478381276130676 and parameters: {'hidden_dim1': 80, 'hidden_dim2': 64, 'latent_dim': 12, 'learning_rate': 0.006541635441141058, 'batch_size': 48}. Best is trial 59 with value: 0.8298470973968506.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:34,885] Trial 65 finished with value: 0.8928521275520325 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 64, 'latent_dim': 8, 'learning_rate': 0.004636942189038635, 'batch_size': 32}. Best is trial 59 with value: 0.8298470973968506.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:36,108] Trial 66 finished with value: 0.9338319897651672 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 64, 'latent_dim': 10, 'learning_rate': 0.008022771327283637, 'batch_size': 16}. Best is trial 59 with value: 0.8298470973968506.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:37,191] Trial 67 finished with value: 0.9344008564949036 and parameters: {'hidden_dim1': 64, 'hidden_dim2': 64, 'latent_dim': 12, 'learning_rate': 0.0015782391105962628, 'batch_size': 48}. Best is trial 59 with value: 0.8298470973968506.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:38,450] Trial 68 finished with value: 0.9204861521720886 and parameters: {'hidden_dim1': 32, 'hidden_dim2': 48, 'latent_dim': 8, 'learning_rate': 0.005669738838825402, 'batch_size': 16}. Best is trial 59 with value: 0.8298470973968506.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:39,685] Trial 69 finished with value: 0.9588420391082764 and parameters: {'hidden_dim1': 96, 'hidden_dim2': 16, 'latent_dim': 10, 'learning_rate': 0.002096940573161704, 'batch_size': 32}. Best is trial 59 with value: 0.8298470973968506.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:40,924] Trial 70 finished with value: 0.9830877184867859 and parameters: {'hidden_dim1': 80, 'hidden_dim2': 32, 'latent_dim': 12, 'learning_rate': 0.0009383745208636076, 'batch_size': 16}. Best is trial 59 with value: 0.8298470973968506.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:42,142] Trial 71 finished with value: 0.8621976971626282 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 4, 'learning_rate': 0.0074319263632519296, 'batch_size': 16}. Best is trial 59 with value: 0.8298470973968506.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:43,423] Trial 72 finished with value: 0.8677411079406738 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 6, 'learning_rate': 0.007094536195361194, 'batch_size': 16}. Best is trial 59 with value: 0.8298470973968506.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:44,632] Trial 73 finished with value: 0.8127644658088684 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 6, 'learning_rate': 0.007177297073306272, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:45,837] Trial 74 finished with value: 0.9330582022666931 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 6, 'learning_rate': 0.006536375314340659, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:47,024] Trial 75 finished with value: 0.9548782706260681 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 4, 'learning_rate': 0.008712644462314794, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:48,276] Trial 76 finished with value: 0.9167041778564453 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 6, 'learning_rate': 0.007246790955023135, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:49,476] Trial 77 finished with value: 0.9249253869056702 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 4, 'learning_rate': 0.005269695671904639, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:50,702] Trial 78 finished with value: 0.8484034538269043 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 48, 'latent_dim': 6, 'learning_rate': 0.008772737368612952, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:51,946] Trial 79 finished with value: 0.9092715382575989 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 48, 'latent_dim': 4, 'learning_rate': 0.008534532784681293, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:53,170] Trial 80 finished with value: 0.9630112648010254 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 48, 'latent_dim': 6, 'learning_rate': 0.009769293886676243, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:54,398] Trial 81 finished with value: 0.8680393099784851 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 48, 'latent_dim': 6, 'learning_rate': 0.007509557306947384, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:55,581] Trial 82 finished with value: 0.8656776547431946 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 32, 'latent_dim': 6, 'learning_rate': 0.006318839864953444, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:56,786] Trial 83 finished with value: 0.9533961415290833 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 32, 'latent_dim': 6, 'learning_rate': 0.006113974931186202, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:58,003] Trial 84 finished with value: 0.9319579601287842 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 32, 'latent_dim': 4, 'learning_rate': 0.008874997806149577, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:04:59,175] Trial 85 finished with value: 0.9347037672996521 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 32, 'latent_dim': 2, 'learning_rate': 0.004699587058948019, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:05:00,984] Trial 86 finished with value: 0.8877308964729309 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 32, 'latent_dim': 6, 'learning_rate': 0.006304586334709041, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:05:02,182] Trial 87 finished with value: 0.9099711775779724 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 32, 'latent_dim': 6, 'learning_rate': 0.007617164773353118, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:05:03,315] Trial 88 finished with value: 0.9352661967277527 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 48, 'latent_dim': 4, 'learning_rate': 0.009297619717850047, 'batch_size': 64}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:05:04,514] Trial 89 finished with value: 0.9033570289611816 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 16, 'latent_dim': 8, 'learning_rate': 0.003712200239762518, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:05:05,685] Trial 90 finished with value: 0.9672207832336426 and parameters: {'hidden_dim1': 48, 'hidden_dim2': 32, 'latent_dim': 8, 'learning_rate': 0.00435935075060404, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:05:06,901] Trial 91 finished with value: 0.904390811920166 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 6, 'learning_rate': 0.006997565200725849, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:05:08,110] Trial 92 finished with value: 0.8969466686248779 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 6, 'learning_rate': 0.005574611144423, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:05:09,326] Trial 93 finished with value: 0.8253582119941711 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 6, 'learning_rate': 0.007990485570330748, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:05:10,553] Trial 94 finished with value: 0.8734512329101562 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 4, 'learning_rate': 0.008267271734548897, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:05:11,795] Trial 95 finished with value: 0.9133874773979187 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 6, 'learning_rate': 0.009956173088215748, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:05:13,039] Trial 96 finished with value: 0.8541809916496277 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 8, 'learning_rate': 0.00516036128283974, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:05:14,269] Trial 97 finished with value: 0.9168420433998108 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 8, 'learning_rate': 0.00518444793290224, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:05:15,405] Trial 98 finished with value: 0.9508237838745117 and parameters: {'hidden_dim1': 128, 'hidden_dim2': 48, 'latent_dim': 8, 'learning_rate': 0.0004803650253261473, 'batch_size': 48}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-24 21:05:16,649] Trial 99 finished with value: 0.9606756567955017 and parameters: {'hidden_dim1': 112, 'hidden_dim2': 64, 'latent_dim': 8, 'learning_rate': 0.006135733915407621, 'batch_size': 16}. Best is trial 73 with value: 0.8127644658088684.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim1': 128, 'hidden_dim2': 64, 'latent_dim': 6, 'learning_rate': 0.007177297073306272, 'batch_size': 16}\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 965us/step - loss: 1.1179\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 845us/step - loss: 0.9880\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 756us/step - loss: 0.9790\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 736us/step - loss: 0.8817\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 750us/step - loss: 0.8688\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 769us/step - loss: 0.9143\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 706us/step - loss: 0.9579\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 802us/step - loss: 0.9362\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 720us/step - loss: 0.9047\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 720us/step - loss: 0.8442\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 733us/step - loss: 0.8673\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 684us/step - loss: 0.8829\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 696us/step - loss: 0.8748\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 712us/step - loss: 0.8502\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 698us/step - loss: 0.9046\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 795us/step - loss: 0.8068\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 821us/step - loss: 0.8874\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 731us/step - loss: 0.8838\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 718us/step - loss: 0.8661\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 781us/step - loss: 0.8879\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 769us/step - loss: 0.9121\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 709us/step - loss: 0.9203\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 710us/step - loss: 0.8323\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 850us/step - loss: 0.8692\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 920us/step - loss: 0.9166\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 872us/step - loss: 0.9269\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 899us/step - loss: 0.8921\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 880us/step - loss: 0.9099\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 938us/step - loss: 0.8437\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 899us/step - loss: 0.8598\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 953us/step - loss: 0.8794\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 910us/step - loss: 0.8340\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 948us/step - loss: 0.8437\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 932us/step - loss: 0.8221\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 907us/step - loss: 0.8338\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 894us/step - loss: 0.8607\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 899us/step - loss: 0.8262\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 916us/step - loss: 0.8683\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 887us/step - loss: 0.8697\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 861us/step - loss: 0.8713\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 945us/step - loss: 0.8709\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 909us/step - loss: 0.9187\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 843us/step - loss: 0.8906\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 867us/step - loss: 0.8857\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 896us/step - loss: 0.8384\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 946us/step - loss: 0.8639\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 956us/step - loss: 0.8489\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 868us/step - loss: 0.8914\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 899us/step - loss: 0.8323\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 920us/step - loss: 0.8556\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 910us/step - loss: 0.8751\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 953us/step - loss: 0.8438\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 942us/step - loss: 0.8560\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 882us/step - loss: 0.8467\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 867us/step - loss: 0.8046\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 877us/step - loss: 0.9129\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 984us/step - loss: 0.8556\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 866us/step - loss: 0.8943\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 949us/step - loss: 0.9201\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 918us/step - loss: 0.8320\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 953us/step - loss: 0.8886\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 908us/step - loss: 0.8647\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 897us/step - loss: 0.8495\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 886us/step - loss: 0.9179\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 935us/step - loss: 0.8964\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 887us/step - loss: 0.8656\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 920us/step - loss: 0.8765\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 922us/step - loss: 0.8606\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 922us/step - loss: 0.8348\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 930us/step - loss: 0.8504\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 924us/step - loss: 0.8940\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 934us/step - loss: 0.8891\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 910us/step - loss: 0.8388\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 963us/step - loss: 0.8347\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 868us/step - loss: 0.8385\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 867us/step - loss: 0.8873\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 949us/step - loss: 0.8443\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 926us/step - loss: 0.8437\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 917us/step - loss: 0.8812\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 901us/step - loss: 0.8954\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 915us/step - loss: 0.8285\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 952us/step - loss: 0.8335\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 897us/step - loss: 0.8552\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 925us/step - loss: 0.8436\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 850us/step - loss: 0.8335\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 958us/step - loss: 0.8212\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 894us/step - loss: 0.8292\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 920us/step - loss: 0.7880\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 902us/step - loss: 0.9181\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 953us/step - loss: 0.9001\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 872us/step - loss: 0.8619\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 919us/step - loss: 0.8844\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 880us/step - loss: 0.8664\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 958us/step - loss: 0.8207\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 916us/step - loss: 0.8728\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 920us/step - loss: 0.8325\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 858us/step - loss: 0.8227\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 917us/step - loss: 0.8865\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 881us/step - loss: 0.8592\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 867us/step - loss: 0.7559\n",
      "2/2 [==============================] - 0s 829us/step\n",
      "2/2 [==============================] - 0s 745us/step\n",
      "2/2 [==============================] - 0s 686us/step\n",
      "2/2 [==============================] - 0s 737us/step\n",
      "2/2 [==============================] - 0s 743us/step\n",
      "2/2 [==============================] - 0s 729us/step\n",
      "2/2 [==============================] - 0s 749us/step\n",
      "2/2 [==============================] - 0s 661us/step\n",
      "2/2 [==============================] - 0s 680us/step\n",
      "2/2 [==============================] - 0s 640us/step\n",
      "2/2 [==============================] - 0s 686us/step\n",
      "2/2 [==============================] - 0s 665us/step\n",
      "2/2 [==============================] - 0s 626us/step\n",
      "2/2 [==============================] - 0s 646us/step\n",
      "2/2 [==============================] - 0s 653us/step\n",
      "2/2 [==============================] - 0s 613us/step\n",
      "2/2 [==============================] - 0s 680us/step\n",
      "2/2 [==============================] - 0s 667us/step\n",
      "2/2 [==============================] - 0s 635us/step\n",
      "2/2 [==============================] - 0s 686us/step\n",
      "2/2 [==============================] - 0s 575us/step\n",
      "2/2 [==============================] - 0s 726us/step\n",
      "2/2 [==============================] - 0s 628us/step\n",
      "2/2 [==============================] - 0s 645us/step\n",
      "2/2 [==============================] - 0s 703us/step\n",
      "2/2 [==============================] - 0s 659us/step\n",
      "2/2 [==============================] - 0s 684us/step\n",
      "2/2 [==============================] - 0s 672us/step\n",
      "2/2 [==============================] - 0s 667us/step\n",
      "2/2 [==============================] - 0s 619us/step\n",
      "2/2 [==============================] - 0s 618us/step\n",
      "2/2 [==============================] - 0s 606us/step\n",
      "2/2 [==============================] - 0s 704us/step\n",
      "2/2 [==============================] - 0s 690us/step\n",
      "2/2 [==============================] - 0s 693us/step\n",
      "2/2 [==============================] - 0s 704us/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "Final augmented data shape: (2500, 13)\n",
      "Augmented balanced data saved to /Users/hsiaopingni/Desktop/Final_Imbalanced_Augmented_Chiller_Data_VAE.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import Input, Dense, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "import optuna\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('/Users/hsiaopingni/Desktop/Hsiao-Ping PhD/Data/paper- Chiller/Anomaly_Detection_Results_VAE.csv', encoding='unicode_escape')\n",
    "data = data[['Discharge Temp (F)', 'Input % full load amps (Motor) (%)', 'Condenser liq temp IN (F)',\n",
    "             'Condenser liq temp OUT (F)', 'Chilled liq temp IN (F)', 'Chilled liq temp OUT (F)',\n",
    "             'Condenser saturation (F)', 'Evaporator saturation (F)', 'Evaporator pressure (PSIG)',\n",
    "             'Condenser pressure (PSIG)', 'Oil sump temp (F)', 'Oil pressure (PSIG)', 'Anomaly']]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data.drop(columns=['Anomaly']))  # Normalize only the features\n",
    "\n",
    "# Separate anomalies and normal data\n",
    "anomalies = data[data['Anomaly'] == 1]  # Assuming '1' is the label for anomalies\n",
    "normal_data = data[data['Anomaly'] == 0]  # Assuming '0' is the label for normal data\n",
    "\n",
    "# Normalize the data separately for anomalies and normal data\n",
    "anomalies_scaled = scaler.transform(anomalies.drop(columns=['Anomaly']))\n",
    "normal_data_scaled = scaler.transform(normal_data.drop(columns=['Anomaly']))\n",
    "\n",
    "# Define the Sampling layer for VAE\n",
    "class Sampling(Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        batch = tf.shape(mean)[0]\n",
    "        dim = tf.shape(mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return mean + tf.exp(0.5 * log_var) * epsilon\n",
    "\n",
    "# Define VAE architecture\n",
    "def build_vae(input_dim, hidden_dims, latent_dim):\n",
    "    # Encoder\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = inputs\n",
    "    for dim in hidden_dims:\n",
    "        x = Dense(dim, activation='relu')(x)\n",
    "    mean = Dense(latent_dim)(x)\n",
    "    log_var = Dense(latent_dim)(x)\n",
    "    z = Sampling()([mean, log_var])\n",
    "    encoder = Model(inputs, [mean, log_var, z], name=\"encoder\")\n",
    "    \n",
    "    # Decoder\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    x = latent_inputs\n",
    "    for dim in reversed(hidden_dims):\n",
    "        x = Dense(dim, activation='relu')(x)\n",
    "    outputs = Dense(input_dim, activation='sigmoid')(x)\n",
    "    decoder = Model(latent_inputs, outputs, name=\"decoder\")\n",
    "    \n",
    "    # VAE Model\n",
    "    reconstructed = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, reconstructed, name=\"vae\")\n",
    "    \n",
    "    # VAE Loss\n",
    "    reconstruction_loss = mse(inputs, reconstructed) * input_dim\n",
    "    kl_loss = 1 + log_var - tf.square(mean) - tf.exp(log_var)\n",
    "    kl_loss = tf.reduce_sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    \n",
    "    return vae, encoder, decoder\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters using TPE\n",
    "    hidden_dim1 = trial.suggest_int('hidden_dim1', 32, 128, step=16)\n",
    "    hidden_dim2 = trial.suggest_int('hidden_dim2', 16, 64, step=16)\n",
    "    latent_dim = trial.suggest_int('latent_dim', 2, 16, step=2)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 64, step=16)\n",
    "    \n",
    "    hidden_dims = [hidden_dim1, hidden_dim2]\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val = train_test_split(data_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the VAE using training and validation data\n",
    "    vae, encoder, decoder = build_vae(input_dim=data_scaled.shape[1], hidden_dims=hidden_dims, latent_dim=latent_dim)\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    history = vae.fit(X_train, X_train, validation_data=(X_val, X_val), epochs=100, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Access 'val_loss' after adding validation data\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna study using TPE\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best hyperparameters from Optuna\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Train the final VAE using the best hyperparameters\n",
    "hidden_dims = [best_params['hidden_dim1'], best_params['hidden_dim2']]\n",
    "latent_dim = best_params['latent_dim']\n",
    "learning_rate = best_params['learning_rate']\n",
    "batch_size = best_params['batch_size']\n",
    "\n",
    "vae, encoder, decoder = build_vae(input_dim=data_scaled.shape[1], hidden_dims=hidden_dims, latent_dim=latent_dim)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "history = vae.fit(data_scaled, data_scaled, epochs=100, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# Augment the normal data (generate 1250 normal samples)\n",
    "desired_normal_size = 2000\n",
    "augmented_normal_data_list = []\n",
    "while len(augmented_normal_data_list) < desired_normal_size:\n",
    "    new_samples = vae.predict(normal_data_scaled)\n",
    "    augmented_normal_data_list.extend(new_samples)\n",
    "\n",
    "augmented_normal_data = pd.DataFrame(augmented_normal_data_list[:desired_normal_size], columns=normal_data.columns[:-1])\n",
    "augmented_normal_data['label_column'] = 0  # Label for normal data\n",
    "\n",
    "# Augment the anomalies (generate 1250 anomaly samples)\n",
    "desired_anomaly_size = 500\n",
    "augmented_anomaly_data_list = []\n",
    "while len(augmented_anomaly_data_list) < desired_anomaly_size:\n",
    "    new_samples = vae.predict(anomalies_scaled)\n",
    "    augmented_anomaly_data_list.extend(new_samples)\n",
    "\n",
    "augmented_anomaly_data = pd.DataFrame(augmented_anomaly_data_list[:desired_anomaly_size], columns=anomalies.columns[:-1])\n",
    "augmented_anomaly_data['label_column'] = 1  # Label for anomalies\n",
    "\n",
    "# Combine the augmented data and the original data\n",
    "augmented_data = pd.concat([augmented_normal_data, augmented_anomaly_data], axis=0)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "augmented_data = shuffle(augmented_data, random_state=42)\n",
    "\n",
    "# Check final shape of the augmented dataset\n",
    "print(f\"Final augmented data shape: {augmented_data.shape}\")\n",
    "\n",
    "# Save the augmented data to an Excel file\n",
    "output_path = '/Users/hsiaopingni/Desktop/Final_Imbalanced_Augmented_Chiller_Data_VAE.xlsx'\n",
    "augmented_data.to_excel(output_path, index=False)\n",
    "print(f\"Augmented balanced data saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f73f0-eaf2-468d-8057-65e554c8b489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

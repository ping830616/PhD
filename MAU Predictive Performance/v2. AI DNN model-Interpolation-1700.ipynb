{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2449187",
   "metadata": {},
   "source": [
    "# Regression with Deep Neural Network (DNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf46af2",
   "metadata": {},
   "source": [
    "In a regression problem, the aim is to predict the output of a continuous value, like a energy consumption, a temperature value or a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b62a5c",
   "metadata": {},
   "source": [
    "This file demonstrates how to build models to predict the energy efficiency of the MAU system. To do this, you will provide the models with a description of many MAUs from that a certain period. This description includes attributes like temperature, humidity, airflow, and enthalpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "757b6769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn for pairplot.\n",
    "!pip install -q seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bcc84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Make NumPy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04780b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eaa09e",
   "metadata": {},
   "source": [
    "## Load all MAU entrance data (1700 counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd298cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ï»¿Date</th>\n",
       "      <th>Temperature (Â°F)</th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22-Feb-24</td>\n",
       "      <td>73.990000</td>\n",
       "      <td>31.990000</td>\n",
       "      <td>6.730000</td>\n",
       "      <td>38.080000</td>\n",
       "      <td>5.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22-Feb-24</td>\n",
       "      <td>74.116286</td>\n",
       "      <td>31.881889</td>\n",
       "      <td>6.727928</td>\n",
       "      <td>38.150347</td>\n",
       "      <td>5.679812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22-Feb-24</td>\n",
       "      <td>74.242572</td>\n",
       "      <td>31.773779</td>\n",
       "      <td>6.725856</td>\n",
       "      <td>38.220695</td>\n",
       "      <td>5.679623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22-Feb-24</td>\n",
       "      <td>74.368858</td>\n",
       "      <td>31.665668</td>\n",
       "      <td>6.723785</td>\n",
       "      <td>38.291042</td>\n",
       "      <td>5.679435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22-Feb-24</td>\n",
       "      <td>74.495144</td>\n",
       "      <td>31.557557</td>\n",
       "      <td>6.721713</td>\n",
       "      <td>38.361389</td>\n",
       "      <td>5.679247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>4-Apr-24</td>\n",
       "      <td>86.850863</td>\n",
       "      <td>7.938323</td>\n",
       "      <td>2.452054</td>\n",
       "      <td>36.248693</td>\n",
       "      <td>2.127534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>4-Apr-24</td>\n",
       "      <td>86.818147</td>\n",
       "      <td>7.941242</td>\n",
       "      <td>2.449041</td>\n",
       "      <td>36.224020</td>\n",
       "      <td>2.125650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>4-Apr-24</td>\n",
       "      <td>86.785431</td>\n",
       "      <td>7.944161</td>\n",
       "      <td>2.446027</td>\n",
       "      <td>36.199347</td>\n",
       "      <td>2.123767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>4-Apr-24</td>\n",
       "      <td>86.752716</td>\n",
       "      <td>7.947081</td>\n",
       "      <td>2.443014</td>\n",
       "      <td>36.174673</td>\n",
       "      <td>2.121883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>4-Apr-24</td>\n",
       "      <td>86.720000</td>\n",
       "      <td>7.950000</td>\n",
       "      <td>2.440000</td>\n",
       "      <td>36.150000</td>\n",
       "      <td>2.120000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ï»¿Date  Temperature (Â°F)  Humidity (%)  Density (g/m3)  \\\n",
       "0     22-Feb-24          73.990000     31.990000        6.730000   \n",
       "1     22-Feb-24          74.116286     31.881889        6.727928   \n",
       "2     22-Feb-24          74.242572     31.773779        6.725856   \n",
       "3     22-Feb-24          74.368858     31.665668        6.723785   \n",
       "4     22-Feb-24          74.495144     31.557557        6.721713   \n",
       "...         ...                ...           ...             ...   \n",
       "1695   4-Apr-24          86.850863      7.938323        2.452054   \n",
       "1696   4-Apr-24          86.818147      7.941242        2.449041   \n",
       "1697   4-Apr-24          86.785431      7.944161        2.446027   \n",
       "1698   4-Apr-24          86.752716      7.947081        2.443014   \n",
       "1699   4-Apr-24          86.720000      7.950000        2.440000   \n",
       "\n",
       "      Enthalpy, h (kJ/kg)  x (g/kg)  \n",
       "0               38.080000  5.680000  \n",
       "1               38.150347  5.679812  \n",
       "2               38.220695  5.679623  \n",
       "3               38.291042  5.679435  \n",
       "4               38.361389  5.679247  \n",
       "...                   ...       ...  \n",
       "1695            36.248693  2.127534  \n",
       "1696            36.224020  2.125650  \n",
       "1697            36.199347  2.123767  \n",
       "1698            36.174673  2.121883  \n",
       "1699            36.150000  2.120000  \n",
       "\n",
       "[1700 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load entrance data\n",
    "data_entrance = pd.read_csv('/Users/hsiaopingni/Desktop/Hsiao-Ping PhD/Data/MAU_paper/interpolated_data_entrance_1700.csv', encoding='unicode_escape')\n",
    "#data_entrance=pd.read_csv(r'C:\\Users\\Hsiao-ping.ni\\ASU Dropbox\\Hsiao-Ping Ni\\PhD\\Data\\3rd paper- MAU\\interpolated_data_entrance_1700.csv', encoding='unicode_escape')\n",
    "#data_entrance=pd.read_csv(r'C:\\Users\\nick8\\ASU Dropbox\\Hsiao-Ping Ni\\PhD\\Data\\3rd paper- MAU\\interpolated_data_entrance_1700.csv', encoding='unicode_escape')\n",
    "data_entrance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c1e7b",
   "metadata": {},
   "source": [
    "## Load all MAU exit data (1700 counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6f18855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ï»¿Date</th>\n",
       "      <th>Temperature (Â°F)</th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22-Feb-24</td>\n",
       "      <td>75.038000</td>\n",
       "      <td>38.790000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>42.380000</td>\n",
       "      <td>7.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22-Feb-24</td>\n",
       "      <td>75.037491</td>\n",
       "      <td>38.784067</td>\n",
       "      <td>8.398870</td>\n",
       "      <td>42.376892</td>\n",
       "      <td>7.168776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22-Feb-24</td>\n",
       "      <td>75.036983</td>\n",
       "      <td>38.778134</td>\n",
       "      <td>8.397740</td>\n",
       "      <td>42.373785</td>\n",
       "      <td>7.167552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22-Feb-24</td>\n",
       "      <td>75.036474</td>\n",
       "      <td>38.772201</td>\n",
       "      <td>8.396610</td>\n",
       "      <td>42.370677</td>\n",
       "      <td>7.166327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22-Feb-24</td>\n",
       "      <td>75.035966</td>\n",
       "      <td>38.766268</td>\n",
       "      <td>8.395480</td>\n",
       "      <td>42.367569</td>\n",
       "      <td>7.165103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>4-Apr-24</td>\n",
       "      <td>75.180644</td>\n",
       "      <td>39.821442</td>\n",
       "      <td>8.684932</td>\n",
       "      <td>43.056098</td>\n",
       "      <td>7.396816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>4-Apr-24</td>\n",
       "      <td>75.180983</td>\n",
       "      <td>39.838582</td>\n",
       "      <td>8.688699</td>\n",
       "      <td>43.064573</td>\n",
       "      <td>7.400112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>4-Apr-24</td>\n",
       "      <td>75.181322</td>\n",
       "      <td>39.855721</td>\n",
       "      <td>8.692466</td>\n",
       "      <td>43.073049</td>\n",
       "      <td>7.403408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>4-Apr-24</td>\n",
       "      <td>75.181661</td>\n",
       "      <td>39.872861</td>\n",
       "      <td>8.696233</td>\n",
       "      <td>43.081524</td>\n",
       "      <td>7.406704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>4-Apr-24</td>\n",
       "      <td>75.182000</td>\n",
       "      <td>39.890000</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>43.090000</td>\n",
       "      <td>7.410000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ï»¿Date  Temperature (Â°F)  Humidity (%)  Density (g/m3)  \\\n",
       "0     22-Feb-24          75.038000     38.790000        8.400000   \n",
       "1     22-Feb-24          75.037491     38.784067        8.398870   \n",
       "2     22-Feb-24          75.036983     38.778134        8.397740   \n",
       "3     22-Feb-24          75.036474     38.772201        8.396610   \n",
       "4     22-Feb-24          75.035966     38.766268        8.395480   \n",
       "...         ...                ...           ...             ...   \n",
       "1695   4-Apr-24          75.180644     39.821442        8.684932   \n",
       "1696   4-Apr-24          75.180983     39.838582        8.688699   \n",
       "1697   4-Apr-24          75.181322     39.855721        8.692466   \n",
       "1698   4-Apr-24          75.181661     39.872861        8.696233   \n",
       "1699   4-Apr-24          75.182000     39.890000        8.700000   \n",
       "\n",
       "      Enthalpy, h (kJ/kg)  x (g/kg)  \n",
       "0               42.380000  7.170000  \n",
       "1               42.376892  7.168776  \n",
       "2               42.373785  7.167552  \n",
       "3               42.370677  7.166327  \n",
       "4               42.367569  7.165103  \n",
       "...                   ...       ...  \n",
       "1695            43.056098  7.396816  \n",
       "1696            43.064573  7.400112  \n",
       "1697            43.073049  7.403408  \n",
       "1698            43.081524  7.406704  \n",
       "1699            43.090000  7.410000  \n",
       "\n",
       "[1700 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load exit data\n",
    "data_exit = pd.read_csv('/Users/hsiaopingni/Desktop/Hsiao-Ping PhD/Data/MAU_paper/interpolated_data_exit_1700.csv', encoding='unicode_escape')\n",
    "#data_exit=pd.read_csv(r'C:\\Users\\Hsiao-ping.ni\\ASU Dropbox\\Hsiao-Ping Ni\\PhD\\Data\\3rd paper- MAU\\interpolated_data_exit_1700.csv', encoding='unicode_escape')\n",
    "#data_exit=pd.read_csv(r'C:\\Users\\nick8\\ASU Dropbox\\Hsiao-Ping Ni\\PhD\\Data\\3rd paper- MAU\\interpolated_data_exit_1700.csv', encoding='unicode_escape')\n",
    "data_exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4941ff",
   "metadata": {},
   "source": [
    "## Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f127a",
   "metadata": {},
   "source": [
    "Now, split the dataset into a training set and a test set. You will use the test set in the final evaluation of your model.\n",
    "\n",
    "The line of code \"train_dataset = dataset.sample(frac=0.8, random_state=0)\" creates a training dataset by randomly selecting 80% of the rows from the dataset, ensuring that the selection is reproducible by setting a random state. The frac=0.8 parameter specifies that 80% of the data should be sampled, and random_state=0 ensures that the random selection of rows is consistent every time the code is run, facilitating reproducible results in experiments or analyses.\n",
    "\n",
    "The line \"test_dataset = dataset.drop(train_dataset.index)\" removes all rows from dataset that are already included in train_dataset, effectively creating a test dataset. This is achieved by dropping rows indexed in train_dataset.index from the original dataset. The result is a dataset containing 20% of the original data, not selected for training, used for testing model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899dde3",
   "metadata": {},
   "source": [
    "### Train datasets for MAU entrance (1360 counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909af274",
   "metadata": {},
   "source": [
    "Drop 'Count' and 'Which MAU' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ae9f3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature (Â°F)</th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>83.466828</td>\n",
       "      <td>18.543019</td>\n",
       "      <td>5.101725</td>\n",
       "      <td>40.178534</td>\n",
       "      <td>4.394903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>74.747716</td>\n",
       "      <td>31.341336</td>\n",
       "      <td>6.717569</td>\n",
       "      <td>38.502084</td>\n",
       "      <td>5.678870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>88.121089</td>\n",
       "      <td>20.181583</td>\n",
       "      <td>6.567116</td>\n",
       "      <td>46.078323</td>\n",
       "      <td>5.717116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>85.571177</td>\n",
       "      <td>15.360730</td>\n",
       "      <td>4.133055</td>\n",
       "      <td>39.144085</td>\n",
       "      <td>2.858517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>81.889241</td>\n",
       "      <td>20.778311</td>\n",
       "      <td>5.517593</td>\n",
       "      <td>40.126551</td>\n",
       "      <td>4.738446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>87.016398</td>\n",
       "      <td>13.513614</td>\n",
       "      <td>4.166021</td>\n",
       "      <td>40.295497</td>\n",
       "      <td>3.621931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>85.898941</td>\n",
       "      <td>15.096945</td>\n",
       "      <td>4.460594</td>\n",
       "      <td>40.258676</td>\n",
       "      <td>3.865274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>86.006086</td>\n",
       "      <td>14.013467</td>\n",
       "      <td>4.320783</td>\n",
       "      <td>39.944161</td>\n",
       "      <td>3.677151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>89.473867</td>\n",
       "      <td>13.144297</td>\n",
       "      <td>4.418676</td>\n",
       "      <td>42.106704</td>\n",
       "      <td>3.853820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>80.870029</td>\n",
       "      <td>23.606168</td>\n",
       "      <td>5.970930</td>\n",
       "      <td>40.496851</td>\n",
       "      <td>3.864138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1360 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Temperature (Â°F)  Humidity (%)  Density (g/m3)  Enthalpy, h (kJ/kg)  \\\n",
       "678           83.466828     18.543019        5.101725            40.178534   \n",
       "6             74.747716     31.341336        6.717569            38.502084   \n",
       "182           88.121089     20.181583        6.567116            46.078323   \n",
       "1229          85.571177     15.360730        4.133055            39.144085   \n",
       "654           81.889241     20.778311        5.517593            40.126551   \n",
       "...                 ...           ...             ...                  ...   \n",
       "732           87.016398     13.513614        4.166021            40.295497   \n",
       "715           85.898941     15.096945        4.460594            40.258676   \n",
       "832           86.006086     14.013467        4.320783            39.944161   \n",
       "951           89.473867     13.144297        4.418676            42.106704   \n",
       "1144          80.870029     23.606168        5.970930            40.496851   \n",
       "\n",
       "      x (g/kg)  \n",
       "678   4.394903  \n",
       "6     5.678870  \n",
       "182   5.717116  \n",
       "1229  2.858517  \n",
       "654   4.738446  \n",
       "...        ...  \n",
       "732   3.621931  \n",
       "715   3.865274  \n",
       "832   3.677151  \n",
       "951   3.853820  \n",
       "1144  3.864138  \n",
       "\n",
       "[1360 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_entrance = data_entrance.drop(['ï»¿Date'], axis=1)\n",
    "train_dataset_entrance = data_entrance.sample(frac=0.8, random_state=0)\n",
    "test_dataset_entrance = data_entrance.drop(train_dataset_entrance.index)\n",
    "train_dataset_entrance\n",
    "\n",
    "# Spliting data into Feature \n",
    "#X=data[['Humidity (%)','Airflow (g/m^3)','Enthalpy, h (kJ/kg)','x (g/kg)']]\n",
    "#y=data['Temperature (°F)']\n",
    "\n",
    "# Import train_test_split function\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test (validation) set\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  # 80% training and 20% test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f4ed3e",
   "metadata": {},
   "source": [
    "### Test datasets for MAU entrance (340 counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "360ce48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature (Â°F)</th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>75.379147</td>\n",
       "      <td>30.800783</td>\n",
       "      <td>6.707210</td>\n",
       "      <td>38.853820</td>\n",
       "      <td>5.677928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>76.894579</td>\n",
       "      <td>29.503455</td>\n",
       "      <td>6.682348</td>\n",
       "      <td>39.697987</td>\n",
       "      <td>5.675668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>77.020865</td>\n",
       "      <td>29.395344</td>\n",
       "      <td>6.680277</td>\n",
       "      <td>39.768334</td>\n",
       "      <td>5.675480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>77.147151</td>\n",
       "      <td>29.287234</td>\n",
       "      <td>6.678205</td>\n",
       "      <td>39.838682</td>\n",
       "      <td>5.675291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>77.526009</td>\n",
       "      <td>28.962902</td>\n",
       "      <td>6.671989</td>\n",
       "      <td>40.049723</td>\n",
       "      <td>5.674726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>87.995913</td>\n",
       "      <td>7.836145</td>\n",
       "      <td>2.557528</td>\n",
       "      <td>37.112260</td>\n",
       "      <td>2.193455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>87.636040</td>\n",
       "      <td>7.868258</td>\n",
       "      <td>2.524379</td>\n",
       "      <td>36.840853</td>\n",
       "      <td>2.172737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>87.537893</td>\n",
       "      <td>7.877016</td>\n",
       "      <td>2.515338</td>\n",
       "      <td>36.766833</td>\n",
       "      <td>2.167087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>87.210736</td>\n",
       "      <td>7.906210</td>\n",
       "      <td>2.485203</td>\n",
       "      <td>36.520100</td>\n",
       "      <td>2.148252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>86.883579</td>\n",
       "      <td>7.935403</td>\n",
       "      <td>2.455068</td>\n",
       "      <td>36.273367</td>\n",
       "      <td>2.129417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Temperature (Â°F)  Humidity (%)  Density (g/m3)  Enthalpy, h (kJ/kg)  \\\n",
       "11            75.379147     30.800783        6.707210            38.853820   \n",
       "23            76.894579     29.503455        6.682348            39.697987   \n",
       "24            77.020865     29.395344        6.680277            39.768334   \n",
       "25            77.147151     29.287234        6.678205            39.838682   \n",
       "28            77.526009     28.962902        6.671989            40.049723   \n",
       "...                 ...           ...             ...                  ...   \n",
       "1660          87.995913      7.836145        2.557528            37.112260   \n",
       "1671          87.636040      7.868258        2.524379            36.840853   \n",
       "1674          87.537893      7.877016        2.515338            36.766833   \n",
       "1684          87.210736      7.906210        2.485203            36.520100   \n",
       "1694          86.883579      7.935403        2.455068            36.273367   \n",
       "\n",
       "      x (g/kg)  \n",
       "11    5.677928  \n",
       "23    5.675668  \n",
       "24    5.675480  \n",
       "25    5.675291  \n",
       "28    5.674726  \n",
       "...        ...  \n",
       "1660  2.193455  \n",
       "1671  2.172737  \n",
       "1674  2.167087  \n",
       "1684  2.148252  \n",
       "1694  2.129417  \n",
       "\n",
       "[340 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_entrance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e9a567",
   "metadata": {},
   "source": [
    "### Load original entrance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b4d1ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Which MAU</th>\n",
       "      <th>Temperature (°F)</th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m^3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>73.990</td>\n",
       "      <td>31.99</td>\n",
       "      <td>6.73</td>\n",
       "      <td>38.08</td>\n",
       "      <td>5.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>87.400</td>\n",
       "      <td>20.51</td>\n",
       "      <td>6.51</td>\n",
       "      <td>45.55</td>\n",
       "      <td>5.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>88.410</td>\n",
       "      <td>20.05</td>\n",
       "      <td>6.59</td>\n",
       "      <td>46.29</td>\n",
       "      <td>5.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>87.370</td>\n",
       "      <td>20.35</td>\n",
       "      <td>6.45</td>\n",
       "      <td>45.39</td>\n",
       "      <td>5.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>87.330</td>\n",
       "      <td>20.63</td>\n",
       "      <td>6.51</td>\n",
       "      <td>45.59</td>\n",
       "      <td>5.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>75.540</td>\n",
       "      <td>29.08</td>\n",
       "      <td>6.44</td>\n",
       "      <td>38.33</td>\n",
       "      <td>5.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>80.780</td>\n",
       "      <td>22.35</td>\n",
       "      <td>5.81</td>\n",
       "      <td>40.09</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>87.760</td>\n",
       "      <td>12.46</td>\n",
       "      <td>3.97</td>\n",
       "      <td>40.32</td>\n",
       "      <td>3.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>85.660</td>\n",
       "      <td>14.32</td>\n",
       "      <td>4.39</td>\n",
       "      <td>39.87</td>\n",
       "      <td>3.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>89.650</td>\n",
       "      <td>13.09</td>\n",
       "      <td>4.42</td>\n",
       "      <td>42.21</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>88.720</td>\n",
       "      <td>13.32</td>\n",
       "      <td>4.37</td>\n",
       "      <td>41.51</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>78.570</td>\n",
       "      <td>26.62</td>\n",
       "      <td>6.44</td>\n",
       "      <td>40.20</td>\n",
       "      <td>3.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>90.770</td>\n",
       "      <td>7.00</td>\n",
       "      <td>2.42</td>\n",
       "      <td>38.36</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>90.734</td>\n",
       "      <td>7.57</td>\n",
       "      <td>2.63</td>\n",
       "      <td>38.76</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>88.988</td>\n",
       "      <td>7.97</td>\n",
       "      <td>2.71</td>\n",
       "      <td>38.04</td>\n",
       "      <td>2.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>90.194</td>\n",
       "      <td>7.64</td>\n",
       "      <td>2.76</td>\n",
       "      <td>38.77</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>86.720</td>\n",
       "      <td>7.95</td>\n",
       "      <td>2.44</td>\n",
       "      <td>36.15</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Count  Which MAU  Temperature (°F)  Humidity (%)  Density (g/m^3)  \\\n",
       "0       1          1            73.990         31.99             6.73   \n",
       "1       2          2            87.400         20.51             6.51   \n",
       "2       3          3            88.410         20.05             6.59   \n",
       "3       4          4            87.370         20.35             6.45   \n",
       "4       5          5            87.330         20.63             6.51   \n",
       "5       6          9            75.540         29.08             6.44   \n",
       "6       7          1            80.780         22.35             5.81   \n",
       "7       8          1            87.760         12.46             3.97   \n",
       "8       9          1            85.660         14.32             4.39   \n",
       "9      10          3            89.650         13.09             4.42   \n",
       "10     11          4            88.720         13.32             4.37   \n",
       "11     12          1            78.570         26.62             6.44   \n",
       "12     13          2            90.770          7.00             2.42   \n",
       "13     14          3            90.734          7.57             2.63   \n",
       "14     15          4            88.988          7.97             2.71   \n",
       "15     16          5            90.194          7.64             2.76   \n",
       "16     17          6            86.720          7.95             2.44   \n",
       "\n",
       "    Enthalpy, h (kJ/kg)  x (g/kg)  \n",
       "0                 38.08      5.68  \n",
       "1                 45.55      5.66  \n",
       "2                 46.29      5.74  \n",
       "3                 45.39      5.60  \n",
       "4                 45.59      5.66  \n",
       "5                 38.33      5.46  \n",
       "6                 40.09      4.98  \n",
       "7                 40.32      3.46  \n",
       "8                 39.87      3.72  \n",
       "9                 42.21      3.86  \n",
       "10                41.51      3.81  \n",
       "11                40.20      3.88  \n",
       "12                38.36      2.10  \n",
       "13                38.76      2.31  \n",
       "14                38.04      2.30  \n",
       "15                38.77      2.32  \n",
       "16                36.15      2.12  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load entrance data\n",
    "ori_data_entrance = pd.read_csv('/Users/hsiaopingni/Desktop/Hsiao-Ping PhD/Data/MAU_paper/AI_model_MAU_Entrance.csv', encoding='unicode_escape')\n",
    "#ori_data_entrance=pd.read_csv(r'C:\\Users\\Hsiao-ping.ni\\ASU Dropbox\\Hsiao-Ping Ni\\PhD\\Data\\3rd paper- MAU\\AI_model_MAU_Entrance.csv', encoding='unicode_escape')\n",
    "#ori_data_entrance=pd.read_csv(r'C:\\Users\\nick8\\ASU Dropbox\\Hsiao-Ping Ni\\PhD\\Data\\3rd paper- MAU\\AI_model_MAU_Entrance.csv', encoding='unicode_escape')\n",
    "ori_data_entrance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d2635",
   "metadata": {},
   "source": [
    "### Original test datasets for MAU entrance (3 counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b18973b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Which MAU</th>\n",
       "      <th>Temperature (°F)</th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m^3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>75.540</td>\n",
       "      <td>29.08</td>\n",
       "      <td>6.44</td>\n",
       "      <td>38.33</td>\n",
       "      <td>5.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>90.770</td>\n",
       "      <td>7.00</td>\n",
       "      <td>2.42</td>\n",
       "      <td>38.36</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>90.194</td>\n",
       "      <td>7.64</td>\n",
       "      <td>2.76</td>\n",
       "      <td>38.77</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Count  Which MAU  Temperature (°F)  Humidity (%)  Density (g/m^3)  \\\n",
       "5       6          9            75.540         29.08             6.44   \n",
       "12     13          2            90.770          7.00             2.42   \n",
       "15     16          5            90.194          7.64             2.76   \n",
       "\n",
       "    Enthalpy, h (kJ/kg)  x (g/kg)  \n",
       "5                 38.33      5.46  \n",
       "12                38.36      2.10  \n",
       "15                38.77      2.32  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_test_entrance = ori_data_entrance.iloc[[5, 12, 15]]\n",
    "ori_test_entrance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe38de2f",
   "metadata": {},
   "source": [
    "### Train datasets for MAU exit (1360 counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcaa78ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature (Â°F)</th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>74.848285</td>\n",
       "      <td>38.351672</td>\n",
       "      <td>8.283096</td>\n",
       "      <td>41.972343</td>\n",
       "      <td>7.049247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>75.034949</td>\n",
       "      <td>38.754403</td>\n",
       "      <td>8.393220</td>\n",
       "      <td>42.361354</td>\n",
       "      <td>7.162655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>75.061107</td>\n",
       "      <td>39.430830</td>\n",
       "      <td>8.565580</td>\n",
       "      <td>42.721112</td>\n",
       "      <td>7.297022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>74.836218</td>\n",
       "      <td>38.519488</td>\n",
       "      <td>8.321648</td>\n",
       "      <td>42.058411</td>\n",
       "      <td>7.114261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>74.864558</td>\n",
       "      <td>38.256745</td>\n",
       "      <td>8.269535</td>\n",
       "      <td>41.947481</td>\n",
       "      <td>7.037946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>74.811670</td>\n",
       "      <td>38.565256</td>\n",
       "      <td>8.313608</td>\n",
       "      <td>42.028281</td>\n",
       "      <td>7.074673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>74.823197</td>\n",
       "      <td>38.498016</td>\n",
       "      <td>8.304002</td>\n",
       "      <td>42.010671</td>\n",
       "      <td>7.066669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>74.879168</td>\n",
       "      <td>38.559888</td>\n",
       "      <td>8.745951</td>\n",
       "      <td>42.098464</td>\n",
       "      <td>7.505951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>75.066054</td>\n",
       "      <td>39.305127</td>\n",
       "      <td>8.571919</td>\n",
       "      <td>42.702631</td>\n",
       "      <td>7.293685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>74.650603</td>\n",
       "      <td>38.646798</td>\n",
       "      <td>8.306257</td>\n",
       "      <td>41.923155</td>\n",
       "      <td>7.131330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1360 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Temperature (Â°F)  Humidity (%)  Density (g/m3)  Enthalpy, h (kJ/kg)  \\\n",
       "678           74.848285     38.351672        8.283096            41.972343   \n",
       "6             75.034949     38.754403        8.393220            42.361354   \n",
       "182           75.061107     39.430830        8.565580            42.721112   \n",
       "1229          74.836218     38.519488        8.321648            42.058411   \n",
       "654           74.864558     38.256745        8.269535            41.947481   \n",
       "...                 ...           ...             ...                  ...   \n",
       "732           74.811670     38.565256        8.313608            42.028281   \n",
       "715           74.823197     38.498016        8.304002            42.010671   \n",
       "832           74.879168     38.559888        8.745951            42.098464   \n",
       "951           75.066054     39.305127        8.571919            42.702631   \n",
       "1144          74.650603     38.646798        8.306257            41.923155   \n",
       "\n",
       "      x (g/kg)  \n",
       "678   7.049247  \n",
       "6     7.162655  \n",
       "182   7.297022  \n",
       "1229  7.114261  \n",
       "654   7.037946  \n",
       "...        ...  \n",
       "732   7.074673  \n",
       "715   7.066669  \n",
       "832   7.505951  \n",
       "951   7.293685  \n",
       "1144  7.131330  \n",
       "\n",
       "[1360 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_exit = data_exit.drop(['ï»¿Date'], axis=1)\n",
    "train_dataset_exit = data_exit.sample(frac=0.8, random_state=0)\n",
    "test_dataset_exit = data_exit.drop(train_dataset_exit.index)\n",
    "train_dataset_exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a3cca",
   "metadata": {},
   "source": [
    "### Test datasets for MAU exit (340 counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb8dbaca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature (Â°F)</th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>75.032406</td>\n",
       "      <td>38.724738</td>\n",
       "      <td>8.387569</td>\n",
       "      <td>42.345815</td>\n",
       "      <td>7.156533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>75.026304</td>\n",
       "      <td>38.653543</td>\n",
       "      <td>8.374008</td>\n",
       "      <td>42.308523</td>\n",
       "      <td>7.141842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>75.025795</td>\n",
       "      <td>38.647610</td>\n",
       "      <td>8.372878</td>\n",
       "      <td>42.305415</td>\n",
       "      <td>7.140618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>75.025287</td>\n",
       "      <td>38.641677</td>\n",
       "      <td>8.371748</td>\n",
       "      <td>42.302307</td>\n",
       "      <td>7.139394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>75.023761</td>\n",
       "      <td>38.623879</td>\n",
       "      <td>8.368358</td>\n",
       "      <td>42.292984</td>\n",
       "      <td>7.135721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>75.168778</td>\n",
       "      <td>39.221560</td>\n",
       "      <td>8.553090</td>\n",
       "      <td>42.759453</td>\n",
       "      <td>7.281454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>75.172507</td>\n",
       "      <td>39.410094</td>\n",
       "      <td>8.594526</td>\n",
       "      <td>42.852684</td>\n",
       "      <td>7.317710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>75.173524</td>\n",
       "      <td>39.461513</td>\n",
       "      <td>8.605827</td>\n",
       "      <td>42.878111</td>\n",
       "      <td>7.327599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>75.176915</td>\n",
       "      <td>39.632908</td>\n",
       "      <td>8.643496</td>\n",
       "      <td>42.962866</td>\n",
       "      <td>7.360559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>75.180305</td>\n",
       "      <td>39.804303</td>\n",
       "      <td>8.681165</td>\n",
       "      <td>43.047622</td>\n",
       "      <td>7.393520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Temperature (Â°F)  Humidity (%)  Density (g/m3)  Enthalpy, h (kJ/kg)  \\\n",
       "11            75.032406     38.724738        8.387569            42.345815   \n",
       "23            75.026304     38.653543        8.374008            42.308523   \n",
       "24            75.025795     38.647610        8.372878            42.305415   \n",
       "25            75.025287     38.641677        8.371748            42.302307   \n",
       "28            75.023761     38.623879        8.368358            42.292984   \n",
       "...                 ...           ...             ...                  ...   \n",
       "1660          75.168778     39.221560        8.553090            42.759453   \n",
       "1671          75.172507     39.410094        8.594526            42.852684   \n",
       "1674          75.173524     39.461513        8.605827            42.878111   \n",
       "1684          75.176915     39.632908        8.643496            42.962866   \n",
       "1694          75.180305     39.804303        8.681165            43.047622   \n",
       "\n",
       "      x (g/kg)  \n",
       "11    7.156533  \n",
       "23    7.141842  \n",
       "24    7.140618  \n",
       "25    7.139394  \n",
       "28    7.135721  \n",
       "...        ...  \n",
       "1660  7.281454  \n",
       "1671  7.317710  \n",
       "1674  7.327599  \n",
       "1684  7.360559  \n",
       "1694  7.393520  \n",
       "\n",
       "[340 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a552321",
   "metadata": {},
   "source": [
    "### Load original exit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c10d9495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Which MAU</th>\n",
       "      <th>Temperature (°F)</th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m^3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>75.038</td>\n",
       "      <td>38.79</td>\n",
       "      <td>8.40</td>\n",
       "      <td>42.38</td>\n",
       "      <td>7.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>74.984</td>\n",
       "      <td>38.16</td>\n",
       "      <td>8.28</td>\n",
       "      <td>42.05</td>\n",
       "      <td>7.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>75.092</td>\n",
       "      <td>39.94</td>\n",
       "      <td>8.68</td>\n",
       "      <td>42.99</td>\n",
       "      <td>7.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>75.020</td>\n",
       "      <td>39.08</td>\n",
       "      <td>8.48</td>\n",
       "      <td>42.52</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>75.038</td>\n",
       "      <td>38.89</td>\n",
       "      <td>8.45</td>\n",
       "      <td>42.45</td>\n",
       "      <td>7.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>75.110</td>\n",
       "      <td>38.80</td>\n",
       "      <td>8.45</td>\n",
       "      <td>42.50</td>\n",
       "      <td>7.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>74.876</td>\n",
       "      <td>38.19</td>\n",
       "      <td>8.26</td>\n",
       "      <td>41.93</td>\n",
       "      <td>7.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>74.804</td>\n",
       "      <td>38.61</td>\n",
       "      <td>8.32</td>\n",
       "      <td>42.04</td>\n",
       "      <td>7.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>74.894</td>\n",
       "      <td>38.55</td>\n",
       "      <td>8.83</td>\n",
       "      <td>42.11</td>\n",
       "      <td>7.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>75.074</td>\n",
       "      <td>39.34</td>\n",
       "      <td>8.56</td>\n",
       "      <td>42.73</td>\n",
       "      <td>7.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>75.110</td>\n",
       "      <td>38.67</td>\n",
       "      <td>8.43</td>\n",
       "      <td>42.48</td>\n",
       "      <td>7.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>74.516</td>\n",
       "      <td>38.64</td>\n",
       "      <td>8.27</td>\n",
       "      <td>41.76</td>\n",
       "      <td>7.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>75.074</td>\n",
       "      <td>38.43</td>\n",
       "      <td>8.36</td>\n",
       "      <td>42.28</td>\n",
       "      <td>7.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>75.146</td>\n",
       "      <td>39.14</td>\n",
       "      <td>8.52</td>\n",
       "      <td>42.67</td>\n",
       "      <td>7.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>75.074</td>\n",
       "      <td>42.52</td>\n",
       "      <td>8.47</td>\n",
       "      <td>42.52</td>\n",
       "      <td>7.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>75.146</td>\n",
       "      <td>38.07</td>\n",
       "      <td>8.30</td>\n",
       "      <td>42.19</td>\n",
       "      <td>7.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>75.182</td>\n",
       "      <td>39.89</td>\n",
       "      <td>8.70</td>\n",
       "      <td>43.09</td>\n",
       "      <td>7.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Count  Which MAU  Temperature (°F)  Humidity (%)   Density (g/m^3)  \\\n",
       "0       1          1            75.038         38.79              8.40   \n",
       "1       2          2            74.984         38.16              8.28   \n",
       "2       3          3            75.092         39.94              8.68   \n",
       "3       4          4            75.020         39.08              8.48   \n",
       "4       5          5            75.038         38.89              8.45   \n",
       "5       6          9            75.110         38.80              8.45   \n",
       "6       7          1            74.876         38.19              8.26   \n",
       "7       8          1            74.804         38.61              8.32   \n",
       "8       9          1            74.894         38.55              8.83   \n",
       "9      10          3            75.074         39.34              8.56   \n",
       "10     11          4            75.110         38.67              8.43   \n",
       "11     12          1            74.516         38.64              8.27   \n",
       "12     13          2            75.074         38.43              8.36   \n",
       "13     14          3            75.146         39.14              8.52   \n",
       "14     15          4            75.074         42.52              8.47   \n",
       "15     16          5            75.146         38.07              8.30   \n",
       "16     17          6            75.182         39.89              8.70   \n",
       "\n",
       "     Enthalpy, h (kJ/kg)   x (g/kg)  \n",
       "0                  42.38       7.17  \n",
       "1                  42.05       7.04  \n",
       "2                  42.99       7.40  \n",
       "3                  42.52       2.22  \n",
       "4                  42.45       7.19  \n",
       "5                  42.50       7.19  \n",
       "6                  41.93       7.03  \n",
       "7                  42.04       7.08  \n",
       "8                  42.11       7.59  \n",
       "9                  42.73       7.28  \n",
       "10                 42.48       7.17  \n",
       "11                 41.76       7.12  \n",
       "12                 42.28       7.11  \n",
       "13                 42.67       7.26  \n",
       "14                 42.52       7.21  \n",
       "15                 42.19       7.06  \n",
       "16                 43.09       7.41  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load exit data\n",
    "ori_data_exit = pd.read_csv('/Users/hsiaopingni/Desktop/Hsiao-Ping PhD/Data/MAU_paper/AI_model_MAU_Exit.csv', encoding='unicode_escape')\n",
    "#ori_data_exit=pd.read_csv(r'C:\\Users\\Hsiao-ping.ni\\ASU Dropbox\\Hsiao-Ping Ni\\PhD\\Data\\3rd paper- MAU\\AI_model_MAU_Exit.csv', encoding='unicode_escape')\n",
    "#ori_data_exit=pd.read_csv(r'C:\\Users\\nick8\\ASU Dropbox\\Hsiao-Ping Ni\\PhD\\Data\\3rd paper- MAU\\AI_model_MAU_Exit.csv', encoding='unicode_escape')\n",
    "ori_data_exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098023d3",
   "metadata": {},
   "source": [
    "### Original test datasets for MAU exit (3 counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6e4541e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Which MAU</th>\n",
       "      <th>Temperature (°F)</th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m^3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>75.110</td>\n",
       "      <td>38.80</td>\n",
       "      <td>8.45</td>\n",
       "      <td>42.50</td>\n",
       "      <td>7.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>75.074</td>\n",
       "      <td>38.43</td>\n",
       "      <td>8.36</td>\n",
       "      <td>42.28</td>\n",
       "      <td>7.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>75.146</td>\n",
       "      <td>38.07</td>\n",
       "      <td>8.30</td>\n",
       "      <td>42.19</td>\n",
       "      <td>7.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Count  Which MAU  Temperature (°F)  Humidity (%)   Density (g/m^3)  \\\n",
       "5       6          9            75.110         38.80              8.45   \n",
       "12     13          2            75.074         38.43              8.36   \n",
       "15     16          5            75.146         38.07              8.30   \n",
       "\n",
       "     Enthalpy, h (kJ/kg)   x (g/kg)  \n",
       "5                  42.50       7.19  \n",
       "12                 42.28       7.11  \n",
       "15                 42.19       7.06  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_test_exit = ori_data_exit.iloc[[5, 12, 15]]\n",
    "ori_test_exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb1ad4a",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478de120",
   "metadata": {},
   "source": [
    "Let's check the overall statistics. Note how each feature covers a very different range.\n",
    "\n",
    ".describe(): This method generates descriptive statistics that summarize the central tendency, dispersion, and shape of the dataset's distribution, excluding NaN values. By default, it provides information such as count (number of non-missing values), mean, standard deviation (std), minimum, 25th percentile (25%), median (50th percentile), 75th percentile (75%), and maximum for numeric columns.\n",
    "\n",
    ".transpose() or .T: This method transposes the DataFrame, swapping its rows and columns. After calling .describe(), the resulting DataFrame has the descriptive statistics as rows and the features (or columns of the original dataset) as columns. Transposing flips this layout, so the features become rows and the descriptive statistics become columns. This often makes the output more readable and easier to analyze, especially if the dataset has many features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e88be",
   "metadata": {},
   "source": [
    "### Train datasets for MAU entrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9344f266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Temperature (Â°F)</th>\n",
       "      <td>1360.0</td>\n",
       "      <td>86.124192</td>\n",
       "      <td>4.036394</td>\n",
       "      <td>73.990000</td>\n",
       "      <td>84.090256</td>\n",
       "      <td>87.380406</td>\n",
       "      <td>89.129997</td>\n",
       "      <td>90.769746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humidity (%)</th>\n",
       "      <td>1360.0</td>\n",
       "      <td>16.435948</td>\n",
       "      <td>6.775283</td>\n",
       "      <td>7.004026</td>\n",
       "      <td>7.951933</td>\n",
       "      <td>15.950544</td>\n",
       "      <td>20.624067</td>\n",
       "      <td>31.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Density (g/m3)</th>\n",
       "      <td>1360.0</td>\n",
       "      <td>4.807931</td>\n",
       "      <td>1.565589</td>\n",
       "      <td>2.421483</td>\n",
       "      <td>2.754202</td>\n",
       "      <td>4.611454</td>\n",
       "      <td>6.471748</td>\n",
       "      <td>6.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <td>1360.0</td>\n",
       "      <td>41.004123</td>\n",
       "      <td>2.754461</td>\n",
       "      <td>36.150000</td>\n",
       "      <td>38.661074</td>\n",
       "      <td>40.203596</td>\n",
       "      <td>42.160555</td>\n",
       "      <td>46.284703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x (g/kg)</th>\n",
       "      <td>1360.0</td>\n",
       "      <td>4.043720</td>\n",
       "      <td>1.352695</td>\n",
       "      <td>2.101483</td>\n",
       "      <td>2.316551</td>\n",
       "      <td>3.838311</td>\n",
       "      <td>5.609005</td>\n",
       "      <td>5.739176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count       mean       std        min        25%  \\\n",
       "Temperature (Â°F)    1360.0  86.124192  4.036394  73.990000  84.090256   \n",
       "Humidity (%)         1360.0  16.435948  6.775283   7.004026   7.951933   \n",
       "Density (g/m3)       1360.0   4.807931  1.565589   2.421483   2.754202   \n",
       "Enthalpy, h (kJ/kg)  1360.0  41.004123  2.754461  36.150000  38.661074   \n",
       "x (g/kg)             1360.0   4.043720  1.352695   2.101483   2.316551   \n",
       "\n",
       "                           50%        75%        max  \n",
       "Temperature (Â°F)    87.380406  89.129997  90.769746  \n",
       "Humidity (%)         15.950544  20.624067  31.990000  \n",
       "Density (g/m3)        4.611454   6.471748   6.730000  \n",
       "Enthalpy, h (kJ/kg)  40.203596  42.160555  46.284703  \n",
       "x (g/kg)              3.838311   5.609005   5.739176  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_entrance.describe().transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188981e",
   "metadata": {},
   "source": [
    "### Train datasets for MAU exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e38f405b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Temperature (Â°F)</th>\n",
       "      <td>1360.0</td>\n",
       "      <td>75.006312</td>\n",
       "      <td>0.130806</td>\n",
       "      <td>74.516350</td>\n",
       "      <td>74.946519</td>\n",
       "      <td>75.037936</td>\n",
       "      <td>75.094092</td>\n",
       "      <td>75.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humidity (%)</th>\n",
       "      <td>1360.0</td>\n",
       "      <td>39.024597</td>\n",
       "      <td>0.794583</td>\n",
       "      <td>38.073214</td>\n",
       "      <td>38.568222</td>\n",
       "      <td>38.808846</td>\n",
       "      <td>39.158424</td>\n",
       "      <td>42.504285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Density (g/m3)</th>\n",
       "      <td>1360.0</td>\n",
       "      <td>8.450169</td>\n",
       "      <td>0.124396</td>\n",
       "      <td>8.260494</td>\n",
       "      <td>8.347993</td>\n",
       "      <td>8.450000</td>\n",
       "      <td>8.509529</td>\n",
       "      <td>8.826186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <td>1360.0</td>\n",
       "      <td>42.376201</td>\n",
       "      <td>0.271984</td>\n",
       "      <td>41.760424</td>\n",
       "      <td>42.146346</td>\n",
       "      <td>42.429285</td>\n",
       "      <td>42.558180</td>\n",
       "      <td>43.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x (g/kg)</th>\n",
       "      <td>1360.0</td>\n",
       "      <td>6.880821</td>\n",
       "      <td>0.972392</td>\n",
       "      <td>2.240477</td>\n",
       "      <td>7.083040</td>\n",
       "      <td>7.154517</td>\n",
       "      <td>7.227363</td>\n",
       "      <td>7.585621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count       mean       std        min        25%  \\\n",
       "Temperature (Â°F)    1360.0  75.006312  0.130806  74.516350  74.946519   \n",
       "Humidity (%)         1360.0  39.024597  0.794583  38.073214  38.568222   \n",
       "Density (g/m3)       1360.0   8.450169  0.124396   8.260494   8.347993   \n",
       "Enthalpy, h (kJ/kg)  1360.0  42.376201  0.271984  41.760424  42.146346   \n",
       "x (g/kg)             1360.0   6.880821  0.972392   2.240477   7.083040   \n",
       "\n",
       "                           50%        75%        max  \n",
       "Temperature (Â°F)    75.037936  75.094092  75.182000  \n",
       "Humidity (%)         38.808846  39.158424  42.504285  \n",
       "Density (g/m3)        8.450000   8.509529   8.826186  \n",
       "Enthalpy, h (kJ/kg)  42.429285  42.558180  43.090000  \n",
       "x (g/kg)              7.154517   7.227363   7.585621  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_exit.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af1dc6",
   "metadata": {},
   "source": [
    "## Split features from labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512c35e",
   "metadata": {},
   "source": [
    "Separate the target value—the \"label\"—from the features. This label is the value that you will train the model to predict.\n",
    "\n",
    "train_features = train_dataset.copy(): This line creates a copy of the train_dataset DataFrame and assigns it to train_features. The .copy() method ensures that the original dataset remains unchanged when modifications are made to train_features. This dataset contains the features used to train the machine learning model.\n",
    "\n",
    "test_features = test_dataset.copy(): Similarly, this line duplicates the test_dataset DataFrame, storing the copy in test_features. This is done to preserve the original test_dataset while allowing modifications on test_features. This dataset is used to evaluate the model's performance after training.\n",
    "\n",
    "train_labels = train_features.pop('Temperature (°F)'): The .pop() method removes the column named 'Temperature (°F)' from train_features and returns it. This removed column is then stored in train_labels. In our machine learning context, 'Temperature (°F)' is considered the target variable (or label) that the model will be trained to predict. By doing this, train_features now only contains the input features (or independent variables) for the training data, while train_labels holds the corresponding target values.\n",
    "\n",
    "test_labels = test_features.pop('Temperature (°F)'): This line does the same operation as the previous one but for the testing dataset. It removes the 'Temperature (°F)' column from test_features and stores it in test_labels. Now, test_features only includes the input features for the testing data, and test_labels contains the corresponding target values that will be used to evaluate the model's predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a918516",
   "metadata": {},
   "source": [
    "### Train and test features, and train and test labels for MAU entrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed65d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_entrance = train_dataset_entrance.copy()\n",
    "test_features_entrance = test_dataset_entrance.copy()\n",
    "train_labels_entrance = train_features_entrance.pop('Temperature (Â°F)')\n",
    "test_labels_entrance = test_features_entrance.pop('Temperature (Â°F)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6867cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "ori_test_features_entrance = ori_test_entrance.copy()\n",
    "ori_test_labels_entrance = ori_test_features_entrance.pop('Temperature (°F)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2479641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>18.543019</td>\n",
       "      <td>5.101725</td>\n",
       "      <td>40.178534</td>\n",
       "      <td>4.394903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31.341336</td>\n",
       "      <td>6.717569</td>\n",
       "      <td>38.502084</td>\n",
       "      <td>5.678870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>20.181583</td>\n",
       "      <td>6.567116</td>\n",
       "      <td>46.078323</td>\n",
       "      <td>5.717116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>15.360730</td>\n",
       "      <td>4.133055</td>\n",
       "      <td>39.144085</td>\n",
       "      <td>2.858517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>20.778311</td>\n",
       "      <td>5.517593</td>\n",
       "      <td>40.126551</td>\n",
       "      <td>4.738446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>13.513614</td>\n",
       "      <td>4.166021</td>\n",
       "      <td>40.295497</td>\n",
       "      <td>3.621931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>15.096945</td>\n",
       "      <td>4.460594</td>\n",
       "      <td>40.258676</td>\n",
       "      <td>3.865274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>14.013467</td>\n",
       "      <td>4.320783</td>\n",
       "      <td>39.944161</td>\n",
       "      <td>3.677151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>13.144297</td>\n",
       "      <td>4.418676</td>\n",
       "      <td>42.106704</td>\n",
       "      <td>3.853820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>23.606168</td>\n",
       "      <td>5.970930</td>\n",
       "      <td>40.496851</td>\n",
       "      <td>3.864138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1360 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Humidity (%)  Density (g/m3)  Enthalpy, h (kJ/kg)  x (g/kg)\n",
       "678      18.543019        5.101725            40.178534  4.394903\n",
       "6        31.341336        6.717569            38.502084  5.678870\n",
       "182      20.181583        6.567116            46.078323  5.717116\n",
       "1229     15.360730        4.133055            39.144085  2.858517\n",
       "654      20.778311        5.517593            40.126551  4.738446\n",
       "...            ...             ...                  ...       ...\n",
       "732      13.513614        4.166021            40.295497  3.621931\n",
       "715      15.096945        4.460594            40.258676  3.865274\n",
       "832      14.013467        4.320783            39.944161  3.677151\n",
       "951      13.144297        4.418676            42.106704  3.853820\n",
       "1144     23.606168        5.970930            40.496851  3.864138\n",
       "\n",
       "[1360 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_entrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1a40da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>30.800783</td>\n",
       "      <td>6.707210</td>\n",
       "      <td>38.853820</td>\n",
       "      <td>5.677928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>29.503455</td>\n",
       "      <td>6.682348</td>\n",
       "      <td>39.697987</td>\n",
       "      <td>5.675668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29.395344</td>\n",
       "      <td>6.680277</td>\n",
       "      <td>39.768334</td>\n",
       "      <td>5.675480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>29.287234</td>\n",
       "      <td>6.678205</td>\n",
       "      <td>39.838682</td>\n",
       "      <td>5.675291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28.962902</td>\n",
       "      <td>6.671989</td>\n",
       "      <td>40.049723</td>\n",
       "      <td>5.674726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>7.836145</td>\n",
       "      <td>2.557528</td>\n",
       "      <td>37.112260</td>\n",
       "      <td>2.193455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>7.868258</td>\n",
       "      <td>2.524379</td>\n",
       "      <td>36.840853</td>\n",
       "      <td>2.172737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>7.877016</td>\n",
       "      <td>2.515338</td>\n",
       "      <td>36.766833</td>\n",
       "      <td>2.167087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>7.906210</td>\n",
       "      <td>2.485203</td>\n",
       "      <td>36.520100</td>\n",
       "      <td>2.148252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>7.935403</td>\n",
       "      <td>2.455068</td>\n",
       "      <td>36.273367</td>\n",
       "      <td>2.129417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Humidity (%)  Density (g/m3)  Enthalpy, h (kJ/kg)  x (g/kg)\n",
       "11       30.800783        6.707210            38.853820  5.677928\n",
       "23       29.503455        6.682348            39.697987  5.675668\n",
       "24       29.395344        6.680277            39.768334  5.675480\n",
       "25       29.287234        6.678205            39.838682  5.675291\n",
       "28       28.962902        6.671989            40.049723  5.674726\n",
       "...            ...             ...                  ...       ...\n",
       "1660      7.836145        2.557528            37.112260  2.193455\n",
       "1671      7.868258        2.524379            36.840853  2.172737\n",
       "1674      7.877016        2.515338            36.766833  2.167087\n",
       "1684      7.906210        2.485203            36.520100  2.148252\n",
       "1694      7.935403        2.455068            36.273367  2.129417\n",
       "\n",
       "[340 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features_entrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc49e212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m^3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29.08</td>\n",
       "      <td>6.44</td>\n",
       "      <td>38.33</td>\n",
       "      <td>5.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.00</td>\n",
       "      <td>2.42</td>\n",
       "      <td>38.36</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7.64</td>\n",
       "      <td>2.76</td>\n",
       "      <td>38.77</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Humidity (%)  Density (g/m^3)  Enthalpy, h (kJ/kg)  x (g/kg)\n",
       "5          29.08             6.44                38.33      5.46\n",
       "12          7.00             2.42                38.36      2.10\n",
       "15          7.64             2.76                38.77      2.32"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_test_features_entrance = ori_test_features_entrance.drop(['Count','Which MAU'], axis=1)\n",
    "ori_test_features_entrance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04f9d966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "678     83.466828\n",
       "6       74.747716\n",
       "182     88.121089\n",
       "1229    85.571177\n",
       "654     81.889241\n",
       "          ...    \n",
       "732     87.016398\n",
       "715     85.898941\n",
       "832     86.006086\n",
       "951     89.473867\n",
       "1144    80.870029\n",
       "Name: Temperature (Â°F), Length: 1360, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_entrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd346d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11      75.379147\n",
       "23      76.894579\n",
       "24      77.020865\n",
       "25      77.147151\n",
       "28      77.526009\n",
       "          ...    \n",
       "1660    87.995913\n",
       "1671    87.636040\n",
       "1674    87.537893\n",
       "1684    87.210736\n",
       "1694    86.883579\n",
       "Name: Temperature (Â°F), Length: 340, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_entrance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd02152",
   "metadata": {},
   "source": [
    "### Train and test features, and train and test labels for MAU exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a99eca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_exit = train_dataset_exit.copy()\n",
    "test_features_exit = test_dataset_exit.copy()\n",
    "train_labels_exit = train_features_exit.pop('Temperature (Â°F)')\n",
    "test_labels_exit = test_features_exit.pop('Temperature (Â°F)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82174b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "ori_test_features_exit = ori_test_exit.copy()\n",
    "ori_test_labels_exit = ori_test_features_exit.pop('Temperature (°F)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ba7c11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>38.351672</td>\n",
       "      <td>8.283096</td>\n",
       "      <td>41.972343</td>\n",
       "      <td>7.049247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38.754403</td>\n",
       "      <td>8.393220</td>\n",
       "      <td>42.361354</td>\n",
       "      <td>7.162655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>39.430830</td>\n",
       "      <td>8.565580</td>\n",
       "      <td>42.721112</td>\n",
       "      <td>7.297022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>38.519488</td>\n",
       "      <td>8.321648</td>\n",
       "      <td>42.058411</td>\n",
       "      <td>7.114261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>38.256745</td>\n",
       "      <td>8.269535</td>\n",
       "      <td>41.947481</td>\n",
       "      <td>7.037946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>38.565256</td>\n",
       "      <td>8.313608</td>\n",
       "      <td>42.028281</td>\n",
       "      <td>7.074673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>38.498016</td>\n",
       "      <td>8.304002</td>\n",
       "      <td>42.010671</td>\n",
       "      <td>7.066669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>38.559888</td>\n",
       "      <td>8.745951</td>\n",
       "      <td>42.098464</td>\n",
       "      <td>7.505951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>39.305127</td>\n",
       "      <td>8.571919</td>\n",
       "      <td>42.702631</td>\n",
       "      <td>7.293685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>38.646798</td>\n",
       "      <td>8.306257</td>\n",
       "      <td>41.923155</td>\n",
       "      <td>7.131330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1360 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Humidity (%)  Density (g/m3)  Enthalpy, h (kJ/kg)  x (g/kg)\n",
       "678      38.351672        8.283096            41.972343  7.049247\n",
       "6        38.754403        8.393220            42.361354  7.162655\n",
       "182      39.430830        8.565580            42.721112  7.297022\n",
       "1229     38.519488        8.321648            42.058411  7.114261\n",
       "654      38.256745        8.269535            41.947481  7.037946\n",
       "...            ...             ...                  ...       ...\n",
       "732      38.565256        8.313608            42.028281  7.074673\n",
       "715      38.498016        8.304002            42.010671  7.066669\n",
       "832      38.559888        8.745951            42.098464  7.505951\n",
       "951      39.305127        8.571919            42.702631  7.293685\n",
       "1144     38.646798        8.306257            41.923155  7.131330\n",
       "\n",
       "[1360 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57ab8ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>38.724738</td>\n",
       "      <td>8.387569</td>\n",
       "      <td>42.345815</td>\n",
       "      <td>7.156533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>38.653543</td>\n",
       "      <td>8.374008</td>\n",
       "      <td>42.308523</td>\n",
       "      <td>7.141842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>38.647610</td>\n",
       "      <td>8.372878</td>\n",
       "      <td>42.305415</td>\n",
       "      <td>7.140618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>38.641677</td>\n",
       "      <td>8.371748</td>\n",
       "      <td>42.302307</td>\n",
       "      <td>7.139394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>38.623879</td>\n",
       "      <td>8.368358</td>\n",
       "      <td>42.292984</td>\n",
       "      <td>7.135721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>39.221560</td>\n",
       "      <td>8.553090</td>\n",
       "      <td>42.759453</td>\n",
       "      <td>7.281454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>39.410094</td>\n",
       "      <td>8.594526</td>\n",
       "      <td>42.852684</td>\n",
       "      <td>7.317710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>39.461513</td>\n",
       "      <td>8.605827</td>\n",
       "      <td>42.878111</td>\n",
       "      <td>7.327599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>39.632908</td>\n",
       "      <td>8.643496</td>\n",
       "      <td>42.962866</td>\n",
       "      <td>7.360559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>39.804303</td>\n",
       "      <td>8.681165</td>\n",
       "      <td>43.047622</td>\n",
       "      <td>7.393520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Humidity (%)  Density (g/m3)  Enthalpy, h (kJ/kg)  x (g/kg)\n",
       "11       38.724738        8.387569            42.345815  7.156533\n",
       "23       38.653543        8.374008            42.308523  7.141842\n",
       "24       38.647610        8.372878            42.305415  7.140618\n",
       "25       38.641677        8.371748            42.302307  7.139394\n",
       "28       38.623879        8.368358            42.292984  7.135721\n",
       "...            ...             ...                  ...       ...\n",
       "1660     39.221560        8.553090            42.759453  7.281454\n",
       "1671     39.410094        8.594526            42.852684  7.317710\n",
       "1674     39.461513        8.605827            42.878111  7.327599\n",
       "1684     39.632908        8.643496            42.962866  7.360559\n",
       "1694     39.804303        8.681165            43.047622  7.393520\n",
       "\n",
       "[340 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features_exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c01fb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Density (g/m^3)</th>\n",
       "      <th>Enthalpy, h (kJ/kg)</th>\n",
       "      <th>x (g/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>38.80</td>\n",
       "      <td>8.45</td>\n",
       "      <td>42.50</td>\n",
       "      <td>7.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>38.43</td>\n",
       "      <td>8.36</td>\n",
       "      <td>42.28</td>\n",
       "      <td>7.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>38.07</td>\n",
       "      <td>8.30</td>\n",
       "      <td>42.19</td>\n",
       "      <td>7.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Humidity (%)   Density (g/m^3)   Enthalpy, h (kJ/kg)   x (g/kg)\n",
       "5          38.80              8.45                 42.50       7.19\n",
       "12         38.43              8.36                 42.28       7.11\n",
       "15         38.07              8.30                 42.19       7.06"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_test_features_exit = ori_test_features_exit.drop(['Count','Which MAU'], axis=1)\n",
    "ori_test_features_exit  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ad33b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "678     74.848285\n",
       "6       75.034949\n",
       "182     75.061107\n",
       "1229    74.836218\n",
       "654     74.864558\n",
       "          ...    \n",
       "732     74.811670\n",
       "715     74.823197\n",
       "832     74.879168\n",
       "951     75.066054\n",
       "1144    74.650603\n",
       "Name: Temperature (Â°F), Length: 1360, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7da236b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11      75.032406\n",
       "23      75.026304\n",
       "24      75.025795\n",
       "25      75.025287\n",
       "28      75.023761\n",
       "          ...    \n",
       "1660    75.168778\n",
       "1671    75.172507\n",
       "1674    75.173524\n",
       "1684    75.176915\n",
       "1694    75.180305\n",
       "Name: Temperature (Â°F), Length: 340, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06c8501c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     75.110\n",
       "12    75.074\n",
       "15    75.146\n",
       "Name: Temperature (°F), dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_test_labels_exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e07e6e9",
   "metadata": {},
   "source": [
    "## Regression with a deep neural network (DNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db869e0a",
   "metadata": {},
   "source": [
    "Here, you will implement a multiple-input DNN model.\n",
    "\n",
    "The name \"hidden\" here just means not directly connected to the inputs or outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a282793",
   "metadata": {},
   "source": [
    "This model will contain a few layers.\n",
    "\n",
    "* The dense input layer.\n",
    "* Two hidden, non-linear, `Dense` layers with the ReLU (`relu`) activation function nonlinearity.\n",
    "* A linear `Dense` single-output layer.\n",
    "\n",
    "The `compile` method is included in the `build_and_compile_model` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8e0518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model():\n",
    "  model = keras.Sequential([\n",
    "      layers.Dense(4,  kernel_initializer='normal', input_dim = train_features_entrance.shape[1], activation='relu'),\n",
    "      layers.Dense(64,  kernel_initializer='normal', activation='relu'),\n",
    "      layers.Dense(64,  kernel_initializer='normal', activation='relu'),\n",
    "      layers.Dense(1)])\n",
    "\n",
    "  model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a2a043ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different configurations\n",
    "#configs = [\n",
    " #   {\"layers\": 2, \"neurons_per_layer\": [32, 16]},\n",
    " #   {\"layers\": 3, \"neurons_per_layer\": [64, 32, 16]},\n",
    " #   {\"layers\": 4, \"neurons_per_layer\": [128, 64, 32, 16]},\n",
    " #   {\"layers\": 2, \"neurons_per_layer\": [64, 64]},\n",
    " #   {\"layers\": 3, \"neurons_per_layer\": [128, 64, 32]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1449909e",
   "metadata": {},
   "source": [
    "### Regression using a DNN and multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d03c2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                320       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,565\n",
      "Trainable params: 4,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x3083aaf50>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model = build_and_compile_model()\n",
    "dnn_model.summary()\n",
    "dnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a155532",
   "metadata": {},
   "source": [
    "This table summarizes the architecture of the neural network model. The table is divided into several columns detailing the layers in the model, their types, output shapes, and the number of parameters (weights and biases) each layer has. Here's a breakdown of the table:\n",
    "\n",
    "Layer: Indicates the name and type of each layer in the model. The model consists of a normalization layer followed by three dense (fully connected) layers.\n",
    "\n",
    "Output Shape: The shape of the output for each layer. The notation (None, x) indicates that the batch size is variable (denoted by None), and x is the dimensionality of the output from that layer.\n",
    "\n",
    "The normalization layer takes an input with 9 features ((None, 9)).\n",
    "The first dense layer outputs 64 units ((None, 64)).\n",
    "The second dense layer, identical to the first, also outputs 64 units.\n",
    "The final dense layer outputs a single unit ((None, 1)), corresponding to the model's prediction.\n",
    "\n",
    "Param #: Lists the number of parameters in each layer, which are learned during the training process.\n",
    "\n",
    "The normalization layer has 19 parameters, which are not trainable. These parameters might include statistics like mean and variance for each input feature used for data normalization.\n",
    "The first dense layer has 640 parameters, calculated as (9 input features * 64 output units) + 64 bias terms.\n",
    "The second dense layer has 4160 parameters, derived from (64 input units * 64 output units) + 64 bias terms.\n",
    "The final dense layer has 65 parameters, from (64 input units * 1 output unit) + 1 bias term.\n",
    "\n",
    "Total params: The total number of parameters in the model, summing to 4,884. This includes both trainable and non-trainable parameters.\n",
    "\n",
    "Trainable params: The number of parameters that will be updated during training, totaling 4,865. This excludes the normalization layer's statistics.\n",
    "\n",
    "Non-trainable params: Parameters that do not get updated during the training process, in this case, 19, likely related to the normalization layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3793b510",
   "metadata": {},
   "source": [
    "\n",
    "Train the model with Keras `Model.fit`.\n",
    "\n",
    "The validation_split=0.2 argument in the fit method of the Keras API indicates that 20% of the training data should be set aside for validation. The model will train on 80% of the data and evaluate its performance on the remaining 20% to monitor for issues like overfitting. This validation set is not used to update the model's weights; it's only for evaluation purposes to give an estimate of the model's performance on unseen data.\n",
    "\n",
    "The verbose parameter controls how much information the training process outputs to the console. Setting verbose=0 means that you won’t see any logging output during training, which can be useful if you don't need to track the training process in detail and want to avoid cluttering your console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "66eccda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 19:59:20.247647: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.73 s, sys: 1.18 s, total: 3.91 s\n",
      "Wall time: 2.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history_entrance = dnn_model.fit(\n",
    "    train_features_entrance,\n",
    "    train_labels_entrance,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73ea7bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 s, sys: 1.11 s, total: 3.68 s\n",
      "Wall time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history_exit = dnn_model.fit(\n",
    "    train_features_exit,\n",
    "    train_labels_exit,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d46c66",
   "metadata": {},
   "source": [
    "### Visualize the model's training progress in DNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d182f5f",
   "metadata": {},
   "source": [
    "Visualize the model's training progress using the stats stored in the `history` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "39113f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history_entrance):\n",
    "  plt.plot(history_entrance.history['loss'], label='Training loss')\n",
    "  plt.plot(history_entrance.history['val_loss'], label='Validation loss')\n",
    "  plt.ylim([0, 100])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error [Temperature (°F)]')\n",
    "  plt.title('Training progress in DNN model for MAU entrance')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b62084fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(history_entrance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4657bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history_exit):\n",
    "  plt.plot(history_exit.history['loss'], label='Training loss')\n",
    "  plt.plot(history_exit.history['val_loss'], label='Validation loss')\n",
    "  plt.ylim([0, 2])\n",
    "  plt.xlim([0, 100])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error [Temperature (°F)]')\n",
    "  plt.title('Training progress in DNN model for MAU exit')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "89c506f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAHFCAYAAADv8c1wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC2UklEQVR4nOzdd3gUVRfA4d+m90oaJQm99w7SOwgoIiCCVBE7YkVs2BAbRUAsQD5FqoBYEOlNAlIDSJESCIQESEJ622Tn+2Oym2wau5CyhPM+zz67M3Nn5u7OJjm59865GkVRFIQQQgghRLGsyrsCQgghhBD3AgmahBBCCCFMIEGTEEIIIYQJJGgSQgghhDCBBE1CCCGEECaQoEkIIYQQwgQSNAkhhBBCmECCJiGEEEIIE0jQJIQQQghhAgmahBGNRmPSY+fOnXd1nvfeew+NRnNH++7cubNE6iDKjkaj4b333iux4+m/P/qHk5MTVatWpU+fPnz11VckJSUV2Gfs2LFoNBoaNmxIdnZ2oXV87rnnDMuXLl0yHH/lypVF1iEmJqbE3ldp0L+PkJAQs/c152dt27ZttGrVCmdnZzQaDb/88ovZ5zNV3mtT1Pdq/PjxhjJFadGiBRqNhs8//7zQ7be7xo0aNaJr167mVv+u5X/fp06d4r333uPSpUtlXpf7jQRNwkhoaKjRo3///jg6OhZY36JFi7s6z8SJEwkNDb2jfVu0aFEidRBlJzQ0lIkTJ5b4cTdt2kRoaCibNm3i888/JzAwkNdee42GDRsSFhZW6D6nTp0yO4CYPn06Wq22BGpcMSmKwrBhw7C1teXXX38lNDSULl26lPp5XV1dCQkJQafTGa1PTk5mzZo1uLm5FbnvsWPHOHr0KACLFy8u1XqWtPw/T6dOnWLGjBkSNJUBCZqEkXbt2hk9fHx8sLKyKrA+/y+j1NRUs85TtWpV2rVrd0d1dHNzK7QO9wJzP6d75Vy3065dO6pWrVrix23ZsiXt2rWjc+fOjBgxgu+++479+/eTmJjIoEGDyMjIMCrv7OxMp06dePfdd0lLSzPpHP369ePixYssWrSoxOtfUVy7do24uDgefvhhevToQbt27fD09LyrY6alpXG7qVGHDx/O5cuX2bZtm9H6VatWkZ2dzaBBg4rc9/vvvwdgwIABnDlzhn379t1VfctSaf08iduToEmYrWvXrjRq1Ijdu3fToUMHnJycGD9+PKD+surduzcBAQE4OjpSv3593njjDVJSUoyOUVj3XHBwMA8++CCbNm2iRYsWODo6Uq9ePZYsWWJUrrAug7Fjx+Li4sL58+fp378/Li4uVKtWjZdffrnAH86rV68ydOhQXF1d8fDw4PHHH+fgwYMmdWGEhISg0WjYsmUL48aNw8vLC2dnZwYOHMjFixdN/pwiIiIYNWoUvr6+2NvbU79+fb744osC/zGbWlf9+z9x4gS9e/fG1dWVHj16AJCZmcmHH35IvXr1sLe3x8fHh3HjxnHz5k2jc23fvp2uXbvi7e2No6MjgYGBPPLII0bB19dff03Tpk1xcXHB1dWVevXq8eabbxb7mUHB7gT957hjxw6efvppKlWqhLe3N0OGDOHatWu3PV5xmjZtyvTp04mIiGDVqlUFts+aNYvIyEjmzp1r0vG6d+9Onz59+OCDDwrt9rsd/Xf9+PHjPProo7i7u+Pl5cXUqVPJysri7Nmz9O3bF1dXV4KDg/n0008LHMPU78u1a9cYNmwYrq6uuLu7M3z4cKKjowut16FDhxg0aBBeXl44ODjQvHlzVq9efUfvT/8H/PXXX0ej0RAcHGzYvnfvXnr06IGrqytOTk506NCBP/74w+gY+u/D5s2bGT9+PD4+Pjg5ORX42c2vbt26dOjQocDviCVLljBkyBDc3d0L3S89PZ3ly5fTsmVLZs+ebdintCiKwsKFC2nWrBmOjo54enoydOhQo98ZK1euRKPRMH/+fKN93333XaytrdmyZYthXd6fp5CQEB599FEAunXrZuiSvJPuWHF7EjSJOxIVFcWoUaMYOXIkGzdu5JlnngHg3Llz9O/fn8WLF7Np0yamTJnC6tWrGThwoEnHDQsL4+WXX+all15iw4YNNGnShAkTJrB79+7b7qvVahk0aBA9evRgw4YNjB8/ntmzZzNr1ixDmZSUFLp168aOHTuYNWsWq1evxs/Pj+HDh5v1/idMmICVlRXLly9nzpw5/PPPP3Tt2pX4+HijcoV9Tjdv3qRDhw5s3ryZDz74gF9//ZWePXvyyiuvGI2pMbeumZmZDBo0iO7du7NhwwZmzJiBTqdj8ODBfPLJJ4wcOZI//viDTz75hC1bttC1a1dDa8ulS5cYMGAAdnZ2LFmyhE2bNvHJJ5/g7OxMZmYmoP5Sf+aZZ+jSpQvr16/nl19+4aWXXioQEJtj4sSJ2Nrasnz5cj799FN27tzJqFGj7vh4evoWhsK+N+3bt+fhhx9m1qxZxMXFmXS8WbNmERMTw2effXbHdRo2bBhNmzZl7dq1PPnkk8yePZuXXnqJhx56iAEDBrB+/Xq6d+/O66+/zrp16wz7mfp9SUtLo2fPnmzevJmZM2eyZs0a/P39C/2+7Nixg44dOxIfH8+iRYvYsGEDzZo1Y/jw4Wb/sZ04caKhvs8//zyhoaGsX78egF27dtG9e3cSEhJYvHgxK1aswNXVlYEDBxYa0I4fPx5bW1t+/PFHfv75Z2xtbW97/gkTJvDLL79w69YtAM6ePcu+ffuYMGFCkfusW7eOW7duMX78eGrXrs0DDzzAqlWrSE5ONuu9m+qpp55iypQp9OzZk19++YWFCxfy77//0qFDB65fvw7AiBEjmDx5Mi+//DKHDh0C1H9kPvzwQ95880169epV6LEHDBjAxx9/DMCCBQsMQygGDBhQKu/lvqcIUYwxY8Yozs7ORuu6dOmiAMq2bduK3Ven0ylarVbZtWuXAihhYWGGbe+++66S/+sXFBSkODg4KJcvXzasS0tLU7y8vJSnnnrKsG7Hjh0KoOzYscOonoCyevVqo2P2799fqVu3rmF5wYIFCqD8+eefRuWeeuopBVCWLl1a7HtaunSpAigPP/yw0fq///5bAZQPP/zQsK6oz+mNN95QAOXAgQNG659++mlFo9EoZ8+eNbuu+ve/ZMkSo7IrVqxQAGXt2rVG6w8ePKgAysKFCxVFUZSff/5ZAZRjx44V+d6fe+45xcPDo8jtxQGUd99917Cs/xyfeeYZo3KffvqpAihRUVHFHk///bl582ah29PS0hRA6devn2Fd3u/ymTNnFGtra+Xll182quOzzz5rWA4PD1cA5bPPPlMURVEef/xxxdnZ2VC329Uhf12/+OILo/XNmjVTAGXdunWGdVqtVvHx8VGGDBliWGfq9+Xrr79WAGXDhg1G5Z588skC35d69eopzZs3V7RarVHZBx98UAkICFCys7MVRSn8Z60w+T8rvXbt2im+vr5KUlKSYV1WVpbSqFEjpWrVqopOp1MUJff78MQTTxR7nsLOl5SUpLi4uCjz589XFEVRXn31VaV69eqKTqdTnn322QK/ZxRFUbp37644ODgot27dMjr/4sWLjcrd7ho3bNhQ6dKlS7F1DQ0NLfT6X7lyRXF0dFRee+01w7r09HSlefPmSvXq1ZVTp04pfn5+SpcuXZSsrCyjffP/PK1Zs8ak6yTunrQ0iTvi6elJ9+7dC6y/ePEiI0eOxN/fH2tra2xtbQ0DQk+fPn3b4zZr1ozAwEDDsoODA3Xq1OHy5cu33Vej0RRo0WrSpInRvrt27cLV1ZW+ffsalXvsscdue/y8Hn/8caPlDh06EBQUxI4dO4zWF/Y5bd++nQYNGtCmTRuj9WPHjkVRFLZv337HdX3kkUeMln///Xc8PDwYOHAgWVlZhkezZs3w9/c3dHE2a9YMOzs7Jk2axP/+978CXY0Abdq0IT4+nscee4wNGzaUyF1j+cecNGnSBMCk610c5TZjYerWrcuECROYP38+ERERJh3zww8/RKvVMmPGjDuq04MPPmi0XL9+fTQaDf369TOss7GxoVatWkbv39Tvy44dO3B1dS3wmY4cOdJo+fz585w5c8bwHc77vejfvz9RUVGcPXv2jt5jXikpKRw4cIChQ4fi4uJiWG9tbc3o0aO5evVqgfPk//6awsXFhUcffZQlS5aQlZXFDz/8wLhx44q8ay48PJwdO3YwZMgQPDw8AHj00UdxdXUtlS6633//HY1Gw6hRo4w+a39/f5o2bWo0zMDe3p7Vq1cTGxtLixYtUBSFFStWYG1tXeL1EndGgiZxRwICAgqsS05OplOnThw4cIAPP/yQnTt3cvDgQUPTvSkDb729vQuss7e3N2lfJycnHBwcCuybnp5uWI6NjcXPz6/AvoWtK46/v3+h62JjY43WFfY5xcbGFrq+cuXKhu13UlcnJ6cCg+OvX79OfHw8dnZ22NraGj2io6MNgU/NmjXZunUrvr6+PPvss9SsWZOaNWsajfsZPXo0S5Ys4fLlyzzyyCP4+vrStm1bo7EW5sp/ve3t7QHTvivF0Qcd+s+0MO+99x7W1ta8/fbbJh0zODiYZ555hu+//55z586ZXScvLy+jZTs7u0K/s3Z2dgW+s3fzfcn/XdV3B73yyisFvhP6bvaSCIhv3bqFoigm1V2vsLKmmDBhAkeOHOGjjz7i5s2bjB07tsiyS5YsQVEUhg4dSnx8PPHx8Yau/b///pszZ84YytrY2AAUmqIC1IDzdl2I169fR1EU/Pz8Cnze+/fvL/BZ16pVi06dOpGens7jjz9+x5+JKB025V0BcW8q7L+47du3c+3aNXbu3Gl0u3H+cT7lydvbm3/++afA+qIGyxalsPLR0dHUqlXLaF1hn5O3tzdRUVEF1usHQFeqVOmO6lrYufQDrDdt2lToPq6urobXnTp1olOnTmRnZ3Po0CG++uorpkyZgp+fHyNGjABg3LhxjBs3jpSUFHbv3s27777Lgw8+yH///UdQUFCh5ygPv/76K0CxOXQCAgKYMmUKn3zyCS+//LJJx33rrbdYsmQJb775Jg0bNiyJqt5WSX9f9OWnTZvGkCFDCj1n3bp176rOoLayWllZmVR3vTvN3daxY0fq1q3L+++/T69evahWrVqh5XQ6nWHMVlHvfcmSJYbB+PogNDIyskBAqigKUVFRtGrVqti6VapUCY1Gw549ewz/FOSVf93333/PH3/8QZs2bZg/fz7Dhw+nbdu2xZ5DlB1paRIlRv8LL/8vgW+++aY8qlOoLl26kJSUxJ9//mm0vrDkhcX56aefjJb37dvH5cuXTUp016NHD06dOsWRI0eM1v/www9oNBq6detWYnV98MEHiY2NJTs7m1atWhV4FPbH0dramrZt27JgwQKAAvUE9db9fv36MX36dDIzM/n3339NrlNpCwsL4+OPPyY4OJhhw4YVW/b111/Hy8uLN954w6Rje3t78/rrr/Pzzz8XGqCUBlO/L926dSMpKckQMOotX77caLlu3brUrl2bsLCwQr8TrVq1Mgqm75SzszNt27Zl3bp1Ri2HOp2OZcuWUbVqVerUqXPX59F76623GDhwYLEB8F9//cXVq1d59tln2bFjR4FHw4YN+eGHH8jKygLUOyc1Gk2hg9Y3bdpEYmIiPXv2LLZeDz74IIqiEBkZWehn3bhxY0PZEydO8MILL/DEE0+wZ88emjRpwvDhww2D3ItSUi204vakpUmUmA4dOuDp6cnkyZN59913sbW15aeffioyyWB5GDNmDLNnz2bUqFF8+OGH1KpViz///JO//voLACsr0/6POHToEBMnTuTRRx/lypUrTJ8+nSpVqhi6N4rz0ksv8cMPPzBgwADef/99goKC+OOPP1i4cCFPP/204Q9JSdR1xIgR/PTTT/Tv358XX3yRNm3aYGtry9WrV9mxYweDBw/m4YcfZtGiRWzfvp0BAwYQGBhIenq6YXyH/o/Ck08+iaOjIx07diQgIIDo6GhmzpyJu7s7rVu3NulzK2mHDx/G3d0drVbLtWvX2LZtGz/++CO+vr789ttv2NnZFbu/m5sb06dP56WXXjL5nFOmTGHBggUFgtnSYur35YknnmD27Nk88cQTfPTRR9SuXZuNGzcavi95ffPNN/Tr148+ffowduxYqlSpQlxcHKdPn+bIkSOsWbOmROo+c+ZMevXqRbdu3XjllVews7Nj4cKFnDx5khUrVtxxy1JhRo0adds7LxcvXoyNjQ1vvvlmoV23Tz31FC+88AJ//PEHgwcPpmbNmjz33HN89tlnxMfHG5L9Hjx4kE8++YRWrVoVGDOWX8eOHZk0aRLjxo3j0KFDdO7cGWdnZ6Kioti7dy+NGzfm6aefJiUlhWHDhlG9enUWLlyInZ0dq1evpkWLFowbN67YDOuNGjUC4Ntvv8XV1RUHBweqV69e6HAHcXekpUmUGG9vb/744w+cnJwYNWoU48ePx8XFpdD/0sqLs7OzIR/Ra6+9xiOPPEJERAQLFy4EMAwMvZ3FixeTmZnJiBEjeOGFF2jVqhU7d+4sMG6lMD4+Puzbt4/u3bszbdo0HnzwQf766y8+/fRTvvrqqxKtq7W1Nb/++itvvvkm69at4+GHH+ahhx7ik08+wcHBwfBfbrNmzcjKyuLdd9+lX79+jB49mps3b/Lrr7/Su3dvQO2+O3nyJC+++CK9evXipZdeok6dOuzZswcfHx+TPreS1rdvX9q3b2+oz+XLl5k1axYnT540/CG5nWeeeYbq1aubfE4nJ6cSnRLmdkz9vjg5ObF9+3Z69uzJG2+8wdChQ7l69WqhLZPdunXjn3/+wcPDw3Ar/NNPP83WrVtv23Jiji5durB9+3acnZ0ZO3YsI0aMICEhgV9//dXsNB93KyYmht9++40HH3ywyLFuo0ePxtHR0ShD+Ny5c1m4cCFHjhxh5MiRDBw4kP/973+G1qrbBeagBqnz589n9+7djBgxggEDBvDOO++QkpJiGOA/efJkIiIiWLNmDc7OzgDUqFGD77//ng0bNjBnzpwij1+9enXmzJlDWFgYXbt2pXXr1vz2229mfDrCVBrldreZCHEf+Pjjj3nrrbeIiIgoNtNuSEgI48aN4+DBg7cdy1BaTK2rEEKIkiXdc+K+o8+4W69ePbRaLdu3b2fevHmMGjXK4oKQe6muQghR0UnQJO47Tk5OzJ49m0uXLpGRkUFgYCCvv/46b731VnlXrYB7qa5CCFHRSfecEEIIIYQJynUg+MyZM2ndujWurq74+vry0EMPmZSJdteuXbRs2RIHBwdq1KhR6Ozja9eupUGDBtjb29OgQQPDXEhCCCGEEHeiXIOmXbt28eyzz7J//362bNlCVlYWvXv3LnYC0PDwcPr370+nTp04evQob775Ji+88AJr1641lAkNDWX48OGMHj2asLAwRo8ezbBhwzhw4EBZvC0hhBBCVEAW1T138+ZNfH192bVrF507dy60zOuvv86vv/5qNI/Z5MmTCQsLIzQ0FIDhw4eTmJholEelb9++eHp6smLFitJ9E0IIIYSokCxqIHhCQgJQcI6mvEJDQw15Y/T69OnD4sWL0Wq12NraEhoaWiBZXZ8+fYrMc5GRkUFGRoZhWafTERcXh7e3d4kmXxNCCCFE6VEUhaSkJCpXrmxysmJzWEzQpCgKU6dO5YEHHig2KV10dHSBOYD8/PzIysoiJibGkKm4sDJFzdk1c+bMO565XAghhBCW5cqVK6WSlsVigqbnnnuO48ePs3fv3tuWzd/6o+9hzLu+sDJFtRpNmzaNqVOnGpYTEhIIDAzkv//+MynDsyg9Wq2WHTt20K1bt9vOJi5Kn1wPyyHXwnLItbAccXFx1KlTp0TmTyyMRQRNzz//PL/++iu7d+++bWTo7+9foMXoxo0b2NjYGObZKapM/tYnPXt7+0Jnn/by8pK5e8qZVqvFyckJb29v+WVkAeR6WA65FpZDroXlKa2hNeV695yiKDz33HOsW7eO7du3mzT/U/v27dmyZYvRus2bN9OqVSvDl7WoMh06dCi5ygshhBDivlKuQdOzzz7LsmXLWL58Oa6urkRHRxMdHU1aWpqhzLRp03jiiScMy5MnT+by5ctMnTqV06dPs2TJEhYvXswrr7xiKPPiiy+yefNmZs2axZkzZ5g1axZbt25lypQpZfn2hBBCCFGBlGvQ9PXXX5OQkEDXrl0JCAgwPFatWmUoExUVRUREhGG5evXqbNy4kZ07d9KsWTM++OAD5s2bxyOPPGIo06FDB1auXMnSpUtp0qQJISEhrFq1irZt25bp+xNCCCFExVGuY5pMSREVEhJSYF2XLl04cuRIsfsNHTqUoUOH3mnVhBBClKDs7Gy0Wm15V6NUaLVabGxsSE9PJzs7u7yrU+HZ2dmVSjoBU1jEQHAhhBAVk6IoREdHEx8fX95VKTWKouDv78+VK1ckt18ZsLKyonr16tjZ2ZX5uSVoEkIIUWr0AZOvry9OTk4VMqjQ6XQkJyfj4uJSbi0g9wudTse1a9eIiooiMDCwzL9PEjQJIYQoFdnZ2YaAqSKnb9HpdGRmZuLg4CBBUxnw8fHh2rVrZGVllXmKB7m6QgghSoV+DJOTk1M510RUJPpuufIYPyZBkxBCiFJVEbvkRPkpz++TBE1CCCGEECaQoEkIIYQoA127djUryfKlS5fQaDQcO3as1OoEsHPnTjQaTYW+w7GkyEBwIYQQIo/bdf+MGTOm0ByCt7Nu3TqzBi5Xq1aNqKgoKlWqZPa5ROmQoEkIIYTIIyoqyvB61apVvPPOO5w9e9awztHR0ai8qUk7vby8zKqHtbU1/v7+Zu0jSpd0zwkhhBB5+Pv7Gx7u7u5oNBrDcnp6Oh4eHqxevZquXbvi4ODAsmXLiIuLY+TIkVStWhUnJycaN27MihUrjI6bv3suODiYjz/+mPHjx+Pq6kpgYCDffvutYXv+7jl9N9q2bdto1aoVTk5OdOjQwSigA/jwww/x9fXF1dWViRMn8sYbb9CsWTOzPoO1a9fSsGFD7O3tCQ4O5osvvjDavnDhQmrXro2DgwN+fn5GM3D8/PPPNG7cGEdHR7y9venZsycpKSlmnd9SSdAkhBCizCiKQmpmVrk8TJm6y1Svv/46L7zwAqdPn6ZPnz6kp6fTsmVLfv/9d06ePMmkSZMYPXo0Bw4cKPY4X3zxBa1ateLo0aM888wzPP3005w5c6bYfaZPn84XX3zBoUOHsLGxYfz48YZtP/30Ex999BGzZs3i8OHDBAYG8vXXX5v13g4fPsywYcMYMWIEJ06c4L333uPtt982dEkeOnSIF154gffff5+zZ8+yadMmOnfuDKitdI899hjjx4/n9OnT7Ny5kyFDhpToZ1+epHtOCCFEmUnTZtPgnb/K5dyn3u+Dk13J/NmbMmUKQ4YMAdTkli4uLrz88suG5JbPP/88mzZtYs2aNcVOFt+/f3+eeeYZQA3EZs+ezc6dO6lXr16R+3z00Ud06dIFgDfeeIMBAwaQnp6Og4MDX331FRMmTGDcuHEAvPPOO2zevJnk5GST39uXX35Jjx49ePvttwGoU6cOp06d4rPPPmPs2LFERETg7OzMgw8+iKurK0FBQTRv3hxQg6asrCyGDBlCUFAQAI0bNzb53JZOWpqEEEIIM7Vq1cpoOTs7m48//pgmTZrg7e2Ni4sLmzdvJiIiotjjNGnSxPBa3w1448YNk/cJCAgAMOxz9uxZ2rRpY1Q+//LtnD59mo4dOxqt69ixI+fOnSM7O5tevXoRFBREjRo1GD16ND/99BOpqakANG3alB49etC4cWMeffRRvvvuO27dumXW+S2ZtDQJIYQoM4621px6v0+5nbukODs7Gy3Pnz+fr776ijlz5tC4cWOcnZ2ZMmUKmZmZxR4n/910Go0GnU5n8j76O/3y7pP/7j9zu8YURSn2GK6urhw5coSdO3eyefNm3nnnHd577z0OHjyIh4cHW7ZsYd++fWzevJmvvvqK6dOnc+DAAapXr25WPSyRtDQJIYQoMxqNBic7m3J5lGYm6dDQUAYNGsSoUaNo2rQpNWrU4Ny5c6V2vqLUrVuXf/75x2jdoUOHzDpGgwYN2Lt3r9G6ffv2UadOHayt1cDTxsaGnj178umnn3L8+HEuXbrE9u3bAfUad+zYkRkzZnD06FHs7OxYv379XbwryyEtTUIIIcRdqlGjBr///jv79u3D09OTL7/8kujoaOrXr1+m9Xj++ed58sknadWqFR06dGDVqlUcP36cGjVqmHyMl19+mdatW/PBBx8wfPhwQkNDmT9/PgsXLgTg999/5+LFi3Tu3BlPT082btyITqejbt26HDhwgG3bttG7d298fX05cOAAN2/eLPPPobRI0CSEEELcpVdffZXIyEj69OmDk5MTkyZN4qGHHiIhIaFM6/H4449z8eJFXnnlFdLT0xk2bBhjx44t0PpUnBYtWrB69WreeecdPvjgAwICAnj//fcZO3YsAB4eHqxbt4733nuP9PR0ateuzYoVK2jYsCGnT59m9+7dzJkzh8TERIKCgvjiiy/o169fKb3jsqVRKsp9gCUoMTERd3d3YmJi8Pb2Lu/q3Ne0Wi0bN26kf//+ZmXSFaVDrofluBeuRXp6OuHh4VSvXh0HB4fyrk6p0el0JCYm4ubmZrh7zpL06tULf39/fvzxx/KuSoko7nsVGxtLpUqVSEhIwM3NrcTPLS1NQgghRAWRmprKokWL6NOnD9bW1qxYsYKtW7eyZcuW8q5ahSBBkxBCCFFBaDQaNm7cyIcffkhGRgZ169Zl7dq19OzZs7yrViFI0CSEEEJUEI6OjmzdurW8q1FhWV7nqxBCCCGEBZKgSQghhBDCBBI0CSGEEEKYQIImIYQQQggTSNAkhBBCCGECCZqEEEIIIUwgQZMQQghRCrp27cqUKVMMy8HBwcyZM6fYfTQaDb/88stdn7ukjlOc9957j2bNmpXqOSyNBE1CCCFEHgMHDiwyGWRoaCgajYYjR46YfdyDBw8yadKku62ekaICl6ioqAoz35slkaBJCCGEyGPChAls376dy5cvF9i2ZMkSmjVrRosWLcw+ro+PD05OTiVRxdvy9/fH3t6+TM51P5GgSQghhMjjwQcfxNfXl5CQEKP1qamprFq1igkTJhAbG8tjjz1G1apVcXFxoUOHDqxYsaLY4+bvnjt37hydO3fGwcGBBg0aFDo/3Ouvv06dOnVwcnKiRo0avP3222i1WgBCQkKYMWMGYWFhaDQaNBqNoc75u+dOnDhB9+7dcXR0xNvbm0mTJpGcnGzYPnbsWB566CE+//xzAgIC8Pb25tlnnzWcyxQ6nY7333+fqlWrYm9vT7Nmzdi0aZNhe2ZmJs899xwBAQE4ODgQHBzMzJkzDdvfe+89AgMDsbe3p3Llyrzwwgsmn7usyDQqQgghyo6igDa1fM5t6wQazW2L2djY8MQTTxASEsI777yDJmefNWvWkJmZyeOPP05qaiotW7bk9ddfx8XFhXXr1jFmzBhq1apF27Ztb3sOnU7HkCFDqFSpEvv37ycxMdFo/JOeq6srISEhVK5cmRMnTvDkk0/i6urKa6+9xvDhwzl58iSbNm0yTJ3i7u5e4Bipqan07duXdu3acfDgQW7cuMHEiRN57rnnjALDHTt2EBAQwI4dOzh//jzDhw+nWbNmPPnkk7d9PwBz587liy++4JtvvqF58+YsWbKEQYMG8e+//1K7dm3mzZvHr7/+yurVqwkMDOTKlStcuXIFgJ9//pnZs2ezcuVKGjZsSHR0NGFhYSadtyxJ0CSEEKLsaFPh48rlc+43r4Gds0lFx48fz2effcbOnTvp1q0boHbNDRkyBE9PTzw9PXnllVcANQCaNGkSO3fuZM2aNSYFTVu3buX06dNcunSJqlWrAvDxxx8XGIf01ltvGV4HBwfz8ssvs2rVKl577TUcHR1xcXHBxsYGf3//Is/1008/kZaWxg8//ICzs/r+58+fz8CBA5k1axZ+fn4AeHp6Mn/+fKytralXrx4DBgxg27ZtJgdNn3/+Oa+//jojRowAYNasWezYsYM5c+awYMECIiIiqF27Ng888AAajYagoCDDvhEREfj7+9OzZ09sbW0JDAykTZs2Jp23LEn3nBBCCJFPvXr16NChA0uWLAHgwoUL7Nmzh/HjxwOQnZ3NRx99RJMmTfDx8aFq1aps2bKFiIgIk45/+vRpAgMDDQETQPv27QuU+/nnn3nggQfw9/fHxcWFt99+2+Rz5D1X06ZNDQETQMeOHdHpdJw9e9awrmHDhlhbWxuWAwICuHHjhknnSExM5Nq1a3Ts2NFofceOHTl9+jSgdgEeO3aMunXr8sILL7B582ZDuUcffZS0tDRq1KjBk08+yfr168nKyjLrfZYFaWkSQghRdmyd1Baf8jq3GSZMmMBzzz3HggULWLp0KUFBQfTo0QOAL774gtmzZzNnzhwaNmyIoii8/fbbZGZmmnRsRVEKrNPk6zrcv38/I0aMYMaMGfTp0wd3d3dWrlzJF198Ydb7UBSlwLELO6etrW2BbTqdzqxz5T9P3nO3aNGC8PBw/vzzT7Zu3cqwYcPo2bMnP//8M9WqVePs2bNs2bKFrVu38swzz/DZZ5+xa9euAvUqT+Xa0rR7924GDhxI5cqVTcopMXbsWMNgt7yPhg0bGsqEhIQUWiY9Pb2U340QQojb0mjULrLyeJgwnimvYcOGYW1tzfLly/nf//7HuHHjDAHAnj17GDx4MKNGjaJp06YEBwdz/vx5k4/doEEDIiIiuHYtN4AMDQ01KvP3338TFBTE9OnTadWqFbVr1y5wR5+dnR3Z2dm3PdexY8dISUkxOraVlRV16tQxuc7FcXNzo3Llyuzdu9do/b59+6hfv75RueHDh/Pdd9+xatUq1q5dS1xcHACOjo4MGjSIefPmsXPnTkJDQzlx4kSJ1K+klGtLU0pKCk2bNmXcuHE88sgjty0/d+5cPvnkE8NyVlYWTZs25dFHHzUq5+bmZtTkCODg4FAylRZCCHFfcHFxYfjw4bz55pskJCQwduxYw7ZatWqxdu1a9u3bh7u7O59++inR0dFGAUJxevbsSd26dXniiSf44osvSExMZPr06UZlatWqRUREBCtXrqR169b88ccfrF+/3qhMcHAw4eHhHDt2jKpVq+Lq6log1cDjjz/Ou+++y5gxY3jvvfe4efMmzz//PKNHjzaMZyoJr776Ku+++y41a9akWbNmLF26lGPHjvHTTz8BMHv2bAICAmjWrBlWVlasWbMGf39/PDw8CAkJITs7m7Zt2+Lk5MSPP/6Io6Oj0bgnS1CuLU39+vXjww8/ZMiQISaVd3d3x9/f3/A4dOgQt27dYty4cUblNBqNUbniBsgJIYQQRZkwYQK3bt2iZ8+eBAYGGta//fbbtGjRgj59+tC9e3d8fX0ZPHiwyce1srJi/fr1ZGRk0KZNGyZOnMhHH31kVGbw4MG89NJLPPfcczRr1ox9+/bx9ttvG5V55JFH6Nu3L926dcPHx6fQtAdOTk789ddfxMXF0bp1a4YOHUqPHj2YP3++mZ9G8V544QVefvllXn75ZRo3bsymTZv49ddfqV27NqAGobNmzaJVq1a0bt2aS5cusXHjRqysrPDw8OC7776jY8eONGnShG3btvHbb7/h7e1donW8WxqlsI7VcqDRaFi/fj0PPfSQyfsMHDiQjIwMo8FkISEhTJw4kSpVqpCdnU2zZs344IMPaN68eZHHycjIICMjw7CcmJhItWrViIqKsrgLdr/RarVs2bKFXr16WVS/9v1KrofluBeuRXp6OleuXCE4OLhCt/YrikJSUhKurq5Fjh0SJSc9PZ1Lly5RrVq1At+r2NhYAgICSEhIwM3NrcTPfc8OBI+KiuLPP/9k+fLlRuvr1atHSEgIjRs3JjExkblz59KxY0fCwsIM0W5+M2fOZMaMGQXW79ixo8yyt4riFZb0TZQfuR6Ww5Kvhf5W+OTkZJMHSN/LkpKSyrsK94XMzEzS0tLYvXt3gTvsUlNLNwfYPdvSNHPmTL744guuXbuGnZ1dkeV0Oh0tWrSgc+fOzJs3r9Ay0tJkue6F/6bvJ3I9LMe9cC2kpUmUBmlpMpOiKCxZsoTRo0cXGzCB2m/cunVrzp07V2QZe3v7QufosbW1tdhfRvcbuRaWRa6H5bDka5GdnY1Go8HKygorq4qbFlB/W77+vYrSZWVlhUajKfS7X9o/C/fk1d21axfnz59nwoQJty2rKArHjh0jICCgDGomhBBCiIqqXFuakpOTjfJa6G+b9PLyIjAwkGnTphEZGckPP/xgtN/ixYtp27YtjRo1KnDMGTNm0K5dO2rXrk1iYiLz5s3j2LFjLFiwoNTfjxBCiIIsZBSIqCDK8/tUrkHToUOHDHP6AEydOhWAMWPGEBISQlRUVIF08QkJCaxdu5a5c+cWesz4+HgmTZpEdHQ07u7uNG/enN27d1vkHDZCCFGR6btKUlNTcXR0LOfaiIpCf1NB3ilfykq5Bk1du3YtNmLMO/uynru7e7Gj42fPns3s2bNLonpCCCHugrW1NR4eHob5y5ycnCrkQGmdTkdmZibp6ekypqmU6XQ6bt68iZOTEzY2ZR/C3JMDwYUQQtwb9MmFTZ349V6kKAppaWk4OjpWyKDQ0lhZWREYGFgun7UETUIIIUqNRqMhICAAX19ftFpteVenVGi1Wnbv3k3nzp0t9k7GisTOzq7cWvQkaBJCCFHqrK2ty2UMSlmwtrYmKysLBwcHCZoqOOl8FUIIIYQwgQRNQgghhBAmkKBJCCGEEMIEEjQJIYQQQphAgiYhhBBCCBNI0CSEEEIIYQIJmoQQQgghTCBBkxBCCCGECSRoEkIIIYQwgQRNQgghhBAmkKBJCCGEEMIEEjQJIYQQQphAgiYhhBBCCBNI0CSEEEIIYQIJmoQQQgghTCBBkxBCCCGECSRoEkIIIYQwgQRNQgghhBAmkKBJCCGEEMIEEjQJIYQQQphAgiYhhBBCCBNI0CSEEEIIYQIJmoQQQgghTCBBkxBCCCGECSRoEkIIIYQwgQRNQgghhBAmkKBJCCGEEMIEEjQJIYQQQphAgiYhhBBCCBNI0CSEEEIIYQIJmoQQQgghTCBBkxBCCCGECco1aNq9ezcDBw6kcuXKaDQafvnll2LL79y5E41GU+Bx5swZo3Jr166lQYMG2Nvb06BBA9avX1+K70IIIYQQ94NyDZpSUlJo2rQp8+fPN2u/s2fPEhUVZXjUrl3bsC00NJThw4czevRowsLCGD16NMOGDePAgQMlXX0hhBBC3EdsyvPk/fr1o1+/fmbv5+vri4eHR6Hb5syZQ69evZg2bRoA06ZNY9euXcyZM4cVK1bcTXWFEEIIcR+7J8c0NW/enICAAHr06MGOHTuMtoWGhtK7d2+jdX369GHfvn1lWUUhhBBCVDDl2tJkroCAAL799ltatmxJRkYGP/74Iz169GDnzp107twZgOjoaPz8/Iz28/PzIzo6usjjZmRkkJGRYVhOTEwEICYpFTc3t1J4J8JUWq3W6FmUL7kelkOuheWQa2E5Svsa3FNBU926dalbt65huX379ly5coXPP//cEDQBaDQao/0URSmwLq+ZM2cyY8aMAuuX/fk3rSo7lkDNxd3asmVLeVdB5CHXw3LItbAcci3KX2pqaqke/54KmgrTrl07li1bZlj29/cv0Kp048aNAq1PeU2bNo2pU6calhMTE6lWrRpBtevRv0vDkq+0MJlWq2XLli306tULW1vb8q7OfU+uh+WQa2E55FpYjtjY2FI9/j0fNB09epSAgADDcvv27dmyZQsvvfSSYd3mzZvp0KFDkcewt7fH3t6+wPrkDEV+ACyEra2tXAsLItfDcsi1sBxyLcpfaX/+5Ro0JScnc/78ecNyeHg4x44dw8vLi8DAQKZNm0ZkZCQ//PADoN4ZFxwcTMOGDcnMzGTZsmWsXbuWtWvXGo7x4osv0rlzZ2bNmsXgwYPZsGEDW7duZe/evWbXLyFN+qeFEEIIoSrXoOnQoUN069bNsKzvIhszZgwhISFERUURERFh2J6Zmckrr7xCZGQkjo6ONGzYkD/++IP+/fsbynTo0IGVK1fy1ltv8fbbb1OzZk1WrVpF27Ztza7frVQJmoQQQgihKtegqWvXriiKUuT2kJAQo+XXXnuN11577bbHHTp0KEOHDr3b6pGQlnnXxxBCCCFExXBP5mkqK3ZJV8u7CkIIIYSwEBI0FcMr9UJ5V0EIIYQQFkKCpmI4pJfurYtCCCGEuHdI0FQMp+xbxY65EkIIIcT9w6SB4EOGDDH7wIsWLcLX19fs/SyJpy6e1MxsnO3v+XRWQgghhLhLJkUDv/zyC8OGDcPR0bQpRZYvX05ycvI9HzRV0iRyKzVTgiYhhBBCmJ5yYN68eSYHQT///PMdV8iSVNIkEJ+qpapneddECCGEEOXNpDFNO3bswMvLy+SD/vnnn1SpUuWOK2UpvHNamoQQQgghTGpp6tKli1kHfeCBB+6oMpamEgmcS5GgSQghhBB3mBFcq9USHR1NamoqPj4+ZrVC3UvsNVmkJsYC936rmRBCCCHujskpB5KTk/nmm2/o2rUr7u7uBAcH06BBA3x8fAgKCuLJJ5/k4MGDpVnXcpEZH13eVRBCCCGEBTApaJo9ezbBwcF89913dO/enXXr1nHs2DHOnj1LaGgo7777LllZWfTq1Yu+ffty7ty50q53mdElXS/vKgghhBDCApjUPbdv3z527NhB48aNC93epk0bxo8fz6JFi1i8eDG7du2idu3aJVrRcpMiQZMQQgghTAya1qxZY9LB7O3teeaZZ+6qQpbGNjWmvKsghBBCCAtg8pimixcv3pdTithn3DRvB50OMlNLpzJCCCGEKDcmB021a9fm5s3cAGL48OFcv17xu66cM82ctHflSPiyHqTIZL9CCCFERWJy0JS/lWnjxo2kpKSUeIUsjWtWnOmFFQXCd0F6Atz4t/QqJYQQQogyZ3LQdL/yUuLJzNKZVjg9HrQ5XXPJN0qtTkIIIYQoeyYHTRqNBo1GU2BdReejiSc+zcSs4InXcl+nmDkWSgghhBAWzeSM4IqiMHbsWOzt7QFIT09n8uTJODs7G5Vbt25dydawnHmRyIXkdHxdHW5fWIImIYQQosIyOWgaM2aM0fKoUaNKvDKWJhsrrDUKyXHREOBx+x0SI3NfS/ecEEIIUaGYHDQtXbq0NOthkRI0bniSQNqtKKDe7XeQliYhhBCiwpKB4MVItvYEQJsQZdoO0tIkhBBCVFgmB01paWnMnDmTN954g6goE4OIe1yanRcAukQT81El5AmaUiSTuBBCCFGRmBw0TZgwgfPnz+Pt7U3Pnj1Ls04WI8NeDZo0KSa2Ghl1z91Q8zYJIYQQokIweUzTzp072bJlCw0bNmT69OncuHEDX1/f0qxbuctyrASpYJN6B0FTVjpkJIGDW+lUTgghhBBlyuSgqUuXLsydO5c6deoQGBhY4QMmAMWpEsSCfYYJXW3piZCZpL62toPsTHUwuARNQgghRIVgcvfc999/T1BQENevX2fbtm2lWSeLYeWqBoYmzT+nb2Wydwe3KupruYNOCCGEqDBMbmlydnZm+vTppVkXi2Pn5geAa7YJ88/p75xzqwz2rnArXO6gE0IIISoQk4Om+5G9uxo0eepu3b6wvqXJvQrY5GQPN3UAuRBCCCEsnkndc5MnT+bKlSsmHXDVqlX89NNPd1UpS+Hk5Q+AK6noMlKLL6wPmtwqg7OP+jpZuueEEEKIisKkliYfHx8aNWpEhw4dGDRoEK1ataJy5co4ODhw69YtTp06xd69e1m5ciVVqlTh22+/Le16lwk3d28yFFvsNVqS46JwC6hZdGFD91wVUHTqaxnTJIQQQlQYJgVNH3zwAc8//zyLFy9m0aJFnDx50mi7q6srPXv25Pvvv6d3796lUtHyYG9rTQweVOEmKbGRtwma8rQ0ZWWor6V7TgghhKgwTB7T5Ovry7Rp05g2bRrx8fFcvnyZtLQ0KlWqRM2aNdFoNKVZz3ITb+VJFeUmabeuFV8wb9CUkay+lu45IYQQosK4o4HgHh4eeHh4lHBVLFOijRdoQZsQfZuCebrn0nIGjktLkxBCCFFhlOuEvbt372bgwIFUrlwZjUbDL7/8Umz5devW0atXL3x8fHBzc6N9+/b89ddfRmVCQkLQaDQFHunp6XdUx1Q7b+A2889lpkB6vPo670BwmX9OCCGEqDDKNWhKSUmhadOmzJ8/36Tyu3fvplevXmzcuJHDhw/TrVs3Bg4cyNGjR43Kubm5ERUVZfRwcHC4ozpmOFTKqWwxrUaJORMY27mAvVtu0JSRCNo7C9aEEEIIYVnKNU9Tv3796Nevn8nl58yZY7T88ccfs2HDBn777TeaN29uWK/RaPD39y+ROmY5+UDsbeafy5vYUqMBB/c8U6ncAI/AEqmLEEIIIcpPubY03S2dTkdSUhJeXl5G65OTkwkKCqJq1ao8+OCDBVqizKE4qwkui51/Lm/QBGrg5JwzN5+kHRBCCCEqhDtqacrKymLnzp1cuHCBkSNH4urqyrVr13Bzc8PFxaWk61ikL774gpSUFIYNG2ZYV69ePUJCQmjcuDGJiYnMnTuXjh07EhYWRu3atQs9TkZGBhkZGYblxMREALRaraGrzTkzVl0uhNWtK1gDOpcAsnPKWDt5Y5V4layEKBTfwvcTt6f/zIv67EXZkuthOeRaWA65FpajtK+B2UHT5cuX6du3LxEREWRkZNCrVy9cXV359NNPSU9PZ9GiRaVRzwJWrFjBe++9x4YNG/D19TWsb9euHe3atTMsd+zYkRYtWvDVV18xb968Qo81c+ZMZsyYUWD9jh07uJagZgJ3zYpj4x9/qK1I+TS5sp/qwLnrKZzZuBGAtqngD5zYv52Ic9l38U4FwJYtW8q7CiIPuR6WQ66F5ZBrUf5SU28ze8ddMjtoevHFF2nVqhVhYWF4e3sb1j/88MNMnDixRCtXlFWrVjFhwgTWrFlDz549iy1rZWVF69atOXfuXJFlpk2bxtSpUw3LiYmJVKtWjW7dunHmRhqsAzu09O/xgDpeKR/rVcsgBmq16EyNFv3Vdb9tguNhNKnhT6OO/e/wnQqtVsuWLVvo1asXtra25V2d+55cD8sh18JyyLWwHLGxsaV6fLODpr179/L3339jZ2dntD4oKIjIyMgSq1hRVqxYwfjx41mxYgUDBgy4bXlFUTh27BiNGzcusoy9vT329vYF1tva2uLl5UCi4oSbJhXb9DhwrVTwAMnq3XPWnoFY639g3NSxUNZpcbnrxB2ztbWVX0YWRK6H5ZBrYTnkWpS/0v78zQ6adDod2dkFu5uuXr2Kq6urWcdKTk7m/PnzhuXw8HCOHTuGl5cXgYGBTJs2jcjISH744QdADZieeOIJ5s6dS7t27YiOVhNOOjo64u6utgDNmDGDdu3aUbt2bRITE5k3bx7Hjh1jwYIF5r5VADyd7LipuOOmSYXk6+BTp2ChvNnA9Qy5miTBpRBCCFERmH33XK9evYxu/ddoNCQnJ/Puu+/Sv7953VCHDh2iefPmhnQBU6dOpXnz5rzzzjsAREVFERERYSj/zTffkJWVxbPPPktAQIDh8eKLLxrKxMfHM2nSJOrXr0/v3r2JjIxk9+7dtGnTxty3CoCHky038QAgMz6qYAFtOqTmNAe6Vcldr797LlmCJiGEEKIiMLul6csvv6R79+40aNCA9PR0Ro4cyblz56hUqRIrVqww61hdu3ZFUZQit4eEhBgt79y587bHnD17NrNnzzarHsVxsbchRvEAIO1WFHb5CyTltDLZOIKjZ54d9S1NknJACCGEqAjMDpqqVKnCsWPHWLlyJYcPH0an0zFhwgQef/xxHB0dS6OO5Uqj0ZBk6wU6yEwopKUpb9dc3jvrJE+TEEIIUaGYFTRptVrq1q3L77//zrhx4xg3blxp1cuipNp6QwbokgqZf66w8UyQO6YpNQ6ys8C6XJOvCyGEEOIumTWmydbWloyMDDSF5CqqyNJz5p/TFDY+yZANvIrxeicv0FgBCqTKxL1CCCHEvc7sgeDPP/88s2bNIisrqzTqY5GyndRWI5u0QrraimppsrIGp5z0BDIYXAghhLjnmd1ndODAAbZt28bmzZtp3Lgxzs7ORtvXrVtXYpWzFIb559ILaTEqKmgCcPFVUw7IuCYhhBDinmd20OTh4cEjjzxSGnWxWFau/gA4am+BLlttRdIrqnsOwDmnpUmCJiGEEOKeZ3bQtHTp0tKoh0Vz8PAhW9FgrdFBSgy4+uVuLK6lSXI1CSGEEBWG2WOa7kfuzo7E4aYuJOe5gy4rMzcgKqylyUWfdsDEoCk9UX0IIYQQwuKY3dJUvXr1Yu+eu3jx4l1VyBJ5OtlxTfHGR5MAJ3+GgCbqhqQoQAFrO3DyLrijoXvOhLvnsjLg2y5qhvEXjoBtxct5JYQQQtzLzA6apkyZYrSs1Wo5evQomzZt4tVXXy2pelkUTydbvs4axCK7OfD3XAjuDLV75nbNuQaAVSGNduZ0z53dCHE5AefNM1C5eYnUXQghhBAlw+ygKe88b3ktWLCAQ4cO3XWFLJGHkx2bdG1YSW9GsBnWT4LJe4sfBA7mdc8d/Sn39c2zEjQJIYQQFqbExjT169ePtWvXltThLIqnky0A76aPRPFrrE7Qu3YixOdMJlzYIHDIzQp+u+65xCi4sC13+ebZu6yxEEIIIUpaiQVNP//8M15eXiV1OIvi7qgGTRnYcevB78DOBS7/Dfvm5RQooqXJOc+kvTpd0Sc4vhKUPNslaBJCCCEsjtndc82bNzcaCK4oCtHR0dy8eZOFCxeWaOUshY21Fe6OtiSkaYmzr4rXwLmwdgKk3VILFNU9pw+adFmQHq9OrZKfouR2zTV6BE6uhRgJmoQQQghLY3bQNHjwYKOgycrKCh8fH7p27Uq9evVKtHKWxNNJDZoi4lKp1XgoXNoDh0PUjUV1z9nYgYOHGjAl3yg8aLp6CGLPgY0jdHlDDZriLqp309nYl9K7EUIIIYS5zA6a3nvvvVKohuVrUtWDS7GpvLw6jGUT29Kw7ydw7Rhc/xcCmha9o4svpMdz9uJF6voWElQeW6Y+NxgMlWqDvRtkJELsBfBrUCrvRQghhBDmM3tMk7W1NTduFLwbLDY2Fmtr60L2qBg+eKgRTat5cCtVy8jvDnDyRiZM2AIvnwGPwCL3y3ZUczXN/20f528kGW/MTIWTOXP1NX8cNBrwqasu3zxTGm9DCCGEEHfI7KBJUZRC12dkZGBnZ3fXFbJU7o62/DihDc0DPUhI0zLyu/2ERaXmJrAswpVMFwC8SWDjiWjjjWf+UFuVPAIh6AF1XaWcoCnmv5J+C0IIIYS4CyZ3z82bp94pptFo+P7773FxcTFsy87OZvfu3RV6TBOAm4MtP4xvw7ilBzl0+Rajvj/ADxPa0DzQs9Dy2TqFw7E2BAOVNAlsPBnNCz1q5xbQd801HZmbHFNamoQQQgiLZHLQNHv2bEBtaVq0aJFRV5ydnR3BwcEsWrSo5GtoYVwdbPnf+DaMCznIP+FxjF78D78+15EaPi4Fym45FU14mjPYgo8mkVNRiVyJS6WalxPEX4GLu9SCzR7L3ckQNElLkxBCCGFJTO6eCw8PJzw8nC5duhAWFmZYDg8P5+zZs/z111+0bdu2NOtqMZztbQgZ15pWQZ4kZ2TxxroT6HQFuy2/2xNObM5Ev7Wc0gD469+cLrqwlYACwZ3AMzh3J33QFHsOsrMKnjxbCz88BGvGqukKhBBCCFEmzB7TtGPHDjw9C++Oup842dkwe3gznOys+Sc8jp/+iTDafvjyLQ5fvkW8Rv2sAu2TAdh0Mlq94+7g92rBZo8bH9g9UE0/kJ0J8ZcLnvjKP3BxB/y7HpKiC24XQgghRKkwO+UAwNWrV/n111+JiIggMzPTaNuXX35ZIhW7F1TzcuK1PnV577dTfLLxNN3r+VLFwxGA7/eok+82qF0TwsGTeACCrm5A+e5/aLLSwLM6NBhkfFArKzX1QPRxdVyTd03j7Rd35L6OPgFuAaX19oQQQgiRh9lB07Zt2xg0aBDVq1fn7NmzNGrUiEuXLqEoCi1atCiNOlq0J9oH89vxKA5fvsX09SdYOrY1EXGphm64Ae2bQDjYpMbwrcf/6J3+F2QBNXvAkO/AzrngQX3q5gZN9QYYb7uwPfd19HGo07v03pwQQgghDMzunps2bRovv/wyJ0+exMHBgbVr13LlyhW6dOnCo48+Whp1tGhWVhpmPdIEO2srdp69yS/HIlmyNxydAl3q+FAjuLpaMCud3ul/oVM0/Oz2BDz+Mzh7F37QogaDp92Ca0dzl6OPF1+566cgIfLO3pgQQgghjJgdNJ0+fZoxY8YAYGNjQ1paGi4uLrz//vvMmjWrxCt4L6jl68KLPdVUAjN+O8XqQ1cBeLJTDbUlyU69sy7b0ZvR2jd4I6YfCenZRR+wUhFpB8J3qxP7anLuXIw+UfQx4iPg267wv4EyYFwIIYQoAWYHTc7OzmRkZABQuXJlLly4YNgWExNTcjW7x0zqXIMGAW7Ep2pJ02ZTz9+VjrVyWpI6vwoNh2D99F5ifDqQpVPYduZ60Qfzycl3FXMOdLrc9RdyxjM1GqI+x12E9MTCjxG+G7IzIO4C3Dh1d29OCCGEEOYHTe3atePvv/8GYMCAAbz88st89NFHjB8/nnbt2pV4Be8VttZWfDq0CdZW6mTGkzrXyJ3Y+IEp8OhScKtMn0b+QM5ddEXxqg5WNqBNgcSruev1g8AbDQW3Kurr6/8WfoyI/bmvL+wovIwQQgghTGZ20PTll18a8jG999579OrVi1WrVhEUFMTixYtLvIL3kkZV3PlyWFOe61aLQU0rF1qmT0M/AHafu0lqZiF5mACsbcG7lvpaP64pLhxuXVKDqeCO4N9YXV9UF92VA7mvL0rQJIQQQtwts+6ey87O5sqVKzRp0gQAJycnFi5cWCoVu1cNblal2O0NAtyo5uXIlbg0dv93k76NikgZUKmOOqbp5hmo3TM38KnaBuxd1aDpv00QHVZw39Q447nrLv0NWRlgY3+H70oIIYQQZrU0WVtb06dPH+Lj40upOhWfRqOhb0MTuugM45rOqs/6Lraa3dRnfzVwLbSl6co/6rN3LXDxh6w04+46IYQQQpjN7O65xo0bc/HixdKoy32jb864pm2nb5CuLeIuOkPagbOgy4bwnHnqanZXn/XdczdOq1Or5HUlJ0AKbAc1uqqvpYtOCCGEuCtmB00fffQRr7zyCr///jtRUVEkJiYaPcTtNa/mSWV3B5IysvjpQEThhfIGTdeOQnoCOLhD5ebqes9gsHdTp1u5edZ434ic8UzV2uW2TMlgcCGEEOKumB009e3bl7CwMAYNGkTVqlXx9PTE09MTDw8PmZPORFZWGp7voeZ1WrDjPEnp2oKFvGsBGkiPh+Or1HXVO4NVTo4mjabwweBZmXDtiPo6b0tTVJg61kkIIYQQd8TsaVR27JAWi5LwaMuqfLf7IhdjUvh+Tzgv9apjXMDWUW1NuhUOx5ar62p0My7j3wQu/52TGfwxdV1UGGSlg6OXGnhpNODbQM3VdHFnbo4nIYQQQpjF7KCpS5cupVGP+46NtRUv967Ls8uP8P2ei4xuH0Qll3x3t/nUVYOmzGR1uWY3UjKyGLX4AG4Otixt1khtKszb0qRPNVCtrRowgRps3TilzlsnQZMQQghxR8zungPYs2cPo0aNokOHDkRGqnOb/fjjj+zdu9es4+zevZuBAwdSuXJlNBoNv/zyy2332bVrFy1btsTBwYEaNWqwaNGiAmXWrl1LgwYNsLe3p0GDBqxfv96sepWVfo38aVzFnZTMbBbsOF+wgH5cE4BHEHjV4IvN/3E0Ip5d/91kX0pOLqjo47lTpRgGgbfN3Vc/ePziTplSRQghhLhDZgdNa9eupU+fPjg6OnLkyBHDlCpJSUl8/PHHZh0rJSWFpk2bMn/+fJPKh4eH079/fzp16sTRo0d58803eeGFF1i7dq2hTGhoKMOHD2f06NGEhYUxevRohg0bxoEDB4o5cvmwstLwWl81MPppfwRXb6UaF6iUJ2iq2Y0TVxMI2RduWPXFUQ2Kla06SDw+Qg2I8g4C1wvqANZ2kHAFYnOnvRFCCCGE6cwOmj788EMWLVrEd999h62trWF9hw4dOHLkiFnH6tevHx9++CFDhpjWZbRo0SICAwOZM2cO9evXZ+LEiYwfP57PP//cUGbOnDn06tWLadOmUa9ePaZNm0aPHj2YM2eOWXUrKw/UqkSHmt5kZuuYs/Wc8UZ9riYgO7gr09YfR6dA93q+ONhacfRaKinuOZnDo0+oXXkpN8DKNvcuOwA7J7W7DiT1gBBCCHGHzB7TdPbsWTp37lxgvZubW6knvQwNDaV3795G6/r06cPixYvRarXY2toSGhrKSy+9VKBMcUFTRkaGocUMMKRO0Gq1aLWF3NlWwqb2rMW+C7GsO3KV8e0Dqe3nom7wqI6NrRMoOpZdD+RkZDRuDjZ8NLg+X+8K54f9ERxMr0o3TpMdeRQlNR4bQBfQlGysIU/drYK7YH1pD7pzW8luPrbU31NJ0X/+ZXEdxO3J9bAcci0sh1wLy1Ha18DsoCkgIIDz588THBxstH7v3r3UqFGjpOpVqOjoaPz8/IzW+fn5kZWVRUxMDAEBAUWWiY4uOvv2zJkzmTFjRoH1O3bswMnJqWQqfxtNvKw4HmfFy8v2Mr6ODruczAKe1V8hMRM+2h4FaOhXOYN/dm+jeiZYaazZlRhAN1u4cWIbGTYeBAMXtT78u3Gj0fE9Um3oAmRf2Mmff/yKojH70perLVu2lHcVRB5yPSyHXAvLIdei/KWmpt6+0F0w+y/nU089xYsvvsiSJUvQaDRcu3aN0NBQXnnlFd55553SqKMRjf6OsBxKzsDmvOsLK5N/XV7Tpk1j6tSphuXExESqVatGt27d8Pb2Lolq31adVskMmL+P0/FWzDrlwJMPBDOidVUcbfsx+adjZOpu0irIg/fHtMbKSn0vJ5STnDoWBIC/chOs0gEI7jSCoHr9jU+gy0aZMxfbtFv0b+KHUq0t9wKtVsuWLVvo1auXUXewKB9yPSyHXAvLIdfCcsTGxpbq8c0Oml577TUSEhLo1q0b6enpdO7cGXt7e1555RWee+650qijgb+/f4EWoxs3bmBjY2MIbooqk7/1KS97e3vs7QtOZmtra1tmPwD1q3gyf2QLPt54mqu30pi56T++3XOJHvV92X72JrbWGj55pAn29naGfZ7pVpshx9S77jSJVw3rbap3gAL1toXqXeDUL9hc3gM1HiiLt1ViyvJaiNuT62E55FpYDrkW5a+0P/87Sjnw0UcfERMTwz///MP+/fu5efMmH3zwQUnXrYD27dsXaP7cvHkzrVq1MnxQRZXp0KFDqdfvbvVvHMCOV7oy65HGVPNyJDYlk9WH1GDo6S41qeXralS+lq8LHRrU4LLON3elVw1w8aVQhilVtpdG9YUQQogK7Y4Htjg5OeHn54dGo8HFxeWOjpGcnMz587n5icLDwzl27BheXl4EBgYybdo0IiMj+eGHHwCYPHky8+fPZ+rUqTz55JOEhoayePFiVqxYYTjGiy++SOfOnZk1axaDBw9mw4YNbN261ewcUuXF1tqK4a0DGdKiKr8cjeS7PRfxdLLjmW61Ci3/TLea/PtfMEHcUFcU1+1Wq6f6fPUgJESCe5USrr0QQghRcZnd0pSVlcXbb7+Nu7s7wcHBBAUF4e7uzltvvWX2qPVDhw7RvHlzmjdXb4+fOnUqzZs3N4yNioqKIiIid0Lb6tWrs3HjRnbu3EmzZs344IMPmDdvHo888oihTIcOHVi5ciVLly6lSZMmhISEsGrVKtq2vTfG8OjZWlvxaKtqbH6pC6ueao+DrXWh5ZpU9SDVs37uiuKCJveqOfmbFDj1S4nWVwghhKjozG5peu6551i/fj2ffvop7du3B9RUAO+99x4xMTGFZuguSteuXQ0DuQsTEhJSYF2XLl1umw9q6NChDB061OR63OsatuwEO/4HwEnr+jQqrnCjR9Ss4SfXQvtny6R+QgghREVgdtC0YsUKVq5cSb9+/QzrmjRpQmBgICNGjDAraBIlo17LrqTvdORGtgsTNybya610fF0dCi/c8CHY9DpEHoa4cPCqXqZ1FUIIIe5VZnfPOTg4FMjRBBAcHIydnV3BHUSp07j4oJu0i9fdZhGdpOW55UfRZusKL+ziC9VzkpOeXFt4GSGEEEIUYHbQ9Oyzz/LBBx8YZdDOyMjgo48+KvWUA6JoTgF1+XBMH1zsbfgnPI5PN50punCjnDFgJ9eVTeWEEEKICsDs7rmjR4+ybds2qlatStOmTQEICwsjMzOTHj16GM0jt26d/FEuSzV9XPj80SZMXnaE7/aE0zzQk/6NAwoWrD8Qfp8KN/6FG6fBt37BMpZCUXDMuKlORiyEEEKUI7ODJg8PD6O71QCqVatWYhUSd6dvowCe6lyDb3Zf5NU1YdTxcymQ3wlHTzX9wH9/qq1N3aeXT2VNYHXwG3qfeousYA20mVDe1RFCCHEfMztoWrp0aWnUQ5SgV/vUJexqPPsvxvH0siP89vwDBVMWNHokJ2j6Gbq9CcVMM1OeNJfU/FpWEfskaBJCCFGu7igjuLBsNtZWfPVYC3xc7Tl3I5lP/ixkfFPdfmDjCHEXIepYmdfRVJrY/9TnmP/KuSZCCCHud2YHTbGxsTz77LM0aNCASpUq4eXlZfQQlsHH1Z5PhzYBIGTfJfacu2lcwN4F6vRRX+e/iy47C85vhaTrZVDTYmjT4dYl9XXsedAVcUegEEIIUQbM7p4bNWoUFy5cYMKECYZpVIRl6lbXl9Htgvhx/2VeWRPGX1M64+GUJy1E46FqZvCT66Hn+2oX3fmtsPktuHkGKtWFp/eB9R3PtnN34i6gUdRASaNNhcRI8JDxc0IIIcqH2X8N9+7dy969ew13zgnL9mb/+vx9PoaLMSlM/+Uk8x9rnhvo1uoFdq6QeBWO/A9O/2o8mW/MWQhbDi2eKJ/K3zxrvBzznwRNQgghyo3Z3XP16tUjLS2tNOoiSoGjnTWzhzfDxkrDH8ej2HDsWu5GWweo/6D6+vcpasBkZQvtn4Ou09T1Oz8BbTld78KCJiGEEKKcmB00LVy4kOnTp7Nr1y5iY2NJTEw0egjL07SaBy/0qA3A2xtOEhmfJwhqMjz3dYPB8Nw/0Ocj6DgF3KupXWL/fGf6yZKi4efxEL7n7iseowZNWqucKWHyB1FCCCFEGTI7aPLw8CAhIYHu3bvj6+uLp6cnnp6eeHh44OnpWRp1FCXgma41aR7oQVJ6Fs8tP0JyRpa6oWY3eGwVTNwOw34ArxrqeluH3NamPV9AWrxpJ9o1Sx1Yvm3G3Vc6J0i67t5MXY45d/fHFEIIIe6Q2WOaHn/8cezs7Fi+fLkMBL+H2FhbMXtYMwbO38vRiHieWHyAkPFtcHOwhbp9C9+p6QjYN08dFL7vK+jxdvEnSU+AsFXq68gj6rKD+51VODtLvWMOuObeiqq39htanoQQQojyYHbQdPLkSY4ePUrdunVLoz6iFAVXcuaniW0ZvfgfjkTEM/r7A/wwvi3uTraF72BlDd3fhlWPw/6F0GYSuPoVfYJjK0Cbor5WsuHS31Cv/51V9tYlyM5EsXHkpltjdV3KTUiNAydJbSGEEKLsmd0916pVK65cuVIadRFloElVD5Y/2RZPJ1vCriYw8vv93ErJLHqHegOgamvQpsLuT4supyhw8Hv1tZO3+hy+684rqm9V8q5FlrUjimvlnPXSRSeEEKJ8mB00Pf/887z44ouEhIRw+PBhjh8/bvQQlq9hZXdWTGqHt7Md/15L5LHv9hOTnFF4YY0Ger6nvj4comYQL8zFnRB7Tk1h0OuD3HV36qaaxVzxUVs0lUp11PXSRSeEEKKcmB00DR8+nNOnTzN+/Hhat25Ns2bNaN68ueFZ3Bvq+buxclI7fFztOROdxGPf7udGYnrhhYMfUCf41WXBpjfVVqX89K1MTUeoU7SgUQOfxKg7q+BNNb2A4q0GS7lBk6QdEEIIUT7MDprCw8MLPC5evGh4FveO2n6urJrUDn83B87dSGb4t/u5Fl9ETqae76k5nP77E/Z8brwt/gqc3QhAStNxLDgQR7qvOoXLHXfR6VuaKuWMnfOulbNegiYhhBDlw+ygKSgoqNiHuLfU8HFh9VPtqeLhSHhMCsO+CeVKXGrBgv6NYcAX6uvtH8HZTbnbDi0BRYdSvTNTtqfx2V9n2ZyaE+zcSRedTmcYu6RvYZKWJiGEEOXN7KAJ4Mcff6Rjx45UrlyZy5cvAzBnzhw2bNhQopUTZSPQ24nVk9sT7O3E1VtpDPsmlIs3kwsWbDkGWk8EFFg7UW31ycqAIz8AsN11MFtOqZP8rrmV0zJ0cWfh3XnFSbyq3oVnZQOewQAo3mpyTuIvqxP5CiGEEGXM7KDp66+/ZurUqfTv35/4+Hiys7MBNenlnDlzSrp+ooxU8XBk1VPtqenjTFRCOsO/3c+560kFC/aZCYEdIDMJVj4Gh/8HqTFkOgfw7GE1HYGDrRX/ZNch28oOkqLMv+NN3wXnXQusc9IhOPuqOZ8UnSF/kxBCCFGWzA6avvrqK7777jumT5+OtbW1YX2rVq04ceJEiVZOlC0/NwdWPdWeev6u3EzKYPi3+zl1Ld/UODZ2auZwt6pq8PLnqwD8L7M76dlW9Gnox1Oda5KBHWftG6n7mNtFlzOeCX2XHKh38enHNxXWRaco0gIlhBCiVN3RQPDC7pKzt7cnJSWlRColyk8lF3tWTmpH4yruxKVk8th3+zl+Nd64kIsPjFgGNuqccFnY8E3SA1TxcOTTR5rSp6E/ABuT73Bckz6tgE+9fJUrZlzT3i/hIz8I323euYQQQggTmR00Va9enWPHjhVY/+eff9KgQYOSqJMoZx5Odiyb2JbmgR4kpGl5/LsDHL58y7hQ5eYwaD46jTUrsroSb+XB/JHNcXeypX6AK1U9HdmV1VAte2mPOi2KqfTdcz75ss77FBE0adPg77nq6zN/mH4eIYQQwgwmB03vv/8+qampvPrqqzz77LOsWrUKRVH4559/+Oijj3jzzTd59dVXS7Ouogy5O9ry44S2tAn2IikjiycWH+DAxVgAbiSms/7oVV4+U4e22m94N2ssr/etR/NAdcJmjUZDn4b+/KsEk2rlAhmJcO2oaSdWlNzuufxBk76lKX/agZPr1HnuAKKli1gIIUTpMHnuuRkzZjB58mTGjRtHVlYWr732GqmpqYwcOZIqVaowd+5cRowYUZp1FWXMxd6GkPGtefKHQ/x9PpYxS/8h0MuJ/67nvbPOiV4N/JjwQHWjffs09Gfx3nD26RrSkwMQvhOqtb79SVNuQno8oMnNzaSnD5piz4EuW50bD9SUB3rRJ9SUBVZ3dGOoEEIIUSST/7IoeW4bf/LJJ7l8+TI3btwgOjqaK1euMGHChFKpoChfTnY2LB7Tmq51fUjX6vjvejIaDTSu4s7kLjVZNqEti0a1xMpKY7RfyyBPvJ3t2KnN6bK9aGKSS30rk2cw2Doab/MMBms7yEqHhJz5D6PCIPKQmnjTylZt1Yq/fMfvVwghhCiKyS1NoHa75FWpUqUSrYywTA621nwzuiWrD17B28We9jW88XS2K3YfaysNPev7sfdwzh10Vw5AZgrYORd/spv6QeB1C26zslZbn26cUrvoPIPh0FJ1W4NB6t18UWFqa5NX9YL7CyGEEHfBrKCpR48e2NgUv8uRI0fuqkLCMtnbWDO6fbBZ+/Rp5MeqQ/5EUYmA7BiICFXnsCtOcUETqF10N06pg8ED28Hx1er6VuMhbEVO0HRcDaKEEEKIEmRW0NSnTx9cXFxKqy6igulQsxLOdjbsyWrIMJtdsP1DNct39S5q3qXC6NMNVComaNKXO7FazRxeqS4EdYTr/6rbZDC4EEKIUmBW0PTqq6/i6+tbWnURFYyDrTVd6/qy8mQ3htjuw+baUfhhsJqu4IGXoN6DuYO59W4WkaNJT98CdfM/uHpYfd1qvBqE+edMEixBkxBCiFJg8kDw/OOZhDBF74Z+HFHq8ITz19DmKbBxVNMPrH4CFrSBg99DRs7deGm3IFmdu45KtQs/oL6l6epBuPGverymw9V1fjl5oRIjISW29N6UEEKI+9Id3T0nhKm61fPF1lrDvhgnLrR+B146CZ1fAwcPdeD2Hy/Dlw1g05vw32Z1J7cq4OBW+AG9awEaUNQ5D2n0CDiq+aFwcAPPnAHg0cdL820JIYS4D5kcNIWHh+Pj41OadREVkJuDLe1rqndZLtxxgVRbD+g+XQ2e+s4Cr5qQkQD7F8D6SepOeeecy8/OCTyq5S63Gm+8PUC66IQQQpQOk4KmqVOnUqlSJZO76KZNm0ZcXJxJZRcuXEj16tVxcHCgZcuW7Nmzp8iyY8eORaPRFHg0bNjQUCYkJKTQMunpMplreRnasioAa49cpetnO1l98ArZti7QbjI8dwge/xlq9crdoXLBuQ2N6IMq/yZQpYXxNv/G6rMETUIIIUqYSUHT3LlzSU1NNfmgCxYsID4+/rblVq1axZQpU5g+fTpHjx6lU6dO9OvXj4iIiCLrERUVZXhcuXIFLy8vHn30UaNybm5uRuWioqJwcHAwuf6iZA1qWpn5I5tTzcuRG0kZvLb2OAPm7WHn2RtciU/nrGs7jnb+jiODt3Ku/Sy0HV4s/oB1+gIa6PxqwbvwDIPBpXtOCCFEyTLp7jlFUahTp47JLU0pKSkmlfvyyy+ZMGECEydOBGDOnDn89ddffP3118ycObNAeXd3d9zd3Q3Lv/zyC7du3WLcuHFG5TQaDf7+/ibVQZSNB5tUplcDP37Yd5mvtp/jTHQSY5ceLKRkNQbGhTNvRLOiv29tnoTmowpmDIfcoCnmP3Ui38LKCCGEEHfApKBp6dKlZh/Yz8+v2O2ZmZkcPnyYN954w2h979692bdvn0nnWLx4MT179iQoKMhofXJyMkFBQWRnZ9OsWTM++OADmjcvussnIyODjIwMw3JiYiIAWq0WrVZrUl3E7VkBY9tXY3BTPxbuvMjqw5EoioKjnTWOturjUmwqv4Vdo76/M08+UN3w+Re8DjZQ2LVx8MbGqRKa1Biyrh1HqdyiYBlxx4q+HqKsybWwHHItLEdpXwOTgqYxY8aU+IljYmLIzs4uEFz5+fkRHR192/2joqL4888/Wb58udH6evXqERISQuPGjUlMTGTu3Ll07NiRsLAwatcu/Db2mTNnMmPGjALrd+zYgZOTkxnvSpiqOdC8ZcH1e1w0/BxuzWd//UdSxBnqeah3bW7ZssXkY7e39seXGE5uXcHlSrf/LlVklW8dINnen0SnoNsXNoM510OULrkWlkOuRfkzZyjRnTAruWVpyN8FoyiKSd2AISEheHh48NBDDxmtb9euHe3atTMsd+zYkRYtWvDVV18xb968Qo81bdo0pk6dalhOTEykWrVqdOvWDW9vbzPejbhb/RQFzYZTrDkcyfJL9qya0Iqzh/fSq1cvbG1tTTqG1faDEHqSxj7QsF//Uq6xBbtxGtvvnkDxrkXW5P0lckitVsuWLVvMuh6idMi1sBxyLSxHbGzp5ugrt6CpUqVKWFtbF2hVunHjxm279hRFYcmSJYwePRo7u+InjrWysqJ169acO3euyDL29vbY29sXWG9rays/AOXgw4cbc+5GCseuxPPC6pNMDDbzWlRuBoD1jX+xvp+vX8IlADRx4dhaacC65H7c5WfDcsi1sBxyLcpfaX/+JudpKml2dna0bNmyQHPmli1b6NChQ7H77tq1i/PnzzNhwoTbnkdRFI4dO0ZAQMBd1VeUHXsbaxaNaomPqz3/3Uhm+Xmr2yZXVRSFdG1Owkt92oHr/4Iuu5Rra8ESI9VnJRuSrpVvXYQQogIot6AJ1PxP33//PUuWLOH06dO89NJLREREMHnyZEDtNnviiScK7Ld48WLatm1Lo0aNCmybMWMGf/31FxcvXuTYsWNMmDCBY8eOGY4p7g3+7g4sGtUCW2sNx+KsGLn4IFtPXUenMw6esrJ1/H78Gg8t+JuG7/7Fol0XULxqqtOraFMg7mI5vQMLkHA193V84Wk8hBBCmM6s9vqsrCwcHBw4duxYoQGLuYYPH05sbCzvv/8+UVFRNGrUiI0bNxruhouKiiqQsykhIYG1a9cyd+7cQo8ZHx/PpEmTiI6Oxt3dnebNm7N7927atGlz1/UVZatlkBcfDm7Am+tPcuhyPBN/OERNH2cmda5B7wb+rD8ayeK94UTGpxn2+eTPM0QnpPOuX0M0kYfUfE1FzWNX0elbmgDir5RfPYQQooIwK2iysbEx3MpfUp555hmeeeaZQreFhIQUWOfu7l7s6PjZs2cze/bskqqeKGdDmlch/VIYV5xqsergVS7cTOH1tSd4fW1uxm8vZztGtwvCzsaKz/46S8i+S/TwDaATqJnBGz1SbvUvVwl5gqYECZqEEOJumT0y9K233mLatGksW7YMLy+v0qiTEEY87GFknzq80LMOK/+JYMneS0QnplOjkjMTOlXnkRZVcbC1BqCqpyOvrAnjz1hfOtmCNjKM+3ZYZmKecUzSPSeEEHfN7KBp3rx5nD9/nsqVKxMUFISzs7PR9iNHjpRY5YTIy83BlkmdazK2Q3WiEtKo5umElZVxeorBzapQycWe+T+qY5mSLh0hOykDH9eCd0dWaLpsSIrKXZaWJiGEuGtmB0358yIJUdbsbKwI8nYucnvHWpXwnDCU7CVv46XEM+F/W1g4uR/2Nta5d9NZWZdRbctJUrR615yejGkSQoi7ZnbQ9O6775ZGPYQoUQ0C/cn0rIn1rXN8cvNpsmZqsNNkoMlKB1tnGLkSqncu72qWHv0gcCtb0GnVO+l0OrAq1xtmhRDinnbHv0EPHz7MsmXL+Omnnzh69GhJ1kmIEmFXqysAPppEnLMT1IAJ1FQEP483HvNT0eiDJv/GoLGC7AxIuVm+dRJCiHuc2S1NN27cYMSIEezcuRMPDw8URSEhIYFu3bqxcuVKfHx8SqOeQpivz8fQZBi/HL/Ogr3XSMeez0e0pO2+yXD9JKwZB2N/B+sKOFRcf+ecZzAk34DEq+q4Jtfis+0LIYQomtktTc8//zyJiYn8+++/xMXFcevWLU6ePEliYiIvvPBCadRRiDtjYwfV2jC4/4M0ad6OK4oPk36J4kqvb8DeDa7sh63vlXctS4e+pcm9CnhUU1/LHXRCCHFXzA6aNm3axNdff039+vUN6xo0aMCCBQv4888/S7RyQpQEjUbDRw83onmgBwlpWsZuiCGl/1fqxtD5cOrX8q1gadBnA3erCu4SNAkhREkwO2jS6XSFTohna2uLTqcrkUoJUdIcbK35ZlRL/N0cuHAzhZF7fcho86y6ccOzEHuhfCtY0vTjtdwq57Y0SdoBIYS4K2YHTd27d+fFF1/k2rXcQbSRkZG89NJL9OjRo0QrJ0RJ8nVzYOm41ng62RJ2JZ6R4X3JqtoOMhJh1WhIiS3bCmWmqo/SkLd7ztDSJEGTEELcDbODpvnz55OUlERwcDA1a9akVq1aVK9enaSkJL766qvSqKMQJaZ+gBvLJrbFw8mWw1eSeCrtOXTOvnDjX1jYFk5tMO+AqXF31kqlTYNvOsH81iUfOGVr1TxNoHbPSUuTEMJS6EpuGrbyYHbQVK1aNY4cOcIff/zBlClTeOGFF9i4cSOHDx+matWqpVFHIUpUw8ruLJvQFndHW7ZFWvGq/btkV6qn3pK/+glYPQaSTbg9P2I/zGsOC9rCNTPTbhz7CWLPq3e1RYTe2RspSlIUoKg5mpx9wD1QXR9/BRSlZM8lhBCmOrkWZlaFs5vKuyZ3zKygKSsrCxsbG06ePEmvXr14/vnneeGFF+jZs2dp1U+IUtGoiho4uTnYsPaaJ6OtPyW9/cugsYZTv6itTid+LjrIOPUr/G8QpMerySP3mjFJdHYW/D0vdzl89928lYL06QbcKqvJLN1z/pnJTFLrK4QQ5eHiLtCmwqU95V2TO2ZW0GRjY0NQUBDZ2fd285oQAI2ruvPjhLa4Otiw73IyfY53Ivzh38CvEaTGwtoJsKgT/LveuEn5wLdqi1R2BgQ9oK479SvEnDftxKd+gfjLucsl/QtEP57JrYr6bOcETpXU1zKuSQhRXtITcp7jy7Uad8Ps7rm33nqLadOmERcXVxr1EaJMNa3mwYon21HFw5HLsan0W5PIL21+hK5vgp0LXD8Ba8bCwnZwbAVseQf+fBVQoNV4eGID1OmnLu+be/sTKkpuq1TLcerztaO5v0xKQt5B4HoyrkkIUd70wVJ6YrlW426YHTTNmzePPXv2ULlyZerWrUuLFi2MHkLcaxpVcef35x+gcx0f0rU6pqw5zdvxA8h8Lgy6vA4O7hDzH/wyGf7OCYy6vw0DvgRrG3jgJXXdsRW3n5rl/FY1G7mdC/R8F7xqgqKDy/tK7g0l5GtpAvDIM65JCCHKg6GlqQT/SSxjZk+j8tBDD5VCNYQoX57Odiwd25q5284xb9s5ftx/mQPhsTQI6Idf3Z50SthAy2s/YZ+VjNWgedBsZO7OgW0hsANE7IP9C6H3h0WfyNDKNBYcPdVJg+MuQPgeqNuvZN6MoaUpz40Z7iXU0hR/GStd5t0dQwhxf7rfgqasrCwAxo8fT7Vq1UqlQkKUF2srDVN71aFpVXdeWnWM/64n89/1ZAC+oQN2tMaJdF5Jb8+o/Ds/8BIs3weHlkKnl9WAKL+IA3D5b/WutvY5iTWrd4LDS0t2MHj+MU2Qp6XpMptORlHTx4Xafq7mHffS39iG9KdRpe7AQyVRUyHE/aQCBE1mDwT//PPPZSC4qNB61Pdjy9QufDa0CW/2r8czXWvyWJtA2tYOIB5X3vv1X/ZdiDHeqXYv8G0ImcnE7ljIoUuFjPnTtzI1HaHe2QYQ3El9vn6i5JJr5r17Ti+npSn5ejiTlx3h+RVmpkgA+E+9Tdg38fjd1lAIcb9RFEiLV19n3Edjmnr06MHOnTtLoSpCWA4/NwcebVWNSZ1r8lrfeswc0pgfxrdhcLPKZOkUnl52hEsxKbk7aDS5Y5sOLGLUop38cjQyd/v1U/Dfn4AGOr6Yu97FF3wbqK8v7737imdlQMoN9XXe7rmcgeCaRHVOujPRSSSma807dk4uKufMmHv6P0UhRDnITAElp8ElPeGezRln9pimfv36MW3aNE6ePEnLli1xdnY22j5o0KASq5wQlkSj0TDrkSZcik0l7Eo8E384xLpnOuDmYEtmlo4Pztdiks6HalY3edR6F2+utaWudST1ra7CoSXqQeoPhEq1jQ8c3AlunFK76BoMvrtK6gei2ziAk3fu+pyWJueseBxJJw0H/o1MpH1N70IOUgidDq4dMyxqbvwLrl3urq5CiPtH3n+0dFlqviY756LLWyizg6ann34agC+//LLANo1GI113okJzsLXmu9EtGTT/b87fSOb55Uf5dGgTnl9+lH8uxYH1g3xgtZS37Zbztu5H7Nbl+3l4YErBg1bvDP98UzLjmvJO1KvR5K539CDbzhXrzCQqa2K5oFThZGSC6UFT7Hk1OWYOzfWTUFOCJiGEifLnZkpPuCeDJrO753Q6XZEPCZjE/cDXzYHvnmiFg60Vu/67SZfPdvDPpThc7W3o8dhL4BqAnZKJnSabZMWBszZ1yWo2Gh4NgSotCx4wuCOgUdMaJEbdXeUKGwSe45atHwBVNep4rBORZnSxXTtitKi5/u+d1U8IcX/K36V/j3bxmx00CSHUbOJfPNoMgHStjhqVnFn/bEe6NgqCCZvh8Z+JHPMPnaz+R5/kd5maNgGlwUOFH8zREwKaqq8v3eW4pgR1zFJhQVO41guA3pXVlAEnzQmaItWgScnp5tNcP3EXlRRC3HcKBE335mBwk4Om/v37k5CQ+6Y/+ugj4uPjDcuxsbE0aNCgRCsnhCUb0CSA2cObMvGB6qx/tiO1fF3UDR6BULsXVarXZcGoVthYafg17Bqzt55Dm60r/GDVc+6iC991d5UqLBs4cDMpg9Np7gD0qaoGTRdjUkgydTB4TkuTrunjOQc8A9lmDiQXQty/7reWpr/++ouMjAzD8qxZs4ymUsnKyuLs2bMlWzshLNzDzavy1oMNcHe0LXR7h5qVeHdQQwDmbTtHm4+28tYvJzh0KQ4l790j1XPGB93tuKbCsoEDO87c4KpOnX+uUvYNKrs7APDvNRP+28vWQrTasqRrMBitlQOa7EyIOXd3dRVC3D/06Qb0KnrQpOS7PTD/shCicKPbBfFm/3r4uNpzK1XLsv0RDF0USqdPd/Dd7ovqz1JgO7CyUSfyvXX59gctSmHZwIGtp69zVfFRF+Kv0KiK2upkUhfdjdOQlQ727uBVk0THnESZ0dJFJ4QwUYGWpvhyqcbdkjFNQpSBSZ1rEvpGd36c0IYhLargbGfN1VtpfLTxNF9tPw/2rlA5Z+7GS3vu/ESFDARP12az51wMkYra0kR8BI1zgiaTBoPrB4FXbgYaKxIMQZMkuRRCmOh+657TaDRo8t7CnLNOCGEaG2srOtX24cthzTj0Vi9e71sPgC+3/MeaQ1fU1ANw51102jRIzckqnicbeOjFWNK02WhdclqfkqJoHOAImBg0ReqDpuYAuUHT9ZN3Vk8hxP0nf5B0j2YFNzlPk6IojB07Fnt7ewDS09OZPHmyIbll3vFOQojiOdpZ83TXmiSma/l65wWmrTtB3b5NaQLqdCU7PobgB6BqG7B1MNpXp1MI2XeJAHcH+jUOyN2gz9Fk62Q0992209cBaFa/NpxygKx0mrip2czDY1JIzsjCxb6YXwX6lqYqaktYolOe7jlFMc4HJYQQhdF3x7kGQFLUPdvSZHLQNGbMGKPlUaMKTFnKE088cfc1EuI+8mrvulyLT2PDsWuM26bhgIMnNum3YNcs9WFtD1VbQ5Nh0OIJ0GiYt/0cc7aew8ZKw95AT/xzBnUbdc3lBDKKorD9tDqtSs8GfhBZFWLP46W9ToC7A1EJ6fwbmUDbGkUkudSmqVPAgKH7MNGhKorGCk1qrPrLL+8cd0IIURh9kORe7f4ImpYuXVqa9RDivmRlpeHToU24kZhB6MVYBtp+xvKeCXhe36/mbEqKUueku7wXbp7hz8rPMWeretdalk5h2f7LvNKnrnqwhILpBk5FJXItIR0HWys61KwE/1RTs3vHX6FRlbpEJaRzorigKfqEOl+Us486uDwrC52VHXjXhpizEH1SgiYhxO3pW5o8AuHqP/ds0CQDwYUoZ/Y21iwa3ZI6fi6cTnZiyP5anGz3BUw9Dc8fgS5vqAX3LyTz56ewIYtm1TwAWP5PBOnanEz8iQUTW27LaWV6oJYPDrbWhol7SbhiGAxe7B10OZP0UrmFUTec4tdIfSGDwYUQptAHSZ5BOcv35pgmCZqEsADujraEjGtDZXcHwmNSeGjB38zddh6tR3XoNo3EfvPJworBVntY47GQVeObUsXDkbiUTH49ljOWqZAcTfrxTD3r++acKGc8UvwV0+6gizQez6Sn+Km5pyTtgCgTty7D33MhI+n2ZYVlSsv5PeOR8ztIWpqEEHejsocjv7/QiQGNA8jSKcze+h+PfL2Pf68lMO5ITSZlTiUDO5qn78d+xaNMaKVOi7Lk73A111O+bOARsamEXVV/MXWvlxM0GVqaIgy5mi7mDAYv1DXjO+f0FL/G6gu5g06UhV2zYMs7ELayvGsi7oROl3u3XM5UTBI03aGFCxdSvXp1HBwcaNmyJXv2FJ2jZufOnYbUB3kfZ86cMSq3du1aGjRogL29PQ0aNGD9+vWl/TaEKBFeznbMH9mcuSOa4e5oy/GrCQyYt5fDl29x0K4NMQ+vVJNMRuxjzMkxDLb9hzPRiey/GJd795xbVbKydby0+hgAHWt54+uWM1jcu7b6HLEfn6TT+Ls5oChwqrDM4OmJuVm/K+drafLNaWmKvQAZySX8KZjg1AZY0A5uyiwE94X4CPVZP7eiuLdkJAI5CbE99N1zEjSZbdWqVUyZMoXp06dz9OhROnXqRL9+/YiIiCh2v7NnzxIVFWV41K5d27AtNDSU4cOHM3r0aMLCwhg9ejTDhg3jwIEDpf12hCgRGo2Gwc2qsPmlznSpo2bxttLA/JEtqNK0B4z7A1wDsI6/xFzrOWywe5vQrWtz/6C4V2He9vMcvnwLV3sbPhnSJPfgVVpA3QGQnQk/j6dlgB1QRBdd1DFAUf8zdPEx3ubiCy5+6vYbp0r8M7it3Z/BzdNwfFXZn1uUvZSbxs/i3qIfBG7jmPu7JDsDtOnlVqU7Va5B05dffsmECROYOHEi9evXZ86cOVSrVo2vv/662P18fX3x9/c3PKytrQ3b5syZQ69evZg2bRr16tVj2rRp9OjRgzlz5pTyuxGiZPm5ORAyrjXfjm7JTxPbGQIo/BvDcwehyxvobJ1panWRqVGvGn4xHYpzYP52tYXow4cbUc3LKfegGg0Mnq+Oe4q7wLNpi4AiBoNHFt41Z+Cf00VX1uOaEiJzzyktTfeHZPWGBgma7lH6ViUHd7BzBXJuKrkHE1yanHKgpGVmZnL48GHeeOMNo/W9e/dm3759xe7bvHlz0tPTadCgAW+99RbdunUzbAsNDeWll14yKt+nT59ig6aMjAyj5JyJieqF1Gq1aLUyk3t50n/+9/N16FZHTQdg9BlYOcADr0DzMWz+7g26J/+OnSYbnb0bz627gE6BIc0r07+hb8HPztYVzeCvsV72EA1u/M5gq0COX+1ToJz11cNYAdn+zdDluw5arRYr34ZYn99K9rUww/ayoDnzp+EXl3LjNFn36XfjvvnZyNZim6ZODq9Luk62Bb7f++Za3CFNciw2gOLgRlZ2NjYObmjSE9Amx4C95233N0dpX4NyC5piYmLIzs7Gz8/PaL2fnx/R0dGF7hMQEMC3335Ly5YtycjI4Mcff6RHjx7s3LmTzp3VKSiio6PNOibAzJkzmTFjRoH1O3bswMnJqZA9RFnbsmVLeVfBYp30GcqHcd2YZPMnsbogopMy8HFQaGcbwcaNRXd11/UbRL3oX/jIdgkPxtRk/W8J2Oc22tLz4j6cgf1XMonZuNFo3y1btlDllpZWQNyZvexVNmJVRonB21xYhiEPetxFNv2+AZ2Vbdmc3AJV9J8Ne208fXNeZ8ReYXO+76IlqejX4k4FxB+iDRCXprB340Z66WxxAkK3b+KW87kSPVdqamqJHi+/cgua9PLPX6coSpFz2tWtW5e6desaltu3b8+VK1f4/PPPDUGTuccEmDZtGlOnTjUsJyYmUq1aNbp164a3dxFJ/0SZ0Gq1bNmyhV69emFre//+YSxOX53C5rl/807cGNCCrbWG78a1o2Flt+J31PVGtywalyv7mWu7gLSGv9Gyuo86ADwhAtujMQC0Gfyk2qxO7vXo0q0HBw67w6WFOKde5YxNdV7pU7+03ypkpWPz5WQANSu5oqNvm9rg26D0z21h7pufjegTkHOTpoMumf79+lnc1D33zbW4Q5pjtyAcPP2D6N+/PzaRn8KNGDq0aIhSs3uJnis2NrZEj5dfuQVNlSpVwtraukAL0I0bNwq0FBWnXbt2LFu2zLDs7+9v9jHt7e0Nc+rlZWtrKz8AFkKuRfHGdgxmxm/qgOzX+9ajWZApwb4tDF1M8tx2NOUimavbYKtLA0WXW8S7FraulQyLN5My+POKhg/mhRKXnM6/9nY4aTI4dPQI1v0bY1VUc1PsBTi5DlqMBlf/O3+jl3aBNhVcK6NxrwpX/8H21nmo0vTOj3mPq/A/Gxlxhpea7Exss1PB0aP86lOMCn8t7pRWvcPWyskTK1tbw/WzyUqBEv68SvvzL7eB4HZ2drRs2bJAc+aWLVvo0KGDycc5evQoAQG5k5a2b9++wDE3b95s1jGFuNc82qoabap78WjLqozvWN30Hd2rsr3uu+gUDXbZKbkBk52LmoSu44skpWtZd+Qq40MO0vnz3Wy6ak1MciY+bo4kuKl3rvqnXSj8DrxsLez5Aha2hx0fwm9T7u6N/rdJfa7TG3zrqa9vnCm6vLj3Jecb/C2Dwe89eQeC530uKu1A7AXYOcsi0xKUa/fc1KlTGT16NK1ataJ9+/Z8++23REREMHmy2vw+bdo0IiMj+eGHHwD1zrjg4GAaNmxIZmYmy5YtY+3ataxdu9ZwzBdffJHOnTsza9YsBg8ezIYNG9i6dSt79+4tl/coRFlwsbdh9VPt72hf9+YP0fWYNQ5kcktxQefgQfMq/rQI8uD4vwlsX7eVzKzc1qdgF4UX+zXhwWZVsd34Gxz+l4HWoZw4tJumVQaAVc7AqMgj8OsLcD3P3XXn/lLvfsszP57JFEXdH6BOX4i7qL6+KUFThZZyw3g5+QZUql14WWGZDEGTR86zm/H6/HZ/BmEr1Baptk+Vdu3MUq5B0/Dhw4mNjeX9998nKiqKRo0asXHjRoKC1ORXUVFRRjmbMjMzeeWVV4iMjMTR0ZGGDRvyxx9/0L9/f0OZDh06sHLlSt566y3efvttatasyapVq2jbtm2Zvz8h7gWdalVi0uAe7Dhzg6jwOJLSs9h6+jpbc6ZgAajh48zAJpXp28CH/w7tpn+TAGytrdSkl4dD6Gt9EMJGwxk3qNZWzeMUtkJtuXL0hL6fwJEf1YmHjy6Drq+bX9GbZ9Qkh9b2UL0zWOc0w0vagYotOV/QVFxL06W/4cQa6PV+7h9mUf70eZpMbWnSJzO9dblUq3Unyn0g+DPPPMMzzzxT6LaQkBCj5ddee43XXnvttsccOnQoQ4cOLYnqCVHhWVlpGNUuiFHtgsjK1vHvtURCL8YSdiWe4EpqsFQ/wBWNRoNWq+W/vDs3HUFyUjz/bFtHK6uzuGUkwvk83eONH4U+M9WEdlY2atB05Afo/Epui5Sp/stpZareGeycwSeney7uAmRlgo3d3XwMwlKlxORbLiZo2jkTLu2BKi3V8XPCMpjbPZcUZfxsQco9aBJCWA4bayuaVvOgaTUPE3ewx6Xri8z9txUnrsTxdU87+rhcVFt/6g2A2r1yy9Z7EBy9IPEqnN+mjksyhz5oqtNHfXaroo69ykxWu+r0Y5xExaLvntNf6/wtT3nps+LfCi/9egnTFRU0FZbcUlEgKedmLgsMmsp97jkhxL2ve11fdFix9po3tHsaBs4xDpgAbB2g2Uj19eGQwg+UGgeHlkJKvtuG027BlZypkGrnBFsaDfjkpCCRcU0Vl34guG9OSouiWpoUJXf+RQvs1rmv6YMm/V2P9sWMacpIVO+QhdzraUEkaBJC3LUe9X0B2Hs+hoys7KILthijPv+3qeAvRG06LBsCv0+B77urd9Dond8GSjb41AfPoNz1Pjl/SCVoqrj0LU36XFxFBU2psep8ZgDxEjRZlLR49dmU7rmkaOPXilKqVTOXBE1CiLvWsLIbvq72pGZmc+BiXNEFfepAUEc1ADq6zHjbptfh2lH19a1LsLgXXDmoLufvmjMcT1qaKjSdLndMk18j9bmo7rnEyNzX0tJkWcwZ05S3Sy47Q21ltiASNAkh7ppGo6F7PbW1afuZYsacALQcqz4f+QF0Oa1SR37M6bLTwMPfqJMEp8bC/wbC6d9yB5cXCJpyxjHJHXQVU9otNcCG23fP5W25TLkBmaU7nYYwUbYWtCnqa0PKAX3QVMiYpqR8U55ZWBedBE1CiBLRLU/QpBTXpF5/kPrLM+EKXNgO147BHy/nHORNaDoCxvyujl3KSoNVo9Q/ng4eULWN8bH0LU0x5yA7q6Tfkihv+q45R09wq5yzrqigKdJ4WbroLEPewEg/lsnUlqbClsuZBE1CiBLxQK1K2FlbERGXyoWbKUUXzDsgfN9XsHq02gxfuw90ekVdb+8CI1ZAiydy96vVE6zz3fDrXg1snUCnlTumKiJ9V5yzLzj7qK8zkwtvRcrfIiFddJZBn6PJzjX351cfNGlT1JaovJKu51uWoEkIUQE529vQtoYXANvPXC++sL6LLnyXmsjOMxiGfANWeX4lWdvAwHnQ/W1wqwqtJxQ8jpUVVKqjvpZxTRWPvlXJ2QfsXcHGwXh9XvmDJmlpsgz5E1tCbosTFOyi0wdJmpzfBYkSNAkhKqgepo5r8qkLgTnzQdo4wPBlahdMfhqNmghz6r8QVMT8kT4yB12FpW9pcvFRvwv61qZCg6ac7jln9TsoLU0WIv8gcFD/IbJzydkeb1xeP6ZJ/3OdJGOahBAVVPd6fgAcvHSLhDRt8YW7TQPvWurAb//Gd35SfVJLaWmqeFLydM9BbtBU2B10CTlBU1DOHIzS0mQZ9OkG9Dma9IpKcKlvaarcXH2WliYhREUV6O1ELV8XsnUKaw5dITG9mMCpemd4/jA0fOjuTip30FVc+hYll5xgySUneMo/iW/exJZBHdVnaWmyDIW1NEHhCS7zZgPXB00WNqZJplERQpSo7vV8OX8jmQ//OM2Hf5ymiocjdf1dqenjTEaWjtiUTG6lZBKXkkm6Nps+jfx5slMNKrnYm3yOw5fj+OXoNZ7rXgs/wx10/6kpDMyd005YLn02cENLUyX1OX/3XNot9U5LgMB26nP8ZfWPsEZT+vUURSsqaCrsDrq0W7kJSiVoEkLcD8Z2CCY8JoUTVxOITkwnMj6NyPg0thfRe/bNrov8b98lHmsTyFOda+Lv7lDs8U9cTWD04n9IzczmUmwKP4xticbGAbLS1aSY3jVL/k2J8qFvUdK3MOmDp+R8QZO+lcnJO/fGgIxE9Y+wk1fp11MUzZygSd/K5Oil3hwCaoBsQRNyS9AkhChRlT0c+e6JVgDEp2ZyNjqJs9eTCI9JwdnOBi9nO8MjOSOLb3ZfJOxKPEv/vsRP+yN4tFVVpvSsg49rwZanK3GpjAs5SGqmmvBwz7kYfjt5g0GVakP0CbWLToKmiiM5z91zUHT3nD5ocqsMto7g4gfJ19UgWoKm8mW4e87DeH1hCS71rUquAWrgZGWrphNJvg4e1Uq7piaRoEkIUWo8nOxoW8ObtjW8iyzTr5E/e8/H8NW28/xzKY6fDkTw58loPnyoEf0bBxjKxaVkMmbJP8QkZ1A/wI3OtSvxze6LfPD7KfrWroNd9Am4eRrq9S+Lt1a24i6CjSO4Bdy+bEWhKHkGgvsYP+unVtHT3znnVkV99ghS/9DGX4YqLUq/rqJod9LS5OqvphNxDYCECDWYspCgSQaCCyHKlUajoVNtH1ZPbs/KSe2o5+9KXEomz/x0hOdXHOVWSiZpmdlM/N9BLsakUMXDkZBxrZnauw41KjlzMymD3bdygrKKOBg8JQYWdYaQ/hY3eWmpSk+A7Ez1tctt7p7LHzTpJ3WWweDlr8igqZCB4Mn6oCnnnwP9PwkWNJWKBE1CCIvRroY3vz73AM93r4W1lYbfwq7Re85uxi79hyMR8bg52BAyrjV+bg7Y21jz4UPqJK6rI5zVA1TEtAORRyAzSW1tSrha3rUpO/rWJDtXtcsNTOueg9zxMJJ2oPzdaUtT3mcLGgwuQZMQwqLY2Vjxcu+6rH26AzV91JakA+Fx2NlY8f2Y1tT2czWU7VCrEg83r8I5XVUAlJv/gU5XXlUvHdHHc19fP1l+9ShrKXkSW+rpB4Kn3TKefqOw7jmQliZLcLs8TUZBk35Mkz5oqmy83gJI0CSEsEjNqnnwxwudmNS5BlU9HZk3ojltqhcc1Ptm//rE21cmQ7FBk5WmjoGoSPIGStH3UdCUnG88E6hZ4zU5KSXyjmsq0NKkD5oulWoVhQnytDQlpGn5+fBVNX9bYckt87c0GbrnJGgSQojbcrC15s3+9dn7enf6NvIvtIyPqz2v9GvIRUX9BXvz+JayrGLpiz6R+/r6iaLLVTQp+e6cA3VwsCFXU05QpSi52cDztzQlXKl4LY/3mjxB08Kd53llTRiPfbufZJyMt0OeoCknWJKWJiGEKHmPtQ7korOaDM97+6tcWP6KmtvFFIoCW96Fb7oUPj1HecpMgdgLucv3Y0uTfhyTXv5cTRmJoE1RX+tbJtyqqC1S2ZkW9Qf3vqNNz01W6eBO2JV4AP69lsg7f+WMz9MHTTqdjGkSQoiyYGWlofXEuWxx7IeVRqHmf98R+WUnMm+cu/3Ouz+Dv+dA1DE4HFLKNTXT9VOAkju5adxFNZC6H6Tkywaulz8ruL5rzsED7HJuCLC2AXd1nFu5DAZXFFj2CCzubXrwXhHpczRprFDsXDgTnQSAg60VR2+qd4Iq+jFPaXFqTiZQ82xBbndrYpTF3DkqQZMQokLw9fai2yvLWV97JvGKM1VSz5C98AHi/l5a9C/cIz/Cjo9yl48tL51fzrpsWDUK/jcwd2CsKfSDwAPb5fwhUXICqftA/nnn9PLfQafvmtMHSXr6O+jKYzB43EU4vxWuHIAr+8v+/JZC34pk78b1JC3xqVqsrTSsfqo9mpwxTUpmMolpGbmtSc4+YG2rvta3NGlTCk7sW04kaBJCVBg21lY8/PgznBy0kYM0xJF0vLZMIfn7B+FGvnQE/22G315UX7d7Vm3NuRUOEaXwR+7YT3D6NwjfDaufML31QT8I3L8x+DXKWXefjGsqbCB43mX9dsOdc5WNy+kHg5dHS9OVA7mvz28r+/NbijzjmU5Hq0FPTR9nmlT1YMH4rgBYofD04p2cOpuTY801z9hFO2ewzxkwbiGDwSVoEkJUOA+0bEbA83/xg9MYMhRbXCL3onzdATZNU3+RRx6GNWNAyYamj0Gfj6DBQ+rOx34q2cpkJMG2D3KXw3fB7y+Z1qKlHwTu1wj8c4Kmux3XlJ0Fe76AiAO3L1ueDNnA83fP5csKnv/OOb3yTDuQN2i6IEETDu6ciVK75ur6q0kt61fzQWetTpV0OTKKpZvVz+xAjD0vrw5j5T8RZGbpcsepWci4JgmahBAVUlVvVwY++zmP2c3lr+xWaJRs2L8QvmoJPz0K2lSo2QMGfQUaDTQbqe747y+QmVpyFdnzpRoAeNWEEctBYwXHlsHuz4lKSONGUnrh++myc7vi/JuAX2P19d3majq+Era9D+snFR+4xUfAt93g6LK7O9+d0g/0zj8QPH/3XP4cTXqG7rlLpVG74uUNSKNPWN4NBmUlT46mMzktTfX8c/OsWeXkbnqwjhN1HJMBuJjuytojV3lj3QmW/h2eeyedBE1CCFG6PJ3teO2xvjydNZXRmW+Q5FJdHSuTGgsBTWHY/3LHTwS2V1snMpPgzB8lU4FblyF0gfq694dQbwD0/1xd3vEhsz59n35z9pCQqi24b1y4OpbDxlGdhFjf0nT937u7jf74qpy6XYKYYgbKH1oC147ArlllPwg3MyX3jrgC3XP57p67XUtTWXfPpcWrcyDmrcOF7WVbB0thmKw3t6WpfkBu0KTP1fRG1wCebKZmfW/XtAFDW6rj01b8E4Gi766zkKlUJGgSQlRo7Wp483z32uzRNaFT0kfEPfAeNB0JI9eAfZ5f4FZWua1NJdVFt/Vd9Zbr6p2hbj+OX43nydNN+SZrAACzbL6lZmoYyw4U8oddPwjcrwFYWYN3LbC2g8xkiL90Z/VJiITwPbnL5/4quuzZP9Xn+Iiyn55GPwjcxsH4GkHRd88VGNMUnLs9K6NUqlmoq4fUZ68a0OgR9fX9Oq4pp3su286NCzfVlqR6Od1zgHGCy5x0A9Wr12LGoIa42NtwKTaVyGwPtYw+HUE5k6BJCFHhPd+9Fm2CvYjPgLFnWpM5cAG4qrc1Z2Rlc/BSHDvO3kBpMlzd4eLO3Luy7tTlUPh3vXq7dZ+PeW3tcQbN/5stp64zK/sxjrp0xl6Txdd2c1i+9wzp2mzj/fMOAge1Rcynnvo6+iS7z8UQaW72gZM/A4raRQhwbnPh5WIvGAdK/xUTXJWG5DyJLTUa422G7rmbaoubIWjK1z3nXAlsnQClbOfs098tV60d1Oqhvr6w7f5MspnT0hSvcyJLp+DmYEOAu0Pudvs8k/bqu99c/HG2t2FQMzUI3ndDHfck3XNCCFFGbKytmDOiGe6Othy/msBbv5zgq23nePz7/TSdsZlHF4UybulBZuxNRQnqCCjq2J/80hMh/srtT6jTwV/T1NfNR3MovQqrD13FSgNDmldh80tdaf7iahT3anhrkmiZFsq6I/mCtLyDwPVyAqio/w4y4YcjzD9lTVpmvmCrOMdXq8/tn1OfL+9T31N+/21Sn/VTlpR10JRSxJ1zedcp2YydsxYycgYb529p0mjyDAa/VCrVLJR+EHi1NlC1jXpXZmosRIeVXR0sRU5LU7RWDZTqBbihyRsE551/Ll9iy8daBwKw45qNul6654QQouxU9nDks6FNAFh96CpfbPmPv8/Hkq7V4e1sB0DIvkus13VWdzi2wngsz9lNMLcpzG0Cx9cUf7Ljq+DaUbBzhe5vEbLvEgCPtqzGl8ObUcvXFWwd0TR9DICHrffw/Z6LZOvynE9/l5x/k9x1OQFUzIUjAKRmadj073XTPoDok2rrlbUddJqqdvfpstRWtfz0XXNtn1KfrxyA1LgiDx2TnIFOV4LjnorKBg5gbYvi6AmAY4waWGZYu6CzdSlYtqzTDuiy4Oph9XW1tmBjp3bNwv3ZRZcTNF1NU3++6vvn62rVB02pcZCc8z3OGfjduKo7DSu7EZmdU0ZamoQQomz1bujPCz1qU9XTkQFNAvjgoUZsndqZQ2/15NOhTdBo4O1ztcjUOEDsOXV8SlaGmqpgxXA1a7GiU+88O/Fz4Se5uBM2vaG+7vwy0dlubDqp/hc9pkOwcdmmI9Ri1sdJiolky6mcPxwpMZCU85+1X4Pc8jmDwT0SzxpWrT5sYtfTiZxWptq91Ylva/dWl/OPa0q7pbZAAbR5EnwbqKkZihjM/GvYNVp9uJUvt/xnWj1MUdi8c3mraKtO3NzY+hIAl7QeTF52mKT0fAPqy7ql6cYpdQC7vXtuV2rN7urz/TgYPCdoupiktljWC3Az3q4PmuIuqN8xjZXRNR/RJpBoRb3WSvJ19Y7SciZBkxDivjK1Vx32vt6dBSNbMLpdELV8XdFoNAxrVY0vHm1KmsaR37JaAaDs/RIW91JTFQC0fRqaj1YDp3VPwsm1uQdWFPh7Hvz4sDqWo0oraPs0Px24TLZOoU11LxpUzvdHw7smVG2DNQqDrP9m0a4LKIqS2zXnVcN4IHROS1M1zU0aeCpYoXDocjznbyQV/6Z1utzWMf24LUPQtMW4Re3cVvUPmE999fz6coV00WVm6Zj1pzr2KWTfJZIzsoqswvkbSTzz02FORiYUWcaguJYmIFKrtir18vh/e3ceVlW1PnD8u8/AYRBQQGZRzAHnASecyhxyKocsM3NIrWsOad5uZdkv7WZ6uw02qdmk3UzN1LIbmVhO5RiKc84KKogggsxwzvr9sfHgEdBjN4Hs/TzPeZS1195n7b2A87JGvfXhPL6sPXie/u//Yh9wDBS3NJXTWk2GhB36f2q01icWQPG4poTtpXeF3s6Klhw4kl4UNJVoaSr6ebhQFHB7+Otb4BTp1zyYLLMPVqWhKVulWLpBgiYhhCgysGUobw1uzirbnQBoh6MhcQ/KzQeGLINes+Hed6DFI6BsqBWPceTHz4g9ega+GgUxL+oBVfOhMPI7cjHzxfZ4AEZe28p0RTM9iLnf9AtxCZfYeSqt5CDwIjbXaiRrvgA82TifhtX0YGfpjtLHWRVabXy0+QQHtkbrLVeu3sVBUM32+nibzPOQeNV4m8PR+r/1e+n/1uup/3ssRl8Y8ypfxZ7h7KUcADLzClm1q+xWr//75gDR+5KY8MWukoPer1XWvnNARm4BR7P06enh+fqSCY0iIgj0cuX4hSz6v/cLW4+n6pmvzKArp+457eyVoKltcaJPbagWrnfdndpc+om3q6KWpvhsFzQN6gWU0T2XWrT0xdWrgQNermZ6NQ3hAlX1hMsVP65JgiYhhLhKv+YhDHlwKPFK/8DeZmvAIP7NK0fD2HHyIvFpuXzu/zQ/V+mJpqzU3jQJn//cDQdWgsGkr8PU730wu/Ld3kRSs/IJ8nalR8OA0t+w0UAwmGmonSJCi+eDjcevGgTuGDRtOZ7KvkJ9gGxnr0SiAvSgacWuM+QVlgxEFmw+wSvfHeLgDx/pCQ37g7lo9pLJArXv0v9/ZRZdYb6+ZxpA/d76v6Gt9c1wc9LgzE77tfMKrbz3U1HQUtSCtmjrab2l7Bq74tPYUhTInErNZs66og/J7IuwbX7xytFX2IMmvxLX+u+eRJJt+vuZ8tIA8A0K59uJHWlTy4fLeYU8unAHvxxLuSWrgs/dcIy/f7mn1LW1tCvPp0YbxwNXWpv+auOaiuo1Aw9q+rjjYTE5Hnetqv9bWLTA65WFLK8ypE0NkpQ+hi07tRxnQZahwoOmuXPnEh4ejqurK5GRkWzeXHYkvnLlSrp370716tXx8vIiKiqKH35wbDJeuHAhmqaVeOXmlrHqrhBCXKNPsxAu9F/KO/4zGGl7kdg0Nz76+SQPfrCVzv9ez7RvDjIs5RGWF3bGpNkIN5zngvLmZN9l+jggTUMpxaKtpwB4pF1NTMYyft26+0B9vTVnoHEzP/6WTN6Zopafa1qaFm8/zSGlB02uqYdoUFUR4GUhLbuAH64ZEH78QiZz1h3FQj73aPo0+Mv1Bjq+t72Lrihoit+ir5njUR1CIvU0ownqdCvKV/z79stfz3AuPRd/TwsLH22Du4uRY8mZbD2RWuIW564/BhR3z3y4+QT7z1zSuzjXPFu8B+AV1+me+yo2gRTl7ZjoFUx1TwufjW5Dl/rVyS2wMWrhTn5O9dCP51zUt7P5H8UcPM9raw6zYtcZhn+ynfSc4sDJNf8iWnqCPi4npJXjiXdctfTAHyU/C5Y/Cj/P+eOu+UdSqjhoUu6O6zNd4XpNPV7T0gTQMqwamS7698G+38p5vbBSVGjQtGzZMiZPnswLL7zA7t276dSpE7169SI+Pr7U/Js2baJ79+5ER0cTGxtLly5duPfee9m9e7dDPi8vLxITEx1erq6upV5TCCFKE9kikifHTSb2xXuYN7QlA1uE4O1mxmjQaFPLh6fvaUCDvy3C2n4y29060zdvJsNiDKRl6Zvx7k64xN4z6biYDDzUusb136ypPiD8Qcs2XMnDdLGoJSaweLmB8xm5rD14nkO2otaT5AMYNXigpb4+0dIdxb83bTbFcyv2kl9o44ngY3hpOZxRfoz/2eI4Q69ud/3fM7/qg8+vzJqrd0/xmBwo7qIrGteUW2C1B0Lju9ShuqeFgUXl+GyLY6vOocQM1h1KRtNg7tCW9GkahNWmWLn0o+JWrQOrigefQ5n7zh2/kMmu+Etc1K75sPXW39vVbGT+sEi6NQggr9DGqC9+I99Fb6X4X1ubMnILmPa13gKoabDnTDojPtlhH3zuk6U/DwIag+WamXzhnfRWyLRT+hpYZbmcpK9G/9PMG69KH7tQb91c9xKc3fX7bupWys/Ux8YBGbhT/9rxTFBK0FSypUnTNHwC9T8UTp049ocX82ZVaND05ptvMnr0aMaMGUODBg2YM2cONWrUYN68eaXmnzNnDs888wytW7embt26vPrqq9StW5dvv/3WIZ+maQQGBjq8hBDi9/CwmOjVJIg3Bzcndlo3Dsy4hy/HRjG+Sx0a1/DB2GMG9SeuwOITypm0HMZ/sYtCq41FRcsM3NcsGN8qluu/SdGMtqrWVB41/oARKzkmL2xVitceWrYzAatNYQzSW5+05EOgbAyKDEHT9K6706n6apeLt59m56k03F2M/K2q3mX0nerIpmMXeWNt8cw7vIKLWrOUHsDYxzP1dixfna56C0ryQbgUz7KdCSSm5xLo5cpDwcnwxWDGBp8AIObQec4VjXMCmLtBDxJ6NwmidvUqTL+3EdVdFSMyPih6wEWB0ffP6rOjCvOKu+uuaWn6qmimYFDwNUHoVQtbWkxG5g5tSc9GgeRbbRzK1WdfcXLjdSrgxmZ//xvnM/Ko5evOiifaU9XdTFzCJUZ8soPMvEJ8sooC3avHM9kL5akvdgmciy0KhpSC5N9g61z4cji82QjeqA9LH4ZNr+lpF0+UXhhrQfH2PAA/PF/+W93cSFEdFmAiB4vj9ilXWK5pffIsvQs7PLwOAMasJPYkXPojS3nTKixoys/PJzY2lh49ejik9+jRgy1btpRxliObzcbly5fx8fFxSM/MzKRmzZqEhobSt2/fEi1RQgjxe5iMBlzNxhLpVd1d+HB4K9xdjGw5nsozK/by3V59ZleZA8AdLuwCjQcBMNlV/yNwV14Nxi/ZTU6+lUKrjSVFLUl3d2gHJje0whyq5J0npKobnevq07SX7kzg7KUcZn//G6B4u1Uqbqf0qe51u48G9CBmzf6r1ry50kW35V19yxSTa/FYpyvcfezBQMGhNczdoP/F/3KzNCyL+8ORNYSu/RuDa6RhtSn74PeTKVl8t1cfvDv+Lv2Dr7qnhU/rb6OmIZkk5UPC/d/qU/ST9uqbA2el6O9pMBWPeQG9dapooHmrhvUdy3fNwpYuJgPvPtyCPk2C+NKqD+ov+HHm714gcduJVPs9zRrYlJZh1fh8dFu83czsir/EmM92UTXzOkETkBrUEYCUnz9h+1sPYX2jAcxtqy+CevAbyDgDaPoSDz536APHN8wuvUD7voKMsyh3P5TJDeK3wqHVv+vebtqZX+H9tiXWKrtwOc9xPNtVXXOgOdk9V7KlCcDNVw+S/UljZvShUsfNlRfTjbPcGikpKVitVgICHCPLgIAAkpKc22PmjTfeICsriwcffNCeFhERwcKFC2nSpAkZGRm8/fbbdOjQgT179lC3bt1Sr5OXl0deXvHeRBkZ+rTQgoICCgpK2UhTlJsrz1/qoXKQ+ihbbV9XXhvYmAlL99hX944Mq0p9f3ennpfWaBCmnR9iseqtRYepyff7k0i4uIVBkSEkpudSzd1Mtwh/bDsjMCTuxisnnoKCAh5oGczGIxdY/msC+xPSaFEYx4tVVlF/lz4GxBbalk7tohiVdphPtpzm71/uIdDThUbBXmi1u2La/IZ9xp6tVmesmgtcU2bDHd0wxm8l8devOZ8xnoFVDtB99xtQmIsye6AVZPFS5kzW8n98seM0YzvX4v2fjmJT0KW+H3Wru+nPIT2BRsf1gekzCx4mOeYiizs9jWndi6if/onVqwYmQLn7Umi1glXv4tl0NIXzGXlUdTPTtH4t2KCXS7l4UGhwK1FegNfvb8RzDGX34U20KDzGoU+fIHzscowGrUTe8xm5pGTm0zDI02HV6twCK89+pe8DOLhVKK3CvCgoKKC+vzsLR0QyYuGvHIg/j7er3v1XEBxZoixKKd46EcorQFPDSUg/CUChwYJWsz3U6ogKiUQFNtNbpRLjMH/SDbX3SwrbTSxe80m/GKZf5qAB7+X0oLqrjYcKl6LW/h+F4V31Af6/Q4HVxpm0HEKruWEua/ydNR/T10+gpRxBfTeFwrCO4OHHoq2neSX6MI91rMUz99QDQMtMxQSkK3fczAaCPM0lfw5M7pivLoObX6n1qLn7YwKCDGnsOHmRr36Np3/z4BL54Nb/bqqwoOkK7Zp9hZRSJdJKs2TJEqZPn84333yDv39xE267du1o166d/esOHTrQsmVL3n33Xd55551SrzVr1ixmzJhRIn39+vW4u7s7eyviFoqJianoIoirSH2UrUeIgbVn9Q+dxpZUoqOjnTtRKbpaAqmSp//RGBQUisc5xf5zGew/p/8h16JqHj/G/ECzfC9qAd458cTExGC1QaA5jwbZuxmX8A2tXY5AIVg1Myf9unKk2r0UREfTWEEdLwPHMqD/vG3U8FA09ynk3wYPXG16sLY7N4Qdy6NJytFIzgGFhsmgqGF141nAP2UHAw1NeK3wQzSsJHq3YE+NR+l45BWq5JxjvuVdHs56juc+/YHoBAOg0cycZH8OrU6+S0hhDonuEfyQ3478k2n0SA7jC2MwgVnnyFo5CW8g3Wph41XPbuERA2CgqXce67fG0bcoPVPz4qfvvy/zsd7lAf/1GUWTtGk0SNvAy2/Mon5Ec1yLGgyTc2DdWQM7UzRsSiPATXFnkI3WfgoXI6w+beD0RQPeZkULTnD84zepkpvEcf+eXPKozZi6sPPQcUxYSdOqsennvaDtcyjDnlSNxfFVaWduR6RrIhsKGxGd15QdtggCE8wMNlsJuZQBB4onQrWu2prgSzu5sGwSO2sXD5T3T99D1IXfyFKufJjbhcIcI10ta6h+6RRH/jOFYwF9nPp2O5cNpy5rJGRpnMnUOJcNhUqjjpeNv0XYcCnZoEqd8/+lUYq+ppKWl8GZz8exxmckr+8zAhof/nwK14vHqOMNgem7aIs+c87fYmXNmlLqSCnu1YwYisY+/bj9AHnmkjPkPHPOcjcQYtBXpZ+xeh+F8XG4lxLBZGdnO3X/v1eFBU1+fn4YjcYSrUrJycklWp+utWzZMkaPHs3y5cvp1q3bdfMaDAZat27N0aNHy8wzdepUpkyZYv86IyODGjVq0KVLF3x9fZ24G3GrFBQUEBMTQ/fu3TGbzTc+QdxSUh831tOm+Gf0b1zKLuDZ+xuX/Vd7KQzeh2HjqwB0u+9hVptr87fPd3Psgh7QPD+4MzV93DHsPAdrNxKYvova1d0wJsUxwHgUzah3WxRqLmitH8UW9SQ1qwRQ86r3aH9XPs+u2M/mYykkZGkkZJnpam5Gf6M+LGL8idYk2Ur7aKjJUIsfoVoKb7rMB8DWcAB+982lq9EMyS1QC3vStuAAz5mWMDP+EQDahVdj/ODWAGgnN2HavROlGfB7+AOmn/Fm2jcHOZFl5jnDIyx0eQ3vHL0bLN+zBne07EQtX3dyC238Y+dGwMaUAe1pFOyFOjQZrSALj+B69O7du5TyFusLnFh6lHrHP2V03iLGnWrLkz2bsWLXWb4/cN4+HMjFZOB8jo0vTxj5IdFE3yZBbEg6Ayj+NagZPU7MwpD0NQChl7Zhq3sP1s7P0sMUC4dgS2FdXOu04e76xata5+Rbee3dX1DkcqD9W9zTrS4DbIqcHQnsXneM05mFvHvIhYUjW9EyrGpxoS/cgfqwE8HpsfRpHogKbqlf76P3AFhs7UpErVBSMvP4V9pgXjd/QP0L31HvwellrqQOkJFTwMzvD7NyT+ldlccyDHybVp25D7fAYrrqezc9AdMHYwGwthiBcfciaqVuYG9ef6zKDw8XI1n5VlYlevLfgVFUOXwZTujdc+0iQundu1Gp76cdqQrZqSjNSNf7BhdvJn213Az4bSruKpuGvkYOplrZr4UzvXeDEllTU0vO3vwjVVjQ5OLiQmRkJDExMQwYMMCeHhMTQ79+/co8b8mSJYwaNYolS5bQp8+NI2qlFHFxcTRp0qTMPBaLBYulZJOm2WyWD4ZKQuqicpH6uL5XBjS9cabStBgCm/8NLu6YAxtxh8mFleM7MPv73wjzcadOQNEYkJBmAHjlnoX9X9pPP6OqE+cexT1/m42pagilNBYQWNXMotFtScnMY+2B83y/P5EfT7Smv3ELsba6JNmq4ulqol6AJ3WqV8Fs0sgrsJFXaONoYntCM4rGzrQcjqHvHAyGoncJaQoD5sGXw3nMFM0BWy2+tnVkwt319O+VglyIeR4ArfUYzKHNeTgU7m4QRMzBJNYe9GP96bV0McQBsDnRwN/f34qmQTV3F/ILbUQEetIszEfvjahSHdKyMHiHYnDie7He4FfJeyeGkMwz9L24iNGfPWI/1q2BP+O61KGufxWW/3qGT7ecJOFiDouLFg29t4k/PU/Ogj2L9Q/1uj3g6FoMR3/AcPQH6hcNaI611WfVyv1ET+pEkLe+AOe7609w9lIuwd6uTOxaD7PZhBkY1ekO+jYL4akv4/jlWCpj/rOLZY9HFa8aH9xYX719zxJMm2bDsFUc3bWeuud3kK+MxIU8xKJRbcktsPL4IjP7kn6gScEpTn49nfCRC0p9BhuPXOC5FXtJTM9F06DDHX40CfWmcbA3jUO8SL6cx/CPd7DpaCpPf7Wf9x5uUbxURsw0KMiGmh0w3vc25F1CO/gNwy7NZ7v7DFaM68AjH23nTFoOr687xiuB+srsGXjQMLhq2b8vXL0hOxXNMxCzSxldiyYfMHtAQRavdPNl4LJkvtiZwOA2YTQNreqQ9Vb/XqrQ7rkpU6YwbNgwWrVqRVRUFAsWLCA+Pp6xY/VodurUqZw9e5bPPvsM0AOm4cOH8/bbb9OuXTt7K5Wbmxve3vovkxkzZtCuXTvq1q1LRkYG77zzDnFxcbz//vulF0IIISqLqmHw6Pf6uBSTvsmpl6uZVwdc80dfaBtsTQZz4dQh/Jr1wBjWBoJbUtVcjR5GA2bTjVu3/KpYeLhtGA+3DSM9qwVxG2tQGNiKbXc0IMDLUvowicR/wOId+p553abrc++v1rAfdHoaNr/ObPOHjHDZRvOYXFh1HrKLWgDcfaHL8/ZTAr1dGRZVi2FRtbicMA/bJ50wqEIMnv54Zpu4nFvIxaJlHIa0CSsul0d1fQr/VTPnrsvFHUu/ObB4EKNMa1ht60CtJh0Yd9cdNPA16atSX8hnVFRzRrSvxbpD51m05RS5efn82zwf4r4CzQgDF0CTQZByVB+ovX8FWp7efZrm25y05AImLY1jyWPtOJOWzfxN+gy4F/s2xN3F8SPX38uVj4a3ZtjH2/n1dBrDP9nO8rHtCfcrWl/qruf0Qd/Hf+LkzjWc/u4N6gK/uN/Nv0f1xs3FiJuLkf88FsW8RZNpcmYyYSe/5J+f9MC3dnMiAj2pH+iFt5uZmd8dsk8mqOXrzuuDmtIq3LEnpaavBx8Ob8WohTtZcyCJZ77ay+sPNMNwdA0c/k4fnN/nDdA09jR4mogD0bQ3HuSjtucJ9/PgtUFNGfrRdr7YdoonI7bgz5U1mkqZOXfFlcHgpazRZKdp+vGLx2lp3c+Ld1zmzKkjnPj8S5rUsqJ1mgIhLZ37Pvgfaaoih6GjL2752muvkZiYSOPGjXnrrbfo3FnfFXrkyJGcOnWKDRs2AHDXXXexcWPJaaMjRoxg4cKFADz11FOsXLmSpKQkvL29adGiBdOnTycqKsrpMmVkZODt7U1KSop0z1WwgoICoqOj6d27t7RsVAJSH5VHpa0Lm5Xc/zyA68lSFnI0uUG/9/Sgoyw/zdSn3D/0Bap+b1Iy8zlxIZPMvEK61PfHcGUQ94oxsG85DPjAvvGxU5Y/CgdWYq1WG6NfHbhwWJ81yJU+Ok+ofae+0W7tu2D9q7D/Kz1guP9jaNTf8XrJh7BtfovTiRcovP9j+s/bQVa+lUld63LgXDrrDiXTsY4f/xndpszxuuk5BQxZsI2DiRmEVHXjqyei7C1Vl5Y/SdUDiziuQgjnHAZNkT3mF9xDGztcw2ZTHHm3PxFpG0iwVedDa2++st5JNq4YDZp9fa5pzTIZafge05Hv9DWl7nkVajp+PsYcPM/Yz2Ox2hQjW/vzUsIotEvx0GESdH+ZzLxCer29iQczFjHR9LW+Xc34HWCy8K/lG+m4dyodjAcAeDJ/Av+cNh1v9zK+Rz/rp29yHdEXHlpcdr0t7Fv2NjT93te3NkLvnvPz8yM9PR0vr1Jm7P2PKjxoqowkaKo8Ku0Hw1+U1EflUanrIi8T9iwBs7u+9k6VQL2lwM3HcdHMsmRf1Jc5uJ70M/q2JM0eurkZY5fPw3utIe+arVvcit4v52LJcwwmeGAhNLi31EteXRfRB5KZtDTOfsxk0FgzuTN1/KuUeu4VFy7n8eAHWzmZksUd1T0Yd1cdlv2awKmTx9lkmYyrps8KK7yjB6Zhy0u/SNop8j/oikuuvmxDFu4ss97Ffwq70qnKOf7h/ROeKaUswdNoAHR/WW/pLPJN3FkmL4vj78ZlTDB9Q5rJn286fk2DsECWx57hq9gz1PGGteYpGDKT9JbHoOaoFY+hZV8gW1mYVvAo2zx7sGVq17Jv/Mvh+pILrcforVhl2TYf1k4Dt2rgHcopqy9rz7pw0VSdvz36GNVq6kHkrQ6aKnz2nBBCiNuMpYq+nczvdaOACcA7FCJH3Py1PQNgyBI48r2+kW71CKheX9/nzmaDpD36Qp/HfoKE7WAwwgOLIOL6g82v6Nc8hJ+PprC8aCHO0R3Dbxgwgb5+1edj2jJo3haOX8ji78v1rXSMBh82VRtIj0vLADB1fqrsi1SrhctTcRC3BLbPx+PicUYZoxlljIYCIAUwukCTB/RNpfd9Cbs+01dk/y0a2k+A6g0g4yz9Ms4RGXYM/6QNADybPZS1a04BpwC9x+yVwe0wXJ4Bq/4G62eBNR8NRXbV+tybPIbjthDuDrpB4FKlqFuuas3r52s3Fto8bg+6a9gU37z3MwfOZaAdMvHsDU7/o0jQJIQQ4q+lVgf9dS2DAYJb6K/O/9AXaCzM1wed34QZ/RpxNDmT3AIrE7uWvj5gaUKquvGf0W0Z9vF2rDbFkDZhDGkTRqC5HSw6oAd3YTcYamLxhLaP6y03x9bB9vn6nnce1fW0VqOKV1qv1UFPWzNV7/ra7NjSEwqgwYWQ7jQOfxjOZbD/bDrn0nOZeHdd2tX2BduDsGMBnI3VT4ociXvP2XSNOcXxTSeIqn2D3poOk/QAuOXwGz+gq1opjQaNmQOasP1EKqM6ht/43D+IBE1CCCFEaa5dsdpJ7i4mvh5fSlDmhDr+Vdj8TBeMBu2qMVCu8MQvN3chgwHq9dBfOWn67LOiyQUOApvAiG/1ve62zdVnB3oFF71CoGoY1et040lD8VzMvEIrFpOx+H36z4efXoZGA6Gxvin01F4RDIoMpfaVQe1l8Q6BDk/e3L0VaV6jKs1rVP1d5/5eEjQJIYQQlYjpJtb2copbtesf1zRo0Fd/OcEeMF1RvR4M/vyaS2rUC7jOrLk/qQrdsFcIIYQQ4s9CgiYhhBBCCCdI0CSEEEII4QQJmoQQQgghnCBBkxBCCCGEEyRoEkIIIYRwggRNQgghhBBOkKBJCCGEEMIJEjQJIYQQQjhBgiYhhBBCCCdI0CSEEEII4QQJmoQQQgghnCBBkxBCCCGEEyRoEkIIIYRwggRNQgghhBBOkKBJCCGEEMIJEjQJIYQQQjhBgiYhhBBCCCdI0CSEEEII4QQJmoQQQgghnCBBkxBCCCGEEyRoEkIIIYRwggRNQgghhBBOkKBJCCGEEMIJEjQJIYQQQjhBgiYhhBBCCCdI0CSEEEII4QQJmoQQQgghnCBBkxBCCCGEEyRoEkIIIYRwggRNQgghhBBOqPCgae7cuYSHh+Pq6kpkZCSbN2++bv6NGzcSGRmJq6srtWvXZv78+SXyrFixgoYNG2KxWGjYsCGrVq26VcUXQgghxF9EhQZNy5YtY/Lkybzwwgvs3r2bTp060atXL+Lj40vNf/LkSXr37k2nTp3YvXs3zz//PE8++SQrVqyw59m6dSuDBw9m2LBh7Nmzh2HDhvHggw+yffv28rotIYQQQtyGKjRoevPNNxk9ejRjxoyhQYMGzJkzhxo1ajBv3rxS88+fP5+wsDDmzJlDgwYNGDNmDKNGjeL111+355kzZw7du3dn6tSpREREMHXqVLp27cqcOXPK6a6EEEIIcTuqsKApPz+f2NhYevTo4ZDeo0cPtmzZUuo5W7duLZH/nnvu4ddff6WgoOC6ecq6phBCCCGEM0wV9cYpKSlYrVYCAgIc0gMCAkhKSir1nKSkpFLzFxYWkpKSQlBQUJl5yromQF5eHnl5efav09PTAbh48eJN3ZP44xUUFJCdnU1qaipms7mii/OXJ/VReUhdVB5SF5XHlc9tpdQtuX6FBU1XaJrm8LVSqkTajfJfm36z15w1axYzZswokV6vXr2yCy6EEEKISik1NRVvb+8//LoVFjT5+flhNBpLtAAlJyeXaCm6IjAwsNT8JpMJX1/f6+Yp65oAU6dOZcqUKfavL126RM2aNYmPj78lD104LyMjgxo1apCQkICXl1dFF+cvT+qj8pC6qDykLiqP9PR0wsLC8PHxuSXXr7CgycXFhcjISGJiYhgwYIA9PSYmhn79+pV6TlRUFN9++61D2tq1a2nVqpW9STQqKoqYmBieeuophzzt27cvsywWiwWLxVIi3dvbW34AKgkvLy+pi0pE6qPykLqoPKQuKg+D4dYM2a7Q7rkpU6YwbNgwWrVqRVRUFAsWLCA+Pp6xY8cCegvQ2bNn+eyzzwAYO3Ys7733HlOmTOGxxx5j69atfPzxxyxZssR+zUmTJtG5c2f+9a9/0a9fP7755hvWrVvHzz//XCH3KIQQQojbQ4UGTYMHDyY1NZWXX36ZxMREGjduTHR0NDVr1gQgMTHRYc2m8PBwoqOjeeqpp3j//fcJDg7mnXfe4f7777fnad++PUuXLmXatGm8+OKL3HHHHSxbtoy2bduW+/0JIYQQ4vZR4QPBx40bx7hx40o9tnDhwhJpd955J7t27bruNQcNGsSgQYN+d5ksFgsvvfRSqV12onxJXVQuUh+Vh9RF5SF1UXnc6rrQ1K2alyeEEEIIcRup8L3nhBBCCCH+DCRoEkIIIYRwggRNQgghhBBOkKBJCCGEEMIJEjSVYu7cuYSHh+Pq6kpkZCSbN2+u6CLd9mbNmkXr1q3x9PTE39+f/v37c/jwYYc8SimmT59OcHAwbm5u3HXXXRw4cKCCSvzXMGvWLDRNY/LkyfY0qYfydfbsWR555BF8fX1xd3enefPmxMbG2o9LfZSPwsJCpk2bRnh4OG5ubtSuXZuXX34Zm81mzyN1cets2rSJe++9l+DgYDRN4+uvv3Y47syzz8vLY+LEifj5+eHh4cF9993HmTNnbq4gSjhYunSpMpvN6sMPP1QHDx5UkyZNUh4eHur06dMVXbTb2j333KM+/fRTtX//fhUXF6f69OmjwsLCVGZmpj3P7Nmzlaenp1qxYoXat2+fGjx4sAoKClIZGRkVWPLb144dO1StWrVU06ZN1aRJk+zpUg/l5+LFi6pmzZpq5MiRavv27erkyZNq3bp16tixY/Y8Uh/l45VXXlG+vr7qv//9rzp58qRavny5qlKlipozZ449j9TFrRMdHa1eeOEFtWLFCgWoVatWORx35tmPHTtWhYSEqJiYGLVr1y7VpUsX1axZM1VYWOh0OSRoukabNm3U2LFjHdIiIiLUc889V0El+mtKTk5WgNq4caNSSimbzaYCAwPV7Nmz7Xlyc3OVt7e3mj9/fkUV87Z1+fJlVbduXRUTE6PuvPNOe9Ak9VC+nn32WdWxY8cyj0t9lJ8+ffqoUaNGOaQNHDhQPfLII0opqYvydG3Q5Myzv3TpkjKbzWrp0qX2PGfPnlUGg0GtWbPG6feW7rmr5OfnExsbS48ePRzSe/TowZYtWyqoVH9N6enpAPZNF0+ePElSUpJD3VgsFu68806pm1tg/Pjx9OnTh27dujmkSz2Ur9WrV9OqVSseeOAB/P39adGiBR9++KH9uNRH+enYsSM//vgjR44cAWDPnj38/PPP9O7dG5C6qEjOPPvY2FgKCgoc8gQHB9O4ceObqp8KXxG8MklJScFqtRIQEOCQHhAQQFJSUgWV6q9HKcWUKVPo2LEjjRs3BrA//9Lq5vTp0+VextvZ0qVL2bVrFzt37ixxTOqhfJ04cYJ58+YxZcoUnn/+eXbs2MGTTz6JxWJh+PDhUh/l6NlnnyU9PZ2IiAiMRiNWq5WZM2cyZMgQQH42KpIzzz4pKQkXFxeqVatWIs/NfL5L0FQKTdMcvlZKlUgTt86ECRPYu3dvqZssS93cWgkJCUyaNIm1a9fi6upaZj6ph/Jhs9lo1aoVr776KgAtWrTgwIEDzJs3j+HDh9vzSX3cesuWLePzzz/niy++oFGjRsTFxTF58mSCg4MZMWKEPZ/URcX5Pc/+ZutHuueu4ufnh9FoLBF1Jicnl4hgxa0xceJEVq9ezfr16wkNDbWnBwYGAkjd3GKxsbEkJycTGRmJyWTCZDKxceNG3nnnHUwmk/1ZSz2Uj6CgIBo2bOiQ1qBBA/tG5vJzUX7+8Y9/8Nxzz/HQQw/RpEkThg0bxlNPPcWsWbMAqYuK5MyzDwwMJD8/n7S0tDLzOEOCpqu4uLgQGRlJTEyMQ3pMTAzt27evoFL9NSilmDBhAitXruSnn34iPDzc4Xh4eDiBgYEOdZOfn8/GjRulbv5AXbt2Zd++fcTFxdlfrVq1YujQocTFxVG7dm2ph3LUoUOHEktvHDlyhJo1awLyc1GesrOzMRgcPzKNRqN9yQGpi4rjzLOPjIzEbDY75ElMTGT//v03Vz+/e/j6berKkgMff/yxOnjwoJo8ebLy8PBQp06dquii3daeeOIJ5e3trTZs2KASExPtr+zsbHue2bNnK29vb7Vy5Uq1b98+NWTIEJnOWw6unj2nlNRDedqxY4cymUxq5syZ6ujRo2rx4sXK3d1dff755/Y8Uh/lY8SIESokJMS+5MDKlSuVn5+feuaZZ+x5pC5uncuXL6vdu3er3bt3K0C9+eabavfu3fblgJx59mPHjlWhoaFq3bp1ateuXeruu++WJQf+CO+//76qWbOmcnFxUS1btrRPexe3DlDq69NPP7Xnsdls6qWXXlKBgYHKYrGozp07q3379lVcof8irg2apB7K17fffqsaN26sLBaLioiIUAsWLHA4LvVRPjIyMtSkSZNUWFiYcnV1VbVr11YvvPCCysvLs+eRurh11q9fX+pnxIgRI5RSzj37nJwcNWHCBOXj46Pc3NxU3759VXx8/E2VQ1NKqf+pXUwIIYQQ4i9AxjQJIYQQQjhBgiYhhBBCCCdI0CSEEEII4QQJmoQQQgghnCBBkxBCCCGEEyRoEkIIIYRwggRNQgghhBBOkKBJCCGcoGkaX3/9dUUXQwhRgSRoEkJUeiNHjkTTtBKvnj17VnTRhBB/IaaKLoAQQjijZ8+efPrppw5pFoulgkojhPgrkpYmIcSfgsViITAw0OFVrVo1QO86mzdvHr169cLNzY3w8HCWL1/ucP6+ffu4++67cXNzw9fXl8cff5zMzEyHPJ988gmNGjXCYrEQFBTEhAkTHI6npKQwYMAA3N3dqVu3LqtXr761Ny2EqFQkaBJC3BZefPFF7r//fvbs2cMjjzzCkCFDOHToEADZ2dn07NmTatWqsXPnTpYvX866descgqJ58+Yxfvx4Hn/8cfbt28fq1aupU6eOw3vMmDGDBx98kL1799K7d2+GDh3KxYsXy/U+hRAV6I/Zf1gIIW6dESNGKKPRqDw8PBxeL7/8slJKKUCNHTvW4Zy2bduqJ554Qiml1IIFC1S1atVUZmam/fh3332nDAaDSkpKUkopFRwcrF544YUyywCoadOm2b/OzMxUmqap77///g+7TyFE5SZjmoQQfwpdunRh3rx5Dmk+Pj72/0dFRTkci4qKIi4uDoBDhw7RrFkzPDw87Mc7dOiAzWbj8OHDaJrGuXPn6Nq163XL0LRpU/v/PTw88PT0JDk5+ffekhDiT0aCJiHEn4KHh0eJ7rIb0TQNAKWU/f+l5XFzc3PqemazucS5NpvtpsokhPjzkjFNQojbwrZt20p8HRERAUDDhg2Ji4sjKyvLfvyXX37BYDBQr149PD09qVWrFj/++GO5llkI8eciLU1CiD+FvLw8kpKSHNJMJhN+fn4ALF++nFatWtGxY0cWL17Mjh07+PjjjwEYOnQoL730EiNGjGD69OlcuHCBiRMnMmzYMAICAgCYPn06Y8eOxd/fn169enH58mV++eUXJk6cWL43KoSotCRoEkL8KaxZs4agoCCHtPr16/Pbb78B+sy2pUuXMm7cOAIDA1m8eDENGzYEwN3dnR9++IFJkybRunVr3N3duf/++3nzzTft1xoxYgS5ubm89dZbPP300/j5+TFo0KDyu0EhRKWnKaVURRdCCCH+F5qmsWrVKvr371/RRRFC3MZkTJMQQgghhBMkaBJCCCGEcIKMaRJC/OnJKAMhRHmQliYhhBBCCCdI0CSEEEII4QQJmoQQQgghnCBBkxBCCCGEEyRoEkIIIYRwggRNQgghhBBOkKBJCCGEEMIJEjQJIYQQQjhBgiYhhBBCCCf8P7cu0FmBoM8bAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_loss(history_exit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298aa32c",
   "metadata": {},
   "source": [
    "### Collect the results on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b205b1",
   "metadata": {},
   "source": [
    "#### MAU entrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f4dbd181",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_entrance = np.asarray(test_features_entrance).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa2521ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dnn_model': 3.524531364440918}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results_entrance = {}\n",
    "test_results_entrance['dnn_model'] = dnn_model.evaluate(test_features_entrance, test_labels_entrance, verbose=0)\n",
    "test_results_entrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9e9150d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original\n",
    "ori_test_features_entrance = np.asarray(ori_test_features_entrance).astype(np.float32)\n",
    "ori_test_features_entrance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6db274f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dnn_model': 5.150083065032959}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_test_results_entrance = {}\n",
    "ori_test_results_entrance['dnn_model'] = dnn_model.evaluate(ori_test_features_entrance, ori_test_labels_entrance, verbose=0)\n",
    "ori_test_results_entrance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e20fa7",
   "metadata": {},
   "source": [
    "#### MAU exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c1d5a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_exit = np.asarray(test_features_exit).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1d5d13e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dnn_model': 0.23299700021743774}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results_exit = {}\n",
    "test_results_exit['dnn_model'] = dnn_model.evaluate(test_features_exit, test_labels_exit, verbose=0)\n",
    "test_results_exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "28256387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original\n",
    "ori_test_features_exit = np.asarray(ori_test_features_exit).astype(np.float32)\n",
    "ori_test_features_exit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b38df5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dnn_model': 0.11064910888671875}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_test_results_exit = {}\n",
    "ori_test_results_exit['dnn_model'] = dnn_model.evaluate(ori_test_features_exit, ori_test_labels_exit, verbose=0)\n",
    "ori_test_results_exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09606ee2",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "35c1fff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean absolute error [Temperature]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dnn_model</th>\n",
       "      <td>3.524531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Mean absolute error [Temperature]\n",
       "dnn_model                           3.524531"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_results_entrance, index=['Mean absolute error [Temperature]']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1d288d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean absolute error [Temperature]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dnn_model</th>\n",
       "      <td>5.150083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Mean absolute error [Temperature]\n",
       "dnn_model                           5.150083"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original\n",
    "pd.DataFrame(ori_test_results_entrance, index=['Mean absolute error [Temperature]']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4ad1c10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean absolute error [Temperature]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dnn_model</th>\n",
       "      <td>0.232997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Mean absolute error [Temperature]\n",
       "dnn_model                           0.232997"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_results_exit, index=['Mean absolute error [Temperature]']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "22cdd74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean absolute error [Temperature]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dnn_model</th>\n",
       "      <td>0.110649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Mean absolute error [Temperature]\n",
       "dnn_model                           0.110649"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original\n",
    "pd.DataFrame(ori_test_results_exit, index=['Mean absolute error [Temperature]']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853fab4",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "39d7f904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 417us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([73.06 , 74.753, 74.894, 75.035, 75.458, 76.022, 76.163, 76.587,\n",
       "       77.292, 77.574, 79.549, 80.959, 81.241, 81.665, 81.806, 83.357,\n",
       "       83.639, 83.78 , 84.345, 84.627, 84.768, 85.332, 85.473, 86.46 ,\n",
       "       86.541, 86.586, 86.654, 86.676, 86.732, 86.744, 86.755, 86.822,\n",
       "       86.856, 86.935, 86.946, 86.969, 86.991, 87.093, 87.138, 87.16 ,\n",
       "       87.183, 87.194, 87.25 , 87.273, 87.318, 87.419, 87.509, 87.532,\n",
       "       87.554, 87.577, 87.622, 87.644, 87.678, 87.637, 87.574, 87.436,\n",
       "       87.411, 87.285, 87.134, 87.122, 86.921, 86.908, 86.896, 86.871,\n",
       "       86.758, 86.745, 86.707, 86.695, 86.632, 86.619, 86.494, 86.406,\n",
       "       86.357, 86.359, 86.37 , 86.382, 86.392, 86.394, 86.448, 86.452,\n",
       "       86.458, 86.466, 86.478, 86.486, 86.488, 86.508, 86.558, 86.011,\n",
       "       85.882, 85.494, 84.07 , 83.553, 83.294, 82.647, 80.835, 79.411,\n",
       "       78.376, 77.082, 75.788, 75.659, 75.529, 75.012, 73.588, 73.115,\n",
       "       73.407, 73.455, 73.602, 73.796, 73.991, 74.04 , 74.186, 74.234,\n",
       "       74.332, 75.403, 75.743, 75.889, 76.084, 76.133, 76.181, 76.473,\n",
       "       76.522, 76.717, 77.009, 77.301, 77.544, 77.788, 77.982, 78.088,\n",
       "       78.405, 79.146, 79.358, 79.887, 79.993, 80.469, 81.104, 81.157,\n",
       "       81.368, 81.58 , 81.845, 82.056, 82.321, 82.744, 82.903, 83.114,\n",
       "       83.167, 83.379, 83.498, 83.479, 83.383, 83.364, 83.229, 83.191,\n",
       "       83.152, 83.095, 83.018, 82.96 , 82.941, 82.922, 82.864, 82.692,\n",
       "       82.576, 82.48 , 82.461, 82.442, 82.403, 82.384, 82.346, 82.288,\n",
       "       82.211, 82.115, 81.847, 81.693, 81.655, 81.578, 81.589, 81.961,\n",
       "       82.003, 82.21 , 82.334, 82.499, 82.665, 82.913, 83.037, 83.078,\n",
       "       83.161, 83.203, 83.409, 83.989, 84.03 , 84.071, 84.361, 84.444,\n",
       "       84.692, 84.816, 84.94 , 85.685, 85.809, 85.892, 85.958, 85.868,\n",
       "       85.834, 85.778, 85.766, 85.733, 85.71 , 85.417, 85.383, 85.236,\n",
       "       85.213, 85.089, 85.067, 85.044, 85.01 , 84.976, 84.864, 84.785,\n",
       "       84.515, 84.435, 84.356, 84.276, 84.038, 83.561, 83.163, 82.924,\n",
       "       82.209, 81.413, 81.254, 81.175, 80.857, 80.777, 80.379, 80.22 ,\n",
       "       79.027, 78.709, 78.471, 78.312, 77.596, 77.516, 77.357, 76.801,\n",
       "       76.403, 76.536, 76.611, 76.908, 78.54 , 78.763, 78.986, 79.134,\n",
       "       79.208, 79.283, 79.802, 79.876, 80.099, 80.247, 81.732, 82.4  ,\n",
       "       82.474, 82.548, 84.033, 84.206, 84.216, 84.225, 84.231, 84.238,\n",
       "       84.25 , 84.263, 84.265, 84.27 , 84.292, 84.296, 84.307, 84.319,\n",
       "       84.321, 84.328, 84.331, 84.353, 84.356, 84.363, 84.366, 84.37 ,\n",
       "       84.34 , 84.205, 84.16 , 84.084, 84.009, 83.979, 83.828, 83.813,\n",
       "       83.783, 83.693, 83.647, 83.572, 83.557, 83.406, 83.256, 83.181,\n",
       "       83.15 , 83.09 , 83.045, 83.   , 82.939, 82.834, 82.804, 82.796,\n",
       "       82.855, 82.89 , 83.056, 83.079, 83.115, 83.245, 83.363, 83.387,\n",
       "       83.481, 83.54 , 83.552, 83.564, 83.576, 83.729, 83.741, 83.812,\n",
       "       83.859, 83.883, 83.93 , 83.989, 83.64 , 83.06 , 82.905, 82.828,\n",
       "       82.789, 82.712, 82.093, 82.054, 81.706, 81.668, 81.59 , 81.435,\n",
       "       81.01 , 80.894, 80.507, 80.12 ], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_entrance = dnn_model.predict(test_features_entrance).flatten()\n",
    "test_predictions_entrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "37987aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([72.82 , 84.2  , 84.034], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original\n",
    "ori_test_predictions_entrance = dnn_model.predict(ori_test_features_entrance).flatten()\n",
    "ori_test_predictions_entrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8f24baf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 409us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([74.994, 74.961, 74.958, 74.956, 74.947, 74.936, 74.934, 74.925,\n",
       "       74.912, 74.906, 74.868, 74.84 , 74.835, 74.826, 74.824, 74.793,\n",
       "       74.788, 74.785, 74.774, 74.769, 74.766, 74.755, 74.752, 74.733,\n",
       "       74.763, 74.788, 74.826, 74.838, 74.87 , 74.876, 74.882, 74.92 ,\n",
       "       74.939, 74.983, 74.989, 75.002, 75.015, 75.071, 75.097, 75.109,\n",
       "       75.122, 75.128, 75.16 , 75.172, 75.197, 75.254, 75.305, 75.317,\n",
       "       75.33 , 75.342, 75.368, 75.38 , 75.399, 75.38 , 75.351, 75.286,\n",
       "       75.274, 75.215, 75.144, 75.139, 75.044, 75.038, 75.032, 75.021,\n",
       "       74.968, 74.962, 74.944, 74.938, 74.909, 74.903, 74.844, 74.803,\n",
       "       74.786, 74.788, 74.8  , 74.813, 74.825, 74.827, 74.888, 74.893,\n",
       "       74.9  , 74.909, 74.923, 74.932, 74.934, 74.957, 75.014, 75.022,\n",
       "       75.023, 75.025, 75.035, 75.039, 75.041, 75.045, 75.058, 75.068,\n",
       "       75.075, 75.084, 75.093, 75.094, 75.095, 75.099, 75.109, 75.082,\n",
       "       75.051, 75.046, 75.03 , 75.009, 74.988, 74.983, 74.967, 74.962,\n",
       "       74.952, 74.837, 74.8  , 74.784, 74.763, 74.758, 74.753, 74.722,\n",
       "       74.716, 74.695, 74.664, 74.633, 74.607, 74.58 , 74.559, 74.559,\n",
       "       74.562, 74.567, 74.568, 74.572, 74.573, 74.576, 74.581, 74.581,\n",
       "       74.582, 74.584, 74.586, 74.587, 74.589, 74.592, 74.593, 74.595,\n",
       "       74.595, 74.596, 74.53 , 74.519, 74.459, 74.447, 74.364, 74.34 ,\n",
       "       74.317, 74.281, 74.233, 74.198, 74.186, 74.174, 74.138, 74.031,\n",
       "       73.96 , 73.901, 73.889, 73.877, 73.853, 73.841, 73.818, 73.782,\n",
       "       73.734, 73.675, 73.509, 73.414, 73.39 , 73.342, 73.345, 73.507,\n",
       "       73.525, 73.615, 73.669, 73.74 , 73.812, 73.92 , 73.974, 73.992,\n",
       "       74.028, 74.046, 74.135, 74.387, 74.405, 74.423, 74.548, 74.584,\n",
       "       74.692, 74.746, 74.8  , 75.123, 75.176, 75.212, 75.242, 75.234,\n",
       "       75.231, 75.226, 75.225, 75.222, 75.22 , 75.193, 75.189, 75.176,\n",
       "       75.174, 75.162, 75.16 , 75.158, 75.155, 75.152, 75.142, 75.134,\n",
       "       75.105, 75.096, 75.088, 75.079, 75.053, 75.001, 74.957, 74.931,\n",
       "       74.852, 74.765, 74.748, 74.739, 74.704, 74.695, 74.652, 74.634,\n",
       "       74.504, 74.469, 74.443, 74.425, 74.347, 74.338, 74.321, 74.26 ,\n",
       "       74.216, 74.227, 74.234, 74.262, 74.415, 74.436, 74.457, 74.471,\n",
       "       74.478, 74.485, 74.534, 74.541, 74.562, 74.576, 74.715, 74.778,\n",
       "       74.785, 74.792, 74.931, 74.957, 74.974, 74.988, 74.999, 75.01 ,\n",
       "       75.03 , 75.052, 75.055, 75.063, 75.099, 75.104, 75.124, 75.143,\n",
       "       75.146, 75.157, 75.163, 75.199, 75.204, 75.215, 75.221, 75.241,\n",
       "       75.237, 75.221, 75.215, 75.206, 75.197, 75.194, 75.175, 75.174,\n",
       "       75.17 , 75.159, 75.154, 75.145, 75.143, 75.125, 75.107, 75.098,\n",
       "       75.094, 75.087, 75.081, 75.076, 75.069, 75.056, 75.052, 75.048,\n",
       "       75.043, 75.04 , 75.026, 75.024, 75.021, 75.011, 75.001, 74.999,\n",
       "       74.991, 74.986, 74.985, 74.984, 74.983, 74.97 , 74.969, 74.964,\n",
       "       74.96 , 74.958, 74.954, 74.949, 75.002, 75.086, 75.108, 75.119,\n",
       "       75.125, 75.136, 75.225, 75.23 , 75.281, 75.286, 75.297, 75.32 ,\n",
       "       75.381, 75.398, 75.453, 75.509], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_exit = dnn_model.predict(test_features_exit).flatten()\n",
    "test_predictions_exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "13b69470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([75.114, 74.947, 74.945], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original\n",
    "ori_test_predictions_exit = dnn_model.predict(ori_test_features_exit).flatten()\n",
    "ori_test_predictions_exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "27f0962c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11     -2.318851\n",
       "23     -2.141627\n",
       "24     -2.126868\n",
       "25     -2.112109\n",
       "28     -2.067796\n",
       "          ...   \n",
       "1660   -6.560480\n",
       "1671   -6.626107\n",
       "1674   -6.644010\n",
       "1684   -6.703679\n",
       "1694   -6.763347\n",
       "Name: Temperature (Â°F), Length: 340, dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_entrance = test_predictions_entrance - test_labels_entrance\n",
    "error_entrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e63c1204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    -2.720359\n",
       "12   -6.570316\n",
       "15   -6.159569\n",
       "Name: Temperature (°F), dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original\n",
    "ori_error_entrance = ori_test_predictions_entrance - ori_test_labels_entrance\n",
    "ori_error_entrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c6df03d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(error_entrance, bins=25)\n",
    "\n",
    "plt.xlabel('Prediction Error [Temperature]')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7f07e0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# original\n",
    "plt.hist(ori_error_entrance, bins=25)\n",
    "plt.xlabel('Prediction Error [Temperature]')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "61995131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11     -0.038288\n",
       "23     -0.065198\n",
       "24     -0.067429\n",
       "25     -0.069682\n",
       "28     -0.076411\n",
       "          ...   \n",
       "1660    0.150871\n",
       "1671    0.208459\n",
       "1674    0.224158\n",
       "1684    0.276523\n",
       "1694    0.328866\n",
       "Name: Temperature (Â°F), Length: 340, dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_exit = test_predictions_exit - test_labels_exit\n",
    "error_exit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c004d7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     0.004075\n",
       "12   -0.126986\n",
       "15   -0.200878\n",
       "Name: Temperature (°F), dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original\n",
    "ori_error_exit = ori_test_predictions_exit - ori_test_labels_exit\n",
    "ori_error_exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6066fedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(error_exit, bins=25)\n",
    "plt.xlabel('Prediction Error [Temperature]')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2b51a631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# original\n",
    "plt.hist(ori_error_exit, bins=25)\n",
    "plt.xlabel('Prediction Error [Temperature]')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fa24a0",
   "metadata": {},
   "source": [
    "#### Average error (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c90bb",
   "metadata": {},
   "source": [
    "AE: Provide a measure of bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d98de95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error: -3.524531279532973\n"
     ]
    }
   ],
   "source": [
    "average_error_entrance = np.mean(error_entrance)\n",
    "print(\"Average error:\", average_error_entrance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b24db27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error: -5.1500810953776055\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "ori_average_error_entrance = np.mean(ori_error_entrance)\n",
    "print(\"Average error:\", ori_average_error_entrance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f3634dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error: -0.17970559636488947\n"
     ]
    }
   ],
   "source": [
    "average_error_exit = np.mean(error_exit)\n",
    "print(\"Average error:\", average_error_exit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8ad883d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error: -0.10792989095052026\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "ori_average_error_exit = np.mean(ori_error_exit)\n",
    "print(\"Average error:\", ori_average_error_exit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58aee21",
   "metadata": {},
   "source": [
    "#### Mean squared error (MSE) and mean absolute error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef604f2d",
   "metadata": {},
   "source": [
    "MSE: Penalize significant errors more heavily \\\n",
    "MAE: Provide a measure of the average magnitude of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "33b9d2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 16.30774981409786\n",
      "Mean Absolute Error: 3.524531279532973\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(test_labels_entrance, test_predictions_entrance)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(test_labels_entrance, test_predictions_entrance)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "188e2aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 29.50322918731256\n",
      "Mean Absolute Error: 5.1500810953776055\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "ori_mse = mean_squared_error(ori_test_labels_entrance, ori_test_predictions_entrance)\n",
    "ori_mae = mean_absolute_error(ori_test_labels_entrance, ori_test_predictions_entrance)\n",
    "print(\"Mean Squared Error:\", ori_mse)\n",
    "print(\"Mean Absolute Error:\", ori_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "aa516341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.14690929787311363\n",
      "Mean Absolute Error: 0.2329970111207489\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(test_labels_exit, test_predictions_exit)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(test_labels_exit, test_predictions_exit)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aeb42a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.018831383168699787\n",
      "Mean Absolute Error: 0.11064636230468732\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "ori_mse = mean_squared_error(ori_test_labels_exit, ori_test_predictions_exit)\n",
    "ori_mae = mean_absolute_error(ori_test_labels_exit, ori_test_predictions_exit)\n",
    "print(\"Mean Squared Error:\", ori_mse)\n",
    "print(\"Mean Absolute Error:\", ori_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e699ede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 4.05%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_absolute_percentage_error(y_actual, y_pred):\n",
    "    y_actual, y_pred = np.array(y_actual), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_actual - y_pred) / y_actual)) * 100\n",
    "\n",
    "mape = mean_absolute_percentage_error(test_labels_entrance, test_predictions_entrance)\n",
    "print(f'MAPE: {mape:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5e9d5a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 5.89%\n"
     ]
    }
   ],
   "source": [
    "mape = mean_absolute_percentage_error(ori_test_labels_entrance, ori_test_predictions_entrance)\n",
    "print(f'MAPE: {mape:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "346dbbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 0.31%\n"
     ]
    }
   ],
   "source": [
    "mape = mean_absolute_percentage_error(test_labels_exit, test_predictions_exit)\n",
    "print(f'MAPE: {mape:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6e048300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 0.15%\n"
     ]
    }
   ],
   "source": [
    "mape = mean_absolute_percentage_error(ori_test_labels_exit, ori_test_predictions_exit)\n",
    "print(f'MAPE: {mape:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a6021",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "22ab5183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 19:59:26,141] A new study created in memory with name: no-name-9c42f4f9-a4f3-40aa-9286-4f9467fa5dda\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 19:59:31,796] Trial 0 finished with value: 35.21218395233154 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 24, 'num_neurons_layer_1': 88, 'num_neurons_layer_2': 36, 'learning_rate': 0.00029688254797485846, 'activation': 'tanh'}. Best is trial 0 with value: 35.21218395233154.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 19:59:36,838] Trial 1 finished with value: 0.41268420219421387 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 16, 'num_neurons_layer_1': 96, 'learning_rate': 0.005431063805312115, 'activation': 'relu'}. Best is trial 1 with value: 0.41268420219421387.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 19:59:42,435] Trial 2 finished with value: 7.1848883628845215 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 84, 'num_neurons_layer_1': 108, 'learning_rate': 0.00016218515896232499, 'activation': 'tanh'}. Best is trial 1 with value: 0.41268420219421387.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 19:59:48,675] Trial 3 finished with value: 0.4403950870037079 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 28, 'num_neurons_layer_1': 64, 'num_neurons_layer_2': 36, 'num_neurons_layer_3': 72, 'learning_rate': 0.0002675792453820192, 'activation': 'relu'}. Best is trial 1 with value: 0.41268420219421387.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 19:59:56,211] Trial 4 finished with value: 0.406866118311882 and parameters: {'num_layers': 5, 'num_neurons_layer_0': 28, 'num_neurons_layer_1': 120, 'num_neurons_layer_2': 60, 'num_neurons_layer_3': 120, 'num_neurons_layer_4': 68, 'learning_rate': 0.0002476051384146897, 'activation': 'relu'}. Best is trial 4 with value: 0.406866118311882.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:00:01,452] Trial 5 finished with value: 39.85478973388672 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 56, 'num_neurons_layer_1': 80, 'learning_rate': 0.0001041338723576389, 'activation': 'tanh'}. Best is trial 4 with value: 0.406866118311882.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:00:07,721] Trial 6 finished with value: 0.37131811678409576 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 92, 'num_neurons_layer_1': 60, 'num_neurons_layer_2': 44, 'learning_rate': 0.0006220242650718447, 'activation': 'relu'}. Best is trial 6 with value: 0.37131811678409576.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:00:14,612] Trial 7 finished with value: 3.825786769390106 and parameters: {'num_layers': 5, 'num_neurons_layer_0': 48, 'num_neurons_layer_1': 92, 'num_neurons_layer_2': 48, 'num_neurons_layer_3': 24, 'num_neurons_layer_4': 80, 'learning_rate': 0.00796696378963536, 'activation': 'relu'}. Best is trial 6 with value: 0.37131811678409576.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:00:20,796] Trial 8 finished with value: 0.41180936992168427 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 88, 'num_neurons_layer_1': 60, 'num_neurons_layer_2': 92, 'learning_rate': 0.0013378713081238062, 'activation': 'relu'}. Best is trial 6 with value: 0.37131811678409576.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:00:26,247] Trial 9 finished with value: 0.6084505021572113 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 76, 'num_neurons_layer_1': 92, 'learning_rate': 0.00015472278413263124, 'activation': 'relu'}. Best is trial 6 with value: 0.37131811678409576.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:00:31,084] Trial 10 finished with value: 0.26812744140625 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 124, 'learning_rate': 0.0009670809999791129, 'activation': 'tanh'}. Best is trial 10 with value: 0.26812744140625.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:00:35,726] Trial 11 finished with value: 0.2620934471487999 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 128, 'learning_rate': 0.0009345713813502808, 'activation': 'tanh'}. Best is trial 11 with value: 0.2620934471487999.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:00:40,393] Trial 12 finished with value: 0.22476675361394882 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 128, 'learning_rate': 0.001869139049049143, 'activation': 'tanh'}. Best is trial 12 with value: 0.22476675361394882.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:00:45,128] Trial 13 finished with value: 0.27170631289482117 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 128, 'learning_rate': 0.002661310500592517, 'activation': 'tanh'}. Best is trial 12 with value: 0.22476675361394882.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:00:49,958] Trial 14 finished with value: 0.2520553059875965 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 108, 'learning_rate': 0.0022485700619034902, 'activation': 'tanh'}. Best is trial 12 with value: 0.22476675361394882.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:00:54,664] Trial 15 finished with value: 0.24775905534625053 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 108, 'learning_rate': 0.0025987194275211237, 'activation': 'tanh'}. Best is trial 12 with value: 0.22476675361394882.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:00:59,425] Trial 16 finished with value: 0.2459508366882801 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 108, 'learning_rate': 0.0036166716005556696, 'activation': 'tanh'}. Best is trial 12 with value: 0.22476675361394882.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:01:06,103] Trial 17 finished with value: 1.530828483402729 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 108, 'num_neurons_layer_1': 24, 'num_neurons_layer_2': 128, 'num_neurons_layer_3': 16, 'learning_rate': 0.0043290733967078585, 'activation': 'tanh'}. Best is trial 12 with value: 0.22476675361394882.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:01:11,088] Trial 18 finished with value: 1.527335699647665 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 100, 'num_neurons_layer_1': 16, 'learning_rate': 0.0019100103576920751, 'activation': 'tanh'}. Best is trial 12 with value: 0.22476675361394882.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:01:15,389] Trial 19 finished with value: 0.25528082996606827 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 116, 'learning_rate': 0.004089336761802498, 'activation': 'tanh'}. Best is trial 12 with value: 0.22476675361394882.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:01:20,112] Trial 20 finished with value: 2.4037650898098946 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 64, 'num_neurons_layer_1': 40, 'learning_rate': 0.0005524560734453439, 'activation': 'tanh'}. Best is trial 12 with value: 0.22476675361394882.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:01:24,515] Trial 21 finished with value: 0.23025309666991234 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 112, 'learning_rate': 0.003021496564314291, 'activation': 'tanh'}. Best is trial 12 with value: 0.22476675361394882.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:01:29,040] Trial 22 finished with value: 0.25297911465168 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 116, 'learning_rate': 0.009670329878318909, 'activation': 'tanh'}. Best is trial 12 with value: 0.22476675361394882.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:01:33,547] Trial 23 finished with value: 0.2811254933476448 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 100, 'learning_rate': 0.0014605557652305559, 'activation': 'tanh'}. Best is trial 12 with value: 0.22476675361394882.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:01:39,140] Trial 24 finished with value: 0.20581506937742233 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 116, 'num_neurons_layer_1': 128, 'learning_rate': 0.003446273471114667, 'activation': 'tanh'}. Best is trial 24 with value: 0.20581506937742233.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:01:44,491] Trial 25 finished with value: 0.307591550052166 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 120, 'num_neurons_layer_1': 128, 'learning_rate': 0.0059377887294659705, 'activation': 'tanh'}. Best is trial 24 with value: 0.20581506937742233.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:01:49,725] Trial 26 finished with value: 0.2552456185221672 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 96, 'num_neurons_layer_1': 112, 'learning_rate': 0.00288397157709807, 'activation': 'tanh'}. Best is trial 24 with value: 0.20581506937742233.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:01:54,709] Trial 27 finished with value: 0.33648041635751724 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 80, 'num_neurons_layer_1': 40, 'learning_rate': 0.0015440302459744251, 'activation': 'tanh'}. Best is trial 24 with value: 0.20581506937742233.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:02:01,112] Trial 28 finished with value: 0.5128492191433907 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 116, 'num_neurons_layer_1': 128, 'num_neurons_layer_2': 88, 'learning_rate': 0.0058761048050015, 'activation': 'tanh'}. Best is trial 24 with value: 0.20581506937742233.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:02:07,768] Trial 29 finished with value: 0.3604346923530102 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 68, 'num_neurons_layer_1': 108, 'num_neurons_layer_2': 16, 'num_neurons_layer_3': 128, 'learning_rate': 0.0005036633902180943, 'activation': 'tanh'}. Best is trial 24 with value: 0.20581506937742233.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:02:12,112] Trial 30 finished with value: 0.328628271818161 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 40, 'learning_rate': 0.001887728348418398, 'activation': 'tanh'}. Best is trial 24 with value: 0.20581506937742233.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:02:16,783] Trial 31 finished with value: 0.21795573085546494 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 108, 'learning_rate': 0.00364506460713454, 'activation': 'tanh'}. Best is trial 24 with value: 0.20581506937742233.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:02:21,394] Trial 32 finished with value: 0.29584889486432076 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 120, 'learning_rate': 0.00377139674683263, 'activation': 'tanh'}. Best is trial 24 with value: 0.20581506937742233.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:02:26,653] Trial 33 finished with value: 0.19650382176041603 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 104, 'num_neurons_layer_1': 76, 'learning_rate': 0.003162248567006876, 'activation': 'tanh'}. Best is trial 33 with value: 0.19650382176041603.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:02:32,004] Trial 34 finished with value: 0.2683088667690754 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 100, 'num_neurons_layer_1': 72, 'learning_rate': 0.004911637482875042, 'activation': 'tanh'}. Best is trial 33 with value: 0.19650382176041603.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:02:38,354] Trial 35 finished with value: 0.16556208208203316 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 128, 'num_neurons_layer_1': 76, 'num_neurons_layer_2': 124, 'learning_rate': 0.0020152718027541413, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:02:44,854] Trial 36 finished with value: 1.527197178453207 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 88, 'num_neurons_layer_1': 48, 'num_neurons_layer_2': 124, 'learning_rate': 0.007104394461243354, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:02:51,786] Trial 37 finished with value: 1.3295589685440063 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 104, 'num_neurons_layer_1': 72, 'num_neurons_layer_2': 104, 'num_neurons_layer_3': 68, 'learning_rate': 0.0032769061192439086, 'activation': 'relu'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:02:58,099] Trial 38 finished with value: 0.22876622527837753 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 120, 'num_neurons_layer_1': 84, 'num_neurons_layer_2': 108, 'learning_rate': 0.0011924770233816921, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:03:04,071] Trial 39 finished with value: 0.6495141983032227 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 92, 'num_neurons_layer_1': 48, 'num_neurons_layer_2': 72, 'learning_rate': 0.004662806530693074, 'activation': 'relu'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:03:09,584] Trial 40 finished with value: 0.2731059268116951 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 124, 'num_neurons_layer_1': 104, 'learning_rate': 0.0007433621943405467, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:03:14,955] Trial 41 finished with value: 0.2664371356368065 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 128, 'num_neurons_layer_1': 80, 'learning_rate': 0.002105705547750138, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:03:21,221] Trial 42 finished with value: 0.2787623256444931 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 112, 'num_neurons_layer_1': 100, 'num_neurons_layer_2': 112, 'learning_rate': 0.0016934030816304919, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:03:26,315] Trial 43 finished with value: 0.2002953141927719 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 124, 'num_neurons_layer_1': 68, 'learning_rate': 0.0024222663751937486, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:03:31,511] Trial 44 finished with value: 0.4061339795589447 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 120, 'num_neurons_layer_1': 64, 'learning_rate': 0.0025067804515564195, 'activation': 'relu'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:03:38,094] Trial 45 finished with value: 0.2776757925748825 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 112, 'num_neurons_layer_1': 76, 'num_neurons_layer_2': 84, 'num_neurons_layer_3': 100, 'learning_rate': 0.0012657245819003749, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:03:44,022] Trial 46 finished with value: 0.23848029226064682 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 124, 'num_neurons_layer_1': 52, 'num_neurons_layer_2': 72, 'learning_rate': 0.0033583317494340027, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:03:49,594] Trial 47 finished with value: 0.20021960139274597 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 104, 'num_neurons_layer_1': 88, 'learning_rate': 0.006247676441225755, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:03:54,085] Trial 48 finished with value: 0.33917438983917236 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 20, 'num_neurons_layer_1': 88, 'learning_rate': 0.006534664405621414, 'activation': 'relu'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:03:58,996] Trial 49 finished with value: 0.2744418866932392 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 80, 'num_neurons_layer_1': 68, 'learning_rate': 0.008376427217609056, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:04:05,633] Trial 50 finished with value: 1.5266606956720352 and parameters: {'num_layers': 5, 'num_neurons_layer_0': 92, 'num_neurons_layer_1': 84, 'num_neurons_layer_2': 16, 'num_neurons_layer_3': 44, 'num_neurons_layer_4': 24, 'learning_rate': 0.0024038968042204086, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:04:10,996] Trial 51 finished with value: 0.22408035025000572 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 104, 'num_neurons_layer_1': 116, 'learning_rate': 0.0052755051440806985, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:04:17,062] Trial 52 finished with value: 0.21584459021687508 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 104, 'num_neurons_layer_1': 96, 'num_neurons_layer_2': 100, 'learning_rate': 0.003997615042881017, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:04:23,164] Trial 53 finished with value: 0.3278237283229828 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 124, 'num_neurons_layer_1': 96, 'num_neurons_layer_2': 100, 'learning_rate': 0.00478058297154847, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:04:29,053] Trial 54 finished with value: 0.23300965130329132 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 104, 'num_neurons_layer_1': 76, 'num_neurons_layer_2': 120, 'learning_rate': 0.00282381616547827, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:04:34,189] Trial 55 finished with value: 0.24748185276985168 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 96, 'num_neurons_layer_1': 84, 'learning_rate': 0.004087635897657211, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:04:40,000] Trial 56 finished with value: 0.20157593488693237 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 116, 'num_neurons_layer_1': 64, 'num_neurons_layer_2': 116, 'learning_rate': 0.0021473718323392634, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:04:45,129] Trial 57 finished with value: 0.274687297642231 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 128, 'num_neurons_layer_1': 60, 'learning_rate': 0.0011483004926220151, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:04:50,208] Trial 58 finished with value: 0.3730230927467346 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 116, 'num_neurons_layer_1': 56, 'learning_rate': 0.0008471519316615073, 'activation': 'relu'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:04:56,155] Trial 59 finished with value: 0.19824137911200523 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 112, 'num_neurons_layer_1': 68, 'num_neurons_layer_2': 116, 'learning_rate': 0.002157628668056872, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:05:01,993] Trial 60 finished with value: 0.19919058308005333 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 112, 'num_neurons_layer_1': 68, 'num_neurons_layer_2': 116, 'learning_rate': 0.0021500020016657143, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:05:07,804] Trial 61 finished with value: 0.3663274236023426 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 112, 'num_neurons_layer_1': 68, 'num_neurons_layer_2': 116, 'learning_rate': 0.0020809014872315242, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:05:13,154] Trial 62 finished with value: 0.2182997688651085 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 56, 'num_neurons_layer_1': 68, 'num_neurons_layer_2': 128, 'learning_rate': 0.0017265792520230504, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:05:19,858] Trial 63 finished with value: 0.2905588187277317 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 120, 'num_neurons_layer_1': 64, 'num_neurons_layer_2': 116, 'num_neurons_layer_3': 96, 'learning_rate': 0.0014164372565806738, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:05:25,721] Trial 64 finished with value: 0.20024679973721504 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 112, 'num_neurons_layer_1': 76, 'num_neurons_layer_2': 112, 'learning_rate': 0.002280047404727227, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:05:32,299] Trial 65 finished with value: 0.34982649609446526 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 124, 'num_neurons_layer_1': 76, 'num_neurons_layer_2': 108, 'num_neurons_layer_3': 52, 'learning_rate': 0.002495039290798613, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:05:38,173] Trial 66 finished with value: 0.24007777869701385 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 108, 'num_neurons_layer_1': 80, 'num_neurons_layer_2': 128, 'learning_rate': 0.0030819590294426263, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:05:44,084] Trial 67 finished with value: 0.19088805839419365 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 112, 'num_neurons_layer_1': 88, 'num_neurons_layer_2': 96, 'learning_rate': 0.001678829858917739, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:05:49,921] Trial 68 finished with value: 0.2182762809097767 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 96, 'num_neurons_layer_1': 88, 'num_neurons_layer_2': 100, 'learning_rate': 0.0015588336745234633, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:05:55,252] Trial 69 finished with value: 0.2474994659423828 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 32, 'num_neurons_layer_1': 92, 'num_neurons_layer_2': 92, 'learning_rate': 0.0010445928651200756, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:06:01,303] Trial 70 finished with value: 0.20481907948851585 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 100, 'num_neurons_layer_1': 76, 'num_neurons_layer_2': 120, 'learning_rate': 0.00138061066944395, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:06:07,099] Trial 71 finished with value: 0.1869850791990757 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 112, 'num_neurons_layer_1': 72, 'num_neurons_layer_2': 108, 'learning_rate': 0.0018473298460238063, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:06:12,748] Trial 72 finished with value: 0.25098852813243866 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 112, 'num_neurons_layer_1': 72, 'num_neurons_layer_2': 108, 'learning_rate': 0.0018071328871042337, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:06:18,362] Trial 73 finished with value: 0.1961853764951229 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 108, 'num_neurons_layer_1': 80, 'num_neurons_layer_2': 112, 'learning_rate': 0.0015745731006781174, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:06:24,908] Trial 74 finished with value: 0.2436147779226303 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 104, 'num_neurons_layer_1': 88, 'num_neurons_layer_2': 120, 'num_neurons_layer_3': 96, 'learning_rate': 0.0016337966071527698, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:06:30,456] Trial 75 finished with value: 32.07254219055176 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 88, 'num_neurons_layer_1': 84, 'num_neurons_layer_2': 96, 'learning_rate': 0.00010693899093260481, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:06:36,054] Trial 76 finished with value: 0.28973933309316635 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 108, 'num_neurons_layer_1': 80, 'num_neurons_layer_2': 80, 'learning_rate': 0.0010304503076288196, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:06:42,258] Trial 77 finished with value: 0.19763054698705673 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 100, 'num_neurons_layer_1': 92, 'num_neurons_layer_2': 108, 'learning_rate': 0.0019713157569839685, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:06:47,977] Trial 78 finished with value: 0.39756572246551514 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 96, 'num_neurons_layer_1': 72, 'num_neurons_layer_2': 108, 'learning_rate': 0.0019218641621493633, 'activation': 'relu'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:06:54,630] Trial 79 finished with value: 12.552001714706421 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 116, 'num_neurons_layer_1': 100, 'num_neurons_layer_2': 112, 'num_neurons_layer_3': 40, 'learning_rate': 0.0004298154702702151, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:07:00,341] Trial 80 finished with value: 0.19217174872756004 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 100, 'num_neurons_layer_1': 60, 'num_neurons_layer_2': 124, 'learning_rate': 0.0020049684373044468, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:07:05,966] Trial 81 finished with value: 0.26413894072175026 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 100, 'num_neurons_layer_1': 64, 'num_neurons_layer_2': 124, 'learning_rate': 0.0020098092684542423, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:07:11,728] Trial 82 finished with value: 0.17511086910963058 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 84, 'num_neurons_layer_1': 56, 'num_neurons_layer_2': 120, 'learning_rate': 0.002850498202075651, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:07:17,287] Trial 83 finished with value: 0.22373748198151588 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 84, 'num_neurons_layer_1': 56, 'num_neurons_layer_2': 124, 'learning_rate': 0.0027302490972694092, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:07:22,906] Trial 84 finished with value: 0.1788523644208908 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 92, 'num_neurons_layer_1': 60, 'num_neurons_layer_2': 104, 'learning_rate': 0.0012820597576365018, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:07:28,402] Trial 85 finished with value: 0.2862778566777706 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 72, 'num_neurons_layer_1': 44, 'num_neurons_layer_2': 104, 'learning_rate': 0.000866357171244789, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:07:34,046] Trial 86 finished with value: 0.3063046336174011 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 92, 'num_neurons_layer_1': 56, 'num_neurons_layer_2': 104, 'learning_rate': 0.001269731209561774, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:07:39,545] Trial 87 finished with value: 0.2118481695652008 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 80, 'num_neurons_layer_1': 32, 'num_neurons_layer_2': 128, 'learning_rate': 0.0015116936118646563, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:07:45,078] Trial 88 finished with value: 0.41877366229891777 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 88, 'num_neurons_layer_1': 52, 'num_neurons_layer_2': 60, 'learning_rate': 0.0011228178450478983, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:07:51,074] Trial 89 finished with value: 0.28980210795998573 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 84, 'num_neurons_layer_1': 80, 'num_neurons_layer_2': 92, 'num_neurons_layer_3': 112, 'learning_rate': 0.0006797974361962589, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:07:56,271] Trial 90 finished with value: 1.2948951721191406 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 100, 'num_neurons_layer_1': 92, 'num_neurons_layer_2': 112, 'learning_rate': 0.001732910779610893, 'activation': 'relu'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:08:01,394] Trial 91 finished with value: 0.2741427980363369 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 108, 'num_neurons_layer_1': 60, 'num_neurons_layer_2': 120, 'learning_rate': 0.0031230279237017605, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:08:06,481] Trial 92 finished with value: 0.25982605665922165 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 96, 'num_neurons_layer_1': 72, 'num_neurons_layer_2': 124, 'learning_rate': 0.0013202290562634345, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:08:11,672] Trial 93 finished with value: 0.2783374972641468 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 100, 'num_neurons_layer_1': 60, 'num_neurons_layer_2': 112, 'learning_rate': 0.0018913310633804966, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:08:16,569] Trial 94 finished with value: 0.2663700617849827 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 92, 'num_neurons_layer_1': 52, 'num_neurons_layer_2': 100, 'learning_rate': 0.002663784360595625, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:08:21,663] Trial 95 finished with value: 0.1830802857875824 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 108, 'num_neurons_layer_1': 64, 'num_neurons_layer_2': 116, 'learning_rate': 0.002291167302303741, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:08:26,602] Trial 96 finished with value: 0.22013089805841446 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 104, 'num_neurons_layer_1': 56, 'num_neurons_layer_2': 108, 'learning_rate': 0.0015917611014746955, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:08:31,400] Trial 97 finished with value: 0.361565500497818 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 108, 'num_neurons_layer_1': 48, 'num_neurons_layer_2': 24, 'learning_rate': 0.0028977992569463643, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:08:36,358] Trial 98 finished with value: 0.19161754474043846 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 96, 'num_neurons_layer_1': 60, 'num_neurons_layer_2': 120, 'learning_rate': 0.0025238841195434638, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2090917937.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:08:42,082] Trial 99 finished with value: 1.5290288999676704 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 88, 'num_neurons_layer_1': 60, 'num_neurons_layer_2': 124, 'num_neurons_layer_3': 76, 'learning_rate': 0.003663581519937471, 'activation': 'tanh'}. Best is trial 35 with value: 0.16556208208203316.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'num_layers': 3, 'num_neurons_layer_0': 128, 'num_neurons_layer_1': 76, 'num_neurons_layer_2': 124, 'learning_rate': 0.0020152718027541413, 'activation': 'tanh'}\n",
      "Best trial loss:  0.16556208208203316\n",
      "Epoch 1/50\n",
      "43/43 [==============================] - 0s 460us/step - loss: 71.4810\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 0s 434us/step - loss: 58.2108\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 0s 468us/step - loss: 46.7501\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 0s 481us/step - loss: 35.5258\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 0s 474us/step - loss: 24.4124\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 0s 468us/step - loss: 13.3999\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 0s 470us/step - loss: 5.1535\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 0s 470us/step - loss: 2.9875\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 0s 481us/step - loss: 2.9364\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 0s 474us/step - loss: 2.9332\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 0s 477us/step - loss: 2.9321\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 0s 485us/step - loss: 2.9335\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 0s 470us/step - loss: 2.9370\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 0s 485us/step - loss: 2.9330\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 0s 464us/step - loss: 2.9339\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 0s 462us/step - loss: 2.9338\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 0s 480us/step - loss: 2.9310\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 0s 468us/step - loss: 2.9338\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 0s 477us/step - loss: 2.9322\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 0s 461us/step - loss: 2.8407\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 0s 466us/step - loss: 1.5108\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 0s 474us/step - loss: 1.0115\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 0s 465us/step - loss: 0.9123\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 0s 477us/step - loss: 0.8551\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 0s 473us/step - loss: 0.7700\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 0s 480us/step - loss: 0.7455\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 0s 484us/step - loss: 0.7421\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 0s 473us/step - loss: 0.6476\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 0s 485us/step - loss: 0.7318\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 0s 472us/step - loss: 0.6618\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 0s 482us/step - loss: 0.6266\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 0s 477us/step - loss: 0.6828\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 0s 471us/step - loss: 0.6306\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 0s 467us/step - loss: 0.6098\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 0s 478us/step - loss: 0.5880\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 0s 467us/step - loss: 0.5807\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 0s 477us/step - loss: 0.5976\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 0s 469us/step - loss: 0.5469\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 0s 487us/step - loss: 0.5627\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 0s 485us/step - loss: 0.5262\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 0s 468us/step - loss: 0.5690\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 0s 481us/step - loss: 0.5724\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 0s 487us/step - loss: 0.5489\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 0s 488us/step - loss: 0.4725\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 0s 491us/step - loss: 0.5252\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 0s 474us/step - loss: 0.5269\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 0s 473us/step - loss: 0.4165\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 0s 467us/step - loss: 0.5677\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 0s 478us/step - loss: 0.4284\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 0s 459us/step - loss: 0.5445\n",
      "Epoch 1/50\n",
      "43/43 [==============================] - 0s 448us/step - loss: 59.3132\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 0s 425us/step - loss: 45.9851\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 0s 467us/step - loss: 34.5835\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 0s 470us/step - loss: 23.3933\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 0s 471us/step - loss: 12.3035\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 0s 479us/step - loss: 2.2626\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 0s 472us/step - loss: 0.1493\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 0s 473us/step - loss: 0.0944\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 0s 466us/step - loss: 0.0958\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 0s 481us/step - loss: 0.0959\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 0s 472us/step - loss: 0.0967\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 0s 470us/step - loss: 0.0962\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 0s 478us/step - loss: 0.0971\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 0s 463us/step - loss: 0.0967\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 0s 479us/step - loss: 0.0971\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 0s 484us/step - loss: 0.0956\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 0s 471us/step - loss: 0.0962\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 0s 462us/step - loss: 0.0972\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 0s 461us/step - loss: 0.1003\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 0s 468us/step - loss: 0.0949\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 0s 476us/step - loss: 0.0957\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 0s 469us/step - loss: 0.0985\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 0s 470us/step - loss: 0.0998\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 0s 469us/step - loss: 0.0965\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 0s 475us/step - loss: 0.0983\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 0s 468us/step - loss: 0.0958\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 0s 467us/step - loss: 0.0954\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 0s 476us/step - loss: 0.0948\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 0s 475us/step - loss: 0.1005\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 0s 486us/step - loss: 0.1003\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 0s 473us/step - loss: 0.1008\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 0s 476us/step - loss: 0.1015\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 0s 464us/step - loss: 0.0971\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 0s 466us/step - loss: 0.0972\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 0s 459us/step - loss: 0.1007\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 0s 486us/step - loss: 0.1002\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 0s 468us/step - loss: 0.0985\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 0s 461us/step - loss: 0.0961\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 0s 463us/step - loss: 0.0990\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 0s 471us/step - loss: 0.0979\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 0s 488us/step - loss: 0.0966\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 0s 455us/step - loss: 0.0993\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 0s 467us/step - loss: 0.0964\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 0s 464us/step - loss: 0.0978\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 0s 469us/step - loss: 0.0968\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 0s 471us/step - loss: 0.0990\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 0s 470us/step - loss: 0.1023\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 0s 464us/step - loss: 0.0961\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 0s 458us/step - loss: 0.1050\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 0s 474us/step - loss: 0.1057\n",
      "11/11 [==============================] - 0s 337us/step - loss: 0.6057\n",
      "Test set loss for entrance: 0.6056539416313171\n",
      "11/11 [==============================] - 0s 330us/step - loss: 0.1045\n",
      "Test set loss for exit: 0.1044766828417778\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4271\n",
      "v2. Test set loss for entrance: 0.427093505859375\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1144\n",
      "v2. Test set loss for exit: 0.11435699462890625\n",
      "Test features Entrance: [[30.801  6.707 38.854  5.678]\n",
      " [29.503  6.682 39.698  5.676]\n",
      " [29.395  6.68  39.768  5.675]\n",
      " ...\n",
      " [ 7.877  2.515 36.767  2.167]\n",
      " [ 7.906  2.485 36.52   2.148]\n",
      " [ 7.935  2.455 36.273  2.129]]\n",
      "\n",
      "v2. Test features Entrance: [[29.08  6.44 38.33  5.46]\n",
      " [ 7.    2.42 38.36  2.1 ]\n",
      " [ 7.64  2.76 38.77  2.32]]\n",
      "\n",
      "Test result Entrance: {'dnn_model': 0.6056539416313171}\n",
      "\n",
      "v2. Test result Entrance: {'dnn_model': 0.427093505859375}\n",
      "\n",
      "Test features Exit: [[38.725  8.388 42.346  7.157]\n",
      " [38.654  8.374 42.309  7.142]\n",
      " [38.648  8.373 42.305  7.141]\n",
      " ...\n",
      " [39.462  8.606 42.878  7.328]\n",
      " [39.633  8.643 42.963  7.361]\n",
      " [39.804  8.681 43.048  7.394]]\n",
      "\n",
      "v2. Test features Exit: [[38.8   8.45 42.5   7.19]\n",
      " [38.43  8.36 42.28  7.11]\n",
      " [38.07  8.3  42.19  7.06]]\n",
      "Test result Exit: {'dnn_model': 0.1044766828417778}\n",
      "\n",
      "v2. Test result Exit: {'dnn_model': 0.11435699462890625}\n",
      "\n",
      "11/11 [==============================] - 0s 297us/step\n",
      "Test Predictions Entrance: [75.63  76.595 76.662 76.729 76.936 77.264 77.364 77.739 78.652 79.041\n",
      " 81.104 82.315 82.594 83.031 83.181 85.079 85.481 85.681 86.417 86.726\n",
      " 86.863 87.314 87.406 87.9   87.928 87.943 87.964 87.971 87.989 87.992\n",
      " 87.996 88.016 88.027 88.05  88.054 88.06  88.067 88.097 88.11  88.116\n",
      " 88.123 88.126 88.143 88.149 88.162 88.191 88.217 88.223 88.23  88.236\n",
      " 88.249 88.255 88.265 88.254 88.238 88.202 88.195 88.162 88.122 88.118\n",
      " 88.064 88.06  88.057 88.05  88.018 88.014 88.004 88.    87.982 87.978\n",
      " 87.941 87.915 87.895 87.895 87.893 87.891 87.889 87.888 87.878 87.877\n",
      " 87.876 87.875 87.872 87.871 87.87  87.867 87.857 87.631 87.57  87.367\n",
      " 86.184 85.565 85.24  84.457 82.668 81.527 80.753 79.576 78.06  77.919\n",
      " 77.788 77.371 76.671 76.537 76.724 76.756 76.851 76.985 77.133 77.173\n",
      " 77.303 77.35  77.452 79.215 79.751 79.963 80.232 80.297 80.361 80.73\n",
      " 80.789 81.015 81.334 81.643 81.908 82.19  82.427 82.56  82.963 83.884\n",
      " 84.148 84.833 84.973 85.573 86.182 86.221 86.367 86.501 86.664 86.801\n",
      " 86.992 87.367 87.535 87.786 87.853 88.135 88.376 88.363 88.293 88.279\n",
      " 88.183 88.156 88.129 88.089 88.035 87.996 87.983 87.97  87.932 87.819\n",
      " 87.747 87.689 87.677 87.666 87.643 87.632 87.61  87.577 87.535 87.484\n",
      " 87.349 87.278 87.261 87.228 87.233 87.405 87.424 87.524 87.586 87.669\n",
      " 87.755 87.888 87.956 87.979 88.025 88.049 88.168 88.516 88.542 88.567\n",
      " 88.747 88.798 88.95  89.026 89.1   89.516 89.579 89.62  89.652 89.61\n",
      " 89.594 89.567 89.562 89.545 89.534 89.386 89.368 89.29  89.278 89.21\n",
      " 89.197 89.185 89.166 89.147 89.083 89.037 88.571 88.427 88.286 88.149\n",
      " 87.772 87.151 86.728 86.498 85.81  84.83  84.62  84.516 84.117 84.021\n",
      " 83.552 83.367 82.021 81.712 81.49  81.342 80.584 80.486 80.284 79.496\n",
      " 78.817 79.182 79.344 79.926 82.122 82.411 82.721 82.937 83.046 83.157\n",
      " 83.957 84.076 84.435 84.672 85.848 86.389 86.481 86.581 90.468 90.708\n",
      " 90.702 90.697 90.692 90.688 90.68  90.671 90.67  90.667 90.652 90.649\n",
      " 90.641 90.633 90.631 90.626 90.624 90.608 90.605 90.6   90.598 90.587\n",
      " 90.58  90.549 90.538 90.52  90.501 90.494 90.454 90.45  90.442 90.417\n",
      " 90.404 90.382 90.377 90.332 90.284 90.259 90.249 90.228 90.213 90.197\n",
      " 90.176 90.138 90.127 90.126 90.158 90.177 90.259 90.271 90.287 90.346\n",
      " 90.396 90.405 90.442 90.465 90.469 90.473 90.478 90.531 90.535 90.558\n",
      " 90.573 90.58  90.594 90.611 90.54  90.387 90.34  90.315 90.302 90.276\n",
      " 90.034 90.017 89.852 89.833 89.793 89.711 89.467 89.396 89.146 88.88 ]\n",
      "\n",
      "11/11 [==============================] - 0s 289us/step\n",
      "Test Predictions Exit: [74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996\n",
      " 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996 74.996]\n",
      "\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "v2. Test Predictions Entrance: [76.333 90.712 90.624]\n",
      "\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "v2. Test Predictions Exit: [74.996 74.996 74.996]\n",
      "\n",
      "Error entrance: 11      0.251049\n",
      "23     -0.299128\n",
      "24     -0.358680\n",
      "25     -0.418453\n",
      "28     -0.590340\n",
      "          ...   \n",
      "1660    1.715559\n",
      "1671    1.831093\n",
      "1674    1.858057\n",
      "1684    1.935436\n",
      "1694    1.996808\n",
      "Name: Temperature (Â°F), Length: 340, dtype: float64\n",
      "\n",
      "Error exit: 11      0.251049\n",
      "23     -0.299128\n",
      "24     -0.358680\n",
      "25     -0.418453\n",
      "28     -0.590340\n",
      "          ...   \n",
      "1660    1.715559\n",
      "1671    1.831093\n",
      "1674    1.858057\n",
      "1684    1.935436\n",
      "1694    1.996808\n",
      "Name: Temperature (Â°F), Length: 340, dtype: float64\n",
      "\n",
      "v2. Error entrance: 5     0.793221\n",
      "12   -0.057964\n",
      "15    0.430100\n",
      "Name: Temperature (°F), dtype: float64\n",
      "\n",
      "v2. Error exit: 5     0.793221\n",
      "12   -0.057964\n",
      "15    0.430100\n",
      "Name: Temperature (°F), dtype: float64\n",
      "\n",
      "Average error for entrance: 0.51783629661736\n",
      "\n",
      "Average error for exit: -0.0006522336868104282\n",
      "\n",
      "v2. Average error for entrance: 0.38845243326822754\n",
      "\n",
      "v2. Average error for exit: 0.38845243326822754\n",
      "\n",
      "Mean Squared Error for entrance: 0.6199103859556772\n",
      "Mean Squared Error for exit: 0.01759975018648049\n",
      "\n",
      "v2. Mean Squared Error for entrance: 0.2725152782332309\n",
      "v2. Mean Squared Error for exit: 0.013939637743249472\n",
      "\n",
      "Mean Absolute Error for entrance: 0.6056544630720933\n",
      "Mean Absolute Error for exit: 0.10447322495852451\n",
      "\n",
      "v2. Mean Absolute Error for entrance: 0.42709501139322487\n",
      "v2. Mean Absolute Error for exit: 0.11434875488281193\n",
      "\n",
      "MAPE for entrance: 0.71%\n",
      "MAPE for exit: 0.14%\n",
      "v2. MAPE for entrance: 0.14%\n",
      "v2. MAPE for exit: 0.14%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'num_layers': 1, 'num_neurons_layer_0': 100, 'learning_rate': 0.0027491513753489564, 'activation': 'tanh'}\n",
      "Best trial loss:  0.20058700069785118\n",
      "Epoch 1/50\n",
      "43/43 [==============================] - 0s 715us/step - loss: 78.1137\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 64.8663\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 52.6162\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 40.4349\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 28.3209\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 16.2468\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 6.1000\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 0s 595us/step - loss: 3.0265\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 2.9334\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 0s 645us/step - loss: 2.1299\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 0s 595us/step - loss: 1.0665\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.9620\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.8699\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.8206\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.8178\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.7724\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.7639\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.7477\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.7026\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.6974\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.6547\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.6162\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.6550\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.5806\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.5966\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.6714\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.5975\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.6026\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.5384\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.5450\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.5578\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.5421\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.5627\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 0s 595us/step - loss: 0.5670\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.5372\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.5458\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.6278\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.5283\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.5200\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.4890\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.4856\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.4992\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.5068\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.4758\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.4781\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.5400\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 0s 667us/step - loss: 0.4816\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.4723\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.4861\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 0s 667us/step - loss: 0.5067\n",
      "Epoch 1/50\n",
      "43/43 [==============================] - 0s 667us/step - loss: 66.6590\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 0s 690us/step - loss: 53.5686\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 0s 667us/step - loss: 41.4910\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 0s 640us/step - loss: 29.4458\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 17.4297\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 5.4519\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 0s 667us/step - loss: 0.2921\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.0982\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0937\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 0s 595us/step - loss: 0.0943\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0951\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0986\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.0969\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0967\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0955\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.0982\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 0s 595us/step - loss: 0.0958\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0984\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0995\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 0s 616us/step - loss: 0.0954\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0971\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.0984\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.0981\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0982\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.0945\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.1022\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.0959\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.0932\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.0940\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0990\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0942\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.0998\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.1049\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.1007\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0931\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 0s 595us/step - loss: 0.0956\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.0949\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.1013\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.0947\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0920\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0932\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 0s 595us/step - loss: 0.1035\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.1049\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0922\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 0s 621us/step - loss: 0.1038\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0945\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 0s 643us/step - loss: 0.0946\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.1014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "43/43 [==============================] - 0s 619us/step - loss: 0.0938\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 0s 595us/step - loss: 0.0914\n",
      "11/11 [==============================] - 0s 799us/step - loss: 0.4616\n",
      "Test set loss for entrance: 0.4616129398345947\n",
      "11/11 [==============================] - 0s 800us/step - loss: 0.1062\n",
      "Test set loss for exit: 0.10615528374910355\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0336\n",
      "v2. Test set loss for entrance: 1.0335668325424194\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.1702\n",
      "v2. Test set loss for exit: 0.17015838623046875\n",
      "Test features Entrance: [[30.801  6.707 38.854  5.678]\n",
      " [29.503  6.682 39.698  5.676]\n",
      " [29.395  6.68  39.768  5.675]\n",
      " ...\n",
      " [ 7.877  2.515 36.767  2.167]\n",
      " [ 7.906  2.485 36.52   2.148]\n",
      " [ 7.935  2.455 36.273  2.129]]\n",
      "\n",
      "v2. Test features Entrance: [[31.29  6.52 37.49  5.53]\n",
      " [11.87  3.93 40.75  3.44]\n",
      " [14.23  4.08 38.    3.52]]\n",
      "\n",
      "Test result Entrance: {'dnn_model': 0.4616129398345947}\n",
      "\n",
      "v2. Test result Entrance: {'dnn_model': 1.0335668325424194}\n",
      "\n",
      "Test features Exit: [[38.725  8.388 42.346  7.157]\n",
      " [38.654  8.374 42.309  7.142]\n",
      " [38.648  8.373 42.305  7.141]\n",
      " ...\n",
      " [39.462  8.606 42.878  7.328]\n",
      " [39.633  8.643 42.963  7.361]\n",
      " [39.804  8.681 43.048  7.394]]\n",
      "\n",
      "v2. Test features Exit: [[37.82  8.26 42.16  7.03]\n",
      " [38.12  8.32 42.29  7.09]\n",
      " [37.56  8.32 41.25  6.99]]\n",
      "Test result Exit: {'dnn_model': 0.10615528374910355}\n",
      "\n",
      "v2. Test result Exit: {'dnn_model': 0.17015838623046875}\n",
      "\n",
      "11/11 [==============================] - 0s 697us/step\n",
      "Test Predictions Entrance: [75.346 76.285 76.373 76.463 76.746 77.153 77.26  77.594 78.187 78.434\n",
      " 80.21  81.501 81.77  82.185 82.327 83.986 84.298 84.454 85.073 85.375\n",
      " 85.523 86.099 86.239 87.161 87.223 87.255 87.303 87.319 87.359 87.367\n",
      " 87.375 87.422 87.446 87.5   87.508 87.524 87.54  87.61  87.641 87.656\n",
      " 87.672 87.68  87.718 87.734 87.765 87.834 87.895 87.911 87.926 87.941\n",
      " 87.972 87.987 88.01  87.984 87.945 87.859 87.843 87.764 87.669 87.661\n",
      " 87.533 87.525 87.517 87.501 87.428 87.42  87.396 87.388 87.348 87.339\n",
      " 87.258 87.201 87.16  87.159 87.155 87.149 87.145 87.144 87.12  87.119\n",
      " 87.116 87.113 87.107 87.104 87.103 87.094 87.072 86.626 86.518 86.188\n",
      " 84.884 84.383 84.13  83.499 81.829 80.638 79.797 78.743 77.717 77.619\n",
      " 77.522 77.147 76.224 76.013 76.249 76.29  76.416 76.591 76.775 76.822\n",
      " 76.966 77.015 77.115 78.32  78.728 78.904 79.14  79.199 79.257 79.611\n",
      " 79.67  79.906 80.263 80.624 80.929 81.242 81.497 81.637 82.069 83.102\n",
      " 83.396 84.107 84.243 84.824 85.517 85.571 85.784 85.992 86.247 86.45\n",
      " 86.703 87.113 87.269 87.479 87.532 87.744 87.917 87.907 87.856 87.846\n",
      " 87.775 87.754 87.734 87.703 87.663 87.633 87.622 87.612 87.582 87.491\n",
      " 87.431 87.381 87.371 87.361 87.341 87.331 87.311 87.281 87.241 87.191\n",
      " 87.053 86.974 86.954 86.915 86.919 87.088 87.106 87.2   87.256 87.331\n",
      " 87.406 87.519 87.576 87.594 87.632 87.651 87.745 88.007 88.026 88.045\n",
      " 88.176 88.213 88.325 88.381 88.436 88.767 88.821 88.857 88.886 88.848\n",
      " 88.834 88.811 88.806 88.792 88.782 88.658 88.644 88.581 88.572 88.519\n",
      " 88.509 88.499 88.485 88.47  88.422 88.388 88.054 87.951 87.847 87.745\n",
      " 87.439 86.846 86.374 86.099 85.287 84.353 84.158 84.06  83.665 83.566\n",
      " 83.073 82.879 81.487 81.128 80.86  80.681 79.87  79.78  79.598 78.96\n",
      " 78.51  78.728 78.835 79.268 81.567 81.866 82.165 82.367 82.468 82.57\n",
      " 83.286 83.387 83.686 83.879 85.6   86.461 86.568 86.677 89.749 90.179\n",
      " 90.17  90.162 90.156 90.15  90.139 90.126 90.124 90.119 90.098 90.095\n",
      " 90.083 90.072 90.07  90.064 90.061 90.039 90.036 90.029 90.026 90.01\n",
      " 89.997 89.937 89.917 89.884 89.85  89.837 89.77  89.764 89.75  89.71\n",
      " 89.69  89.657 89.65  89.584 89.518 89.484 89.471 89.445 89.425 89.405\n",
      " 89.379 89.333 89.319 89.319 89.357 89.38  89.486 89.501 89.524 89.607\n",
      " 89.682 89.697 89.758 89.795 89.803 89.81  89.818 89.915 89.923 89.968\n",
      " 89.998 90.013 90.042 90.08  89.923 89.647 89.572 89.535 89.516 89.479\n",
      " 89.175 89.156 88.983 88.963 88.925 88.847 88.633 88.574 88.377 88.179]\n",
      "\n",
      "11/11 [==============================] - 0s 798us/step\n",
      "Test Predictions Exit: [75.081 75.08  75.08  75.079 75.079 75.079 75.078 75.078 75.077 75.077\n",
      " 75.075 75.074 75.074 75.073 75.073 75.072 75.072 75.071 75.071 75.071\n",
      " 75.07  75.07  75.07  75.069 75.071 75.072 75.074 75.075 75.077 75.077\n",
      " 75.078 75.08  75.081 75.083 75.084 75.084 75.085 75.088 75.089 75.09\n",
      " 75.09  75.09  75.092 75.093 75.094 75.096 75.098 75.099 75.1   75.1\n",
      " 75.101 75.102 75.102 75.103 75.103 75.103 75.103 75.101 75.098 75.098\n",
      " 75.09  75.09  75.089 75.088 75.081 75.08  75.077 75.076 75.071 75.07\n",
      " 75.056 75.045 75.043 75.045 75.052 75.06  75.065 75.066 75.083 75.084\n",
      " 75.085 75.086 75.087 75.088 75.088 75.088 75.085 75.084 75.084 75.084\n",
      " 75.084 75.084 75.083 75.083 75.083 75.083 75.082 75.082 75.082 75.082\n",
      " 75.082 75.082 75.081 75.081 75.08  75.08  75.08  75.079 75.079 75.079\n",
      " 75.079 75.079 75.078 75.076 75.076 75.076 75.075 75.075 75.075 75.074\n",
      " 75.074 75.074 75.073 75.073 75.072 75.072 75.072 75.072 75.072 75.074\n",
      " 75.074 75.075 75.075 75.076 75.078 75.078 75.078 75.078 75.079 75.079\n",
      " 75.08  75.081 75.081 75.081 75.081 75.082 75.082 75.082 75.082 75.082\n",
      " 75.081 75.081 75.081 75.081 75.081 75.081 75.08  75.08  75.08  75.08\n",
      " 75.079 75.079 75.079 75.079 75.079 75.079 75.079 75.079 75.078 75.078\n",
      " 75.077 75.077 75.076 75.076 75.076 75.078 75.078 75.079 75.079 75.08\n",
      " 75.081 75.081 75.082 75.082 75.082 75.083 75.083 75.085 75.086 75.086\n",
      " 75.087 75.087 75.088 75.088 75.089 75.091 75.092 75.092 75.092 75.091\n",
      " 75.091 75.09  75.09  75.09  75.089 75.086 75.086 75.084 75.083 75.082\n",
      " 75.082 75.081 75.081 75.08  75.079 75.078 75.078 75.078 75.078 75.078\n",
      " 75.078 75.079 75.079 75.08  75.08  75.081 75.081 75.081 75.082 75.082\n",
      " 75.082 75.082 75.084 75.084 75.084 75.084 75.085 75.085 75.085 75.086\n",
      " 75.086 75.086 75.086 75.085 75.083 75.082 75.082 75.082 75.082 75.082\n",
      " 75.081 75.081 75.08  75.08  75.078 75.077 75.077 75.076 75.074 75.074\n",
      " 75.075 75.076 75.076 75.077 75.078 75.079 75.079 75.079 75.081 75.082\n",
      " 75.082 75.083 75.084 75.084 75.084 75.086 75.086 75.087 75.087 75.088\n",
      " 75.09  75.097 75.099 75.102 75.106 75.107 75.112 75.113 75.114 75.117\n",
      " 75.118 75.12  75.121 75.125 75.128 75.13  75.13  75.132 75.132 75.133\n",
      " 75.134 75.136 75.136 75.136 75.135 75.134 75.129 75.128 75.127 75.122\n",
      " 75.116 75.115 75.11  75.107 75.106 75.105 75.104 75.093 75.092 75.086\n",
      " 75.082 75.08  75.075 75.069 75.068 75.074 75.075 75.076 75.077 75.077\n",
      " 75.083 75.083 75.087 75.087 75.088 75.089 75.092 75.093 75.096 75.099]\n",
      "\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "v2. Test Predictions Entrance: [74.494 88.602 85.859]\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "v2. Test Predictions Exit: [75.055 75.063 75.061]\n",
      "\n",
      "Error entrance: 11     -0.032803\n",
      "23     -0.609575\n",
      "24     -0.647673\n",
      "25     -0.683841\n",
      "28     -0.780022\n",
      "          ...   \n",
      "1660    0.851133\n",
      "1671    0.996513\n",
      "1674    1.035883\n",
      "1684    1.166339\n",
      "1694    1.295750\n",
      "Name: Temperature (Â°F), Length: 340, dtype: float64\n",
      "\n",
      "Error exit: 11     -0.032803\n",
      "23     -0.609575\n",
      "24     -0.647673\n",
      "25     -0.683841\n",
      "28     -0.780022\n",
      "          ...   \n",
      "1660    0.851133\n",
      "1671    0.996513\n",
      "1674    1.035883\n",
      "1684    1.166339\n",
      "1694    1.295750\n",
      "Name: Temperature (Â°F), Length: 340, dtype: float64\n",
      "\n",
      "v2. Error entrance: 5     0.733912\n",
      "12   -0.567568\n",
      "15    1.799222\n",
      "Name: Temperature (°F), dtype: float64\n",
      "\n",
      "v2. Error exit: 5     0.733912\n",
      "12   -0.567568\n",
      "15    1.799222\n",
      "Name: Temperature (°F), dtype: float64\n",
      "\n",
      "Average error for entrance: -0.08533690320456089\n",
      "\n",
      "Average error for exit: 0.08899398231301724\n",
      "\n",
      "v2. Average error for entrance: 0.6551888020833303\n",
      "\n",
      "v2. Average error for exit: 0.6551888020833303\n",
      "\n",
      "Mean Squared Error for entrance: 0.2998475921104851\n",
      "Mean Squared Error for exit: 0.02460784411821848\n",
      "\n",
      "v2. Mean Squared Error for entrance: 1.3659869615741027\n",
      "v2. Mean Squared Error for exit: 0.029215514831319584\n",
      "\n",
      "Mean Absolute Error for entrance: 0.46161229056715325\n",
      "Mean Absolute Error for exit: 0.10615537535472198\n",
      "\n",
      "v2. Mean Absolute Error for entrance: 1.0335673014322897\n",
      "v2. Mean Absolute Error for exit: 0.1701575724283894\n",
      "\n",
      "MAPE for entrance: 0.54%\n",
      "MAPE for exit: 0.14%\n",
      "v2. MAPE for entrance: 0.14%\n",
      "v2. MAPE for exit: 0.14%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import optuna\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "trial_losses = []\n",
    "\n",
    "# Function to create a model with the given hyperparameters\n",
    "# Function to create a model with a specific number of neurons in each layer\n",
    "def create_model(input_shape, neurons_per_layer, activation, learning_rate):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Add the first layer with input shape\n",
    "    model.add(layers.Dense(neurons_per_layer[0], activation=activation, input_shape=(input_shape,)))\n",
    "    \n",
    "    # Add subsequent layers with specified number of neurons\n",
    "    for neurons in neurons_per_layer[1:]:\n",
    "        model.add(layers.Dense(neurons, activation=activation))\n",
    "    \n",
    "    model.add(layers.Dense(1))  # Output layer\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "                  loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# Objective function to optimize both entrance and exit models\n",
    "def objective(trial):\n",
    "    # Suggest number of layers\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 5)\n",
    "    \n",
    "    # Suggest a different number of neurons for each layer\n",
    "    neurons_per_layer = []\n",
    "    for i in range(num_layers):\n",
    "        neurons = trial.suggest_int(f'num_neurons_layer_{i}', 16, 128, step=4)  # Each layer can have 16 to 128 neurons\n",
    "        neurons_per_layer.append(neurons)\n",
    "    \n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
    "\n",
    "    # Create models for both entrance and exit\n",
    "    model_entrance = create_model(train_features_entrance.shape[1], neurons_per_layer, activation, learning_rate)\n",
    "    model_exit = create_model(train_features_exit.shape[1], neurons_per_layer, activation, learning_rate)\n",
    "\n",
    "    # Split data into training and validation for both entrance and exit\n",
    "    x_train_entrance, x_val_entrance, y_train_entrance, y_val_entrance = train_test_split(train_features_entrance, train_labels_entrance, test_size=0.2, random_state=42)\n",
    "    x_train_exit, x_val_exit, y_train_exit, y_val_exit = train_test_split(train_features_exit, train_labels_exit, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train both models\n",
    "    model_entrance.fit(x_train_entrance, y_train_entrance, validation_data=(x_val_entrance, y_val_entrance), \n",
    "                       epochs=100, batch_size=32, verbose=0)\n",
    "    model_exit.fit(x_train_exit, y_train_exit, validation_data=(x_val_exit, y_val_exit), \n",
    "                   epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate both models on validation data\n",
    "    val_loss_entrance = model_entrance.evaluate(x_val_entrance, y_val_entrance, verbose=0)\n",
    "    val_loss_exit = model_exit.evaluate(x_val_exit, y_val_exit, verbose=0)\n",
    "\n",
    "    # Combine the two objectives by returning a weighted sum\n",
    "    combined_loss = 0.5 * val_loss_entrance + 0.5 * val_loss_exit\n",
    "    trial_losses.append(combined_loss)  # Append the loss to the list\n",
    "    \n",
    "    return combined_loss\n",
    "\n",
    "# Create a study and optimize both models simultaneously\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(trial_losses, label='Loss per Trial', marker='o', color='blue')\n",
    "plt.title('Optimization Progress: Loss per Trial')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Objective Value (Loss)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "print(\"Best trial loss: \", study.best_value)\n",
    "\n",
    "# Build the final optimized models\n",
    "def build_best_model(best_params, input_shape):\n",
    "    num_layers = best_params['num_layers']\n",
    "    \n",
    "    # Reconstruct neurons per layer using the best parameters\n",
    "    neurons_per_layer = [best_params[f'num_neurons_layer_{i}'] for i in range(num_layers)]\n",
    "    activation = best_params['activation']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "\n",
    "    model = create_model(input_shape, neurons_per_layer, activation, learning_rate)\n",
    "    return model\n",
    "\n",
    "# Create the final models using the best hyperparameters\n",
    "final_model_entrance = build_best_model(study.best_params, train_features_entrance.shape[1])\n",
    "final_model_exit = build_best_model(study.best_params, train_features_exit.shape[1])\n",
    "\n",
    "\n",
    "# Train the final models on the full datasets\n",
    "final_model_entrance.fit(train_features_entrance, train_labels_entrance, epochs=50, batch_size=32, verbose=1)\n",
    "final_model_exit.fit(train_features_exit, train_labels_exit, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "#----------------------\n",
    "# Optionally, evaluate on a test set (if you have one)\n",
    "test_loss_entrance = final_model_entrance.evaluate(test_features_entrance, test_labels_entrance, verbose=1)\n",
    "print(f\"Test set loss for entrance: {test_loss_entrance}\")\n",
    "test_loss_exit = final_model_exit.evaluate(test_features_exit, test_labels_exit, verbose=1)\n",
    "print(f\"Test set loss for exit: {test_loss_exit}\")\n",
    "\n",
    "ori_test_loss_entrance = final_model_entrance.evaluate(ori_test_features_entrance, ori_test_labels_entrance, verbose=1)\n",
    "print(f\"v2. Test set loss for entrance: {ori_test_loss_entrance}\")\n",
    "ori_test_loss_exit = final_model_exit.evaluate(ori_test_features_exit, ori_test_labels_exit, verbose=1)\n",
    "print(f\"v2. Test set loss for exit: {ori_test_loss_exit}\")\n",
    "\n",
    "#----------------------\n",
    "test_features_entrance = np.asarray(test_features_entrance).astype(np.float32)\n",
    "print(\"Test features Entrance:\", test_features_entrance)\n",
    "print()\n",
    "\n",
    "ori_test_features_entrance = np.asarray(ori_test_features_entrance).astype(np.float32)\n",
    "print(\"v2. Test features Entrance:\", ori_test_features_entrance)\n",
    "print()\n",
    "\n",
    "#----------------------\n",
    "test_results_entrance = {}\n",
    "test_results_entrance['dnn_model'] = final_model_entrance.evaluate(test_features_entrance, test_labels_entrance, verbose=0)\n",
    "print(\"Test result Entrance:\", test_results_entrance)\n",
    "print()\n",
    "\n",
    "ori_test_results_entrance = {}\n",
    "ori_test_results_entrance['dnn_model'] = final_model_entrance.evaluate(ori_test_features_entrance, ori_test_labels_entrance, verbose=0)\n",
    "print(\"v2. Test result Entrance:\", ori_test_results_entrance)\n",
    "print()\n",
    "\n",
    "#----------------------\n",
    "test_features_exit = np.asarray(test_features_exit).astype(np.float32)\n",
    "print(\"Test features Exit:\", test_features_exit)\n",
    "print()\n",
    "\n",
    "ori_test_features_exit = np.asarray(ori_test_features_exit).astype(np.float32)\n",
    "print(\"v2. Test features Exit:\", ori_test_features_exit)\n",
    "\n",
    "#----------------------\n",
    "test_results_exit = {}\n",
    "test_results_exit['dnn_model'] = final_model_exit.evaluate(test_features_exit, test_labels_exit, verbose=0)\n",
    "print(\"Test result Exit:\", test_results_exit)\n",
    "print()\n",
    "\n",
    "ori_test_results_exit = {}\n",
    "ori_test_results_exit['dnn_model'] = final_model_exit.evaluate(ori_test_features_exit, ori_test_labels_exit, verbose=0)\n",
    "print(\"v2. Test result Exit:\", ori_test_results_exit)\n",
    "print()\n",
    "\n",
    "#----------------------\n",
    "pd.DataFrame(test_results_entrance, index=['Mean absolute error [Temperature]']).T\n",
    "\n",
    "pd.DataFrame(test_results_exit, index=['Mean absolute error [Temperature]']).T\n",
    "\n",
    "pd.DataFrame(ori_test_results_entrance, index=['Mean absolute error [Temperature]']).T\n",
    "\n",
    "pd.DataFrame(ori_test_results_exit, index=['Mean absolute error [Temperature]']).T\n",
    "\n",
    "#----------------------\n",
    "test_predictions_entrance = final_model_entrance.predict(test_features_entrance).flatten()\n",
    "print(\"Test Predictions Entrance:\", test_predictions_entrance)\n",
    "print()\n",
    "\n",
    "test_predictions_exit = final_model_exit.predict(test_features_exit).flatten()\n",
    "print(\"Test Predictions Exit:\", test_predictions_exit)\n",
    "print()\n",
    "\n",
    "ori_test_predictions_entrance = final_model_entrance.predict(ori_test_features_entrance).flatten()\n",
    "print(\"v2. Test Predictions Entrance:\", ori_test_predictions_entrance)\n",
    "print()\n",
    "\n",
    "ori_test_predictions_exit = final_model_exit.predict(ori_test_features_exit).flatten()\n",
    "print(\"v2. Test Predictions Exit:\", ori_test_predictions_exit)\n",
    "print()\n",
    "\n",
    "#----------------------\n",
    "error_entrance = test_predictions_entrance - test_labels_entrance\n",
    "print(\"Error entrance:\", error_entrance)\n",
    "print()\n",
    "\n",
    "error_exit = test_predictions_entrance - test_labels_entrance\n",
    "print(\"Error exit:\", error_exit)\n",
    "print()\n",
    "\n",
    "ori_error_entrance = ori_test_predictions_entrance - ori_test_labels_entrance\n",
    "print(\"v2. Error entrance:\", ori_error_entrance)\n",
    "print()\n",
    "\n",
    "ori_error_exit = ori_test_predictions_entrance - ori_test_labels_entrance\n",
    "print(\"v2. Error exit:\", ori_error_exit)\n",
    "print()\n",
    "\n",
    "#----------------------\n",
    "plt.hist(error_entrance, bins=25, color='lightblue')\n",
    "plt.xlabel('Prediction Error [Temperature]')\n",
    "_ = plt.ylabel('Count')\n",
    "\n",
    "error_exit = test_predictions_exit - test_labels_exit\n",
    "error_exit\n",
    "\n",
    "plt.hist(error_exit, bins=25, color='lightblue')\n",
    "plt.xlabel('Prediction Error [Temperature]')\n",
    "_ = plt.ylabel('Count')\n",
    "\n",
    "# average error\n",
    "average_error_entrance = np.mean(error_entrance)\n",
    "print(\"Average error for entrance:\", average_error_entrance)\n",
    "print()\n",
    "\n",
    "average_error_exit = np.mean(error_exit)\n",
    "print(\"Average error for exit:\", average_error_exit)\n",
    "print()\n",
    "\n",
    "ori_average_error_entrance = np.mean(ori_error_entrance)\n",
    "print(\"v2. Average error for entrance:\", ori_average_error_entrance)\n",
    "print()\n",
    "\n",
    "ori_average_error_exit = np.mean(ori_error_exit)\n",
    "print(\"v2. Average error for exit:\", ori_average_error_exit)\n",
    "print()\n",
    "\n",
    "# mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse_entrance = mean_squared_error(test_labels_entrance, test_predictions_entrance)\n",
    "mse_exit = mean_squared_error(test_labels_exit, test_predictions_exit)\n",
    "print(\"Mean Squared Error for entrance:\", mse_entrance)\n",
    "print(\"Mean Squared Error for exit:\", mse_exit)\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "ori_mse_entrance = mean_squared_error(ori_test_labels_entrance, ori_test_predictions_entrance)\n",
    "ori_mse_exit = mean_squared_error(ori_test_labels_exit, ori_test_predictions_exit)\n",
    "print(\"v2. Mean Squared Error for entrance:\", ori_mse_entrance)\n",
    "print(\"v2. Mean Squared Error for exit:\", ori_mse_exit)\n",
    "print()\n",
    "\n",
    "# mean absolute error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae_entrance = mean_absolute_error(test_labels_entrance, test_predictions_entrance)\n",
    "mae_exit = mean_absolute_error(test_labels_exit, test_predictions_exit)\n",
    "print(\"Mean Absolute Error for entrance:\", mae_entrance)\n",
    "print(\"Mean Absolute Error for exit:\", mae_exit)\n",
    "print()\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "ori_mae_entrance = mean_absolute_error(ori_test_labels_entrance, ori_test_predictions_entrance)\n",
    "ori_mae_exit = mean_absolute_error(ori_test_labels_exit, ori_test_predictions_exit)\n",
    "print(\"v2. Mean Absolute Error for entrance:\", ori_mae_entrance)\n",
    "print(\"v2. Mean Absolute Error for exit:\", ori_mae_exit)\n",
    "print()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def mean_absolute_percentage_error(y_actual, y_pred):\n",
    "    y_actual, y_pred = np.array(y_actual), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_actual - y_pred) / y_actual)) * 100\n",
    "\n",
    "mape = mean_absolute_percentage_error(test_labels_entrance, test_predictions_entrance)\n",
    "print(f'MAPE for entrance: {mape:.2f}%')\n",
    "\n",
    "mape = mean_absolute_percentage_error(test_labels_exit, test_predictions_exit)\n",
    "print(f'MAPE for exit: {mape:.2f}%')\n",
    "\n",
    "ori_mape = mean_absolute_percentage_error(ori_test_labels_entrance, ori_test_predictions_entrance)\n",
    "print(f'v2. MAPE for entrance: {mape:.2f}%')\n",
    "\n",
    "ori_mape = mean_absolute_percentage_error(ori_test_labels_exit, ori_test_predictions_exit)\n",
    "print(f'v2. MAPE for exit: {mape:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e0bbd8ed-ac38-43ff-b240-96313bf41863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 20:08:44,848] A new study created in memory with name: no-name-18632748-1f79-4f76-9248-22e67f03bef2\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:08:48,740] Trial 0 finished with value: 0.600098729133606 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 40, 'num_neurons_layer_1': 20, 'learning_rate': 0.0041680748699093224, 'activation': 'relu'}. Best is trial 0 with value: 0.600098729133606.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:08:52,743] Trial 1 finished with value: 2.592792581766844 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 52, 'num_neurons_layer_1': 124, 'learning_rate': 0.00016171547070405767, 'activation': 'tanh'}. Best is trial 0 with value: 0.600098729133606.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:08:57,886] Trial 2 finished with value: 0.40546126663684845 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 20, 'num_neurons_layer_1': 64, 'num_neurons_layer_2': 72, 'num_neurons_layer_3': 76, 'learning_rate': 0.001758427114504454, 'activation': 'relu'}. Best is trial 2 with value: 0.40546126663684845.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:09:03,989] Trial 3 finished with value: 1.5268753208220005 and parameters: {'num_layers': 5, 'num_neurons_layer_0': 128, 'num_neurons_layer_1': 48, 'num_neurons_layer_2': 16, 'num_neurons_layer_3': 96, 'num_neurons_layer_4': 44, 'learning_rate': 0.0021639827624466376, 'activation': 'tanh'}. Best is trial 2 with value: 0.40546126663684845.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:09:07,730] Trial 4 finished with value: 0.4462410658597946 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 120, 'learning_rate': 0.0011915142976188827, 'activation': 'relu'}. Best is trial 2 with value: 0.40546126663684845.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:09:12,976] Trial 5 finished with value: 0.5162580460309982 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 116, 'num_neurons_layer_1': 24, 'num_neurons_layer_2': 40, 'num_neurons_layer_3': 36, 'learning_rate': 0.001133521370337334, 'activation': 'relu'}. Best is trial 2 with value: 0.40546126663684845.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:09:18,261] Trial 6 finished with value: 0.44121450185775757 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 56, 'num_neurons_layer_1': 52, 'num_neurons_layer_2': 112, 'num_neurons_layer_3': 124, 'learning_rate': 0.0004914487020416664, 'activation': 'relu'}. Best is trial 2 with value: 0.40546126663684845.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:09:22,063] Trial 7 finished with value: 0.5194017104804516 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 60, 'num_neurons_layer_1': 20, 'learning_rate': 0.0018915897959799956, 'activation': 'tanh'}. Best is trial 2 with value: 0.40546126663684845.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:09:27,731] Trial 8 finished with value: 0.8599313944578171 and parameters: {'num_layers': 5, 'num_neurons_layer_0': 24, 'num_neurons_layer_1': 52, 'num_neurons_layer_2': 60, 'num_neurons_layer_3': 44, 'num_neurons_layer_4': 44, 'learning_rate': 0.0017115750305582073, 'activation': 'relu'}. Best is trial 2 with value: 0.40546126663684845.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:09:31,079] Trial 9 finished with value: 0.49390222132205963 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 32, 'learning_rate': 0.0012441561487113013, 'activation': 'relu'}. Best is trial 2 with value: 0.40546126663684845.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:09:36,923] Trial 10 finished with value: 1.538405418395996 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 88, 'num_neurons_layer_1': 96, 'num_neurons_layer_2': 104, 'num_neurons_layer_3': 76, 'learning_rate': 0.009713224105090068, 'activation': 'tanh'}. Best is trial 2 with value: 0.40546126663684845.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:09:42,889] Trial 11 finished with value: 0.4049941301345825 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 76, 'num_neurons_layer_1': 72, 'num_neurons_layer_2': 124, 'num_neurons_layer_3': 128, 'learning_rate': 0.000300650010136195, 'activation': 'relu'}. Best is trial 11 with value: 0.4049941301345825.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:09:47,899] Trial 12 finished with value: 0.3831314444541931 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 88, 'num_neurons_layer_1': 84, 'num_neurons_layer_2': 84, 'learning_rate': 0.0004364882533019218, 'activation': 'relu'}. Best is trial 12 with value: 0.3831314444541931.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:09:53,048] Trial 13 finished with value: 0.35319483280181885 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 88, 'num_neurons_layer_1': 84, 'num_neurons_layer_2': 128, 'learning_rate': 0.00037206572351275815, 'activation': 'relu'}. Best is trial 13 with value: 0.35319483280181885.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:09:58,182] Trial 14 finished with value: 0.5186541378498077 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 96, 'num_neurons_layer_1': 92, 'num_neurons_layer_2': 96, 'learning_rate': 0.00047737667140537575, 'activation': 'relu'}. Best is trial 13 with value: 0.35319483280181885.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:10:03,523] Trial 15 finished with value: 0.46404731273651123 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 100, 'num_neurons_layer_1': 96, 'num_neurons_layer_2': 92, 'learning_rate': 0.00010551959809282206, 'activation': 'relu'}. Best is trial 13 with value: 0.35319483280181885.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:10:08,673] Trial 16 finished with value: 0.4135698825120926 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 80, 'num_neurons_layer_1': 116, 'num_neurons_layer_2': 124, 'learning_rate': 0.0005627808287323203, 'activation': 'relu'}. Best is trial 13 with value: 0.35319483280181885.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:10:13,017] Trial 17 finished with value: 0.5323068052530289 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 108, 'num_neurons_layer_1': 84, 'learning_rate': 0.0002643290538210195, 'activation': 'relu'}. Best is trial 13 with value: 0.35319483280181885.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:10:18,049] Trial 18 finished with value: 3.361759789288044 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 72, 'num_neurons_layer_1': 108, 'num_neurons_layer_2': 80, 'learning_rate': 0.0002521464400401012, 'activation': 'tanh'}. Best is trial 13 with value: 0.35319483280181885.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:10:22,986] Trial 19 finished with value: 0.3646059185266495 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 92, 'num_neurons_layer_1': 80, 'num_neurons_layer_2': 48, 'learning_rate': 0.0006980015939080209, 'activation': 'relu'}. Best is trial 13 with value: 0.35319483280181885.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:10:27,143] Trial 20 finished with value: 0.4304930716753006 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 68, 'num_neurons_layer_1': 64, 'learning_rate': 0.0007583128006215092, 'activation': 'relu'}. Best is trial 13 with value: 0.35319483280181885.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:10:32,087] Trial 21 finished with value: 0.49947652220726013 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 88, 'num_neurons_layer_1': 84, 'num_neurons_layer_2': 48, 'learning_rate': 0.000365663935413068, 'activation': 'relu'}. Best is trial 13 with value: 0.35319483280181885.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:10:36,921] Trial 22 finished with value: 0.37186677753925323 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 100, 'num_neurons_layer_1': 76, 'num_neurons_layer_2': 28, 'learning_rate': 0.0008242023125915938, 'activation': 'relu'}. Best is trial 13 with value: 0.35319483280181885.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:10:41,728] Trial 23 finished with value: 0.37479422986507416 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 100, 'num_neurons_layer_1': 68, 'num_neurons_layer_2': 24, 'learning_rate': 0.0007112989275260857, 'activation': 'relu'}. Best is trial 13 with value: 0.35319483280181885.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:10:46,708] Trial 24 finished with value: 0.42356249690055847 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 108, 'num_neurons_layer_1': 108, 'num_neurons_layer_2': 36, 'learning_rate': 0.00017359063101332243, 'activation': 'relu'}. Best is trial 13 with value: 0.35319483280181885.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:10:52,036] Trial 25 finished with value: 0.6494275629520416 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 84, 'num_neurons_layer_1': 76, 'num_neurons_layer_2': 56, 'num_neurons_layer_3': 20, 'learning_rate': 0.0007809603323958663, 'activation': 'relu'}. Best is trial 13 with value: 0.35319483280181885.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:10:56,522] Trial 26 finished with value: 0.22011655569076538 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 108, 'num_neurons_layer_1': 104, 'learning_rate': 0.0038450448009095498, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:00,348] Trial 27 finished with value: 0.29172661155462265 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 112, 'learning_rate': 0.003611659559901974, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:04,124] Trial 28 finished with value: 0.25508512184023857 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 112, 'learning_rate': 0.00418385365933915, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:07,746] Trial 29 finished with value: 0.31567978858947754 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 124, 'learning_rate': 0.00455475878385507, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:11,478] Trial 30 finished with value: 0.23288549855351448 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 112, 'learning_rate': 0.0039019046584271837, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:15,251] Trial 31 finished with value: 0.26552607491612434 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 108, 'learning_rate': 0.003928195536957876, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:18,929] Trial 32 finished with value: 0.2599557340145111 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 108, 'learning_rate': 0.00554112858685276, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:22,570] Trial 33 finished with value: 0.2599499709904194 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 120, 'learning_rate': 0.006496415601004933, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:27,274] Trial 34 finished with value: 0.48406291007995605 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 128, 'num_neurons_layer_1': 128, 'learning_rate': 0.007599866398409867, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:31,009] Trial 35 finished with value: 0.27240071445703506 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 120, 'learning_rate': 0.002848188184181364, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:35,507] Trial 36 finished with value: 0.284542977809906 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 116, 'num_neurons_layer_1': 36, 'learning_rate': 0.0025981759069992317, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:39,530] Trial 37 finished with value: 0.280237402766943 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 44, 'num_neurons_layer_1': 108, 'learning_rate': 0.005645456734551432, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:43,278] Trial 38 finished with value: 0.23644845932722092 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 128, 'learning_rate': 0.006678287888328687, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:46,975] Trial 39 finished with value: 0.2765081785619259 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 128, 'learning_rate': 0.009889434608724268, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:51,333] Trial 40 finished with value: 0.2594994679093361 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 116, 'num_neurons_layer_1': 44, 'learning_rate': 0.003070459779185169, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:55,663] Trial 41 finished with value: 0.24568598344922066 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 116, 'num_neurons_layer_1': 36, 'learning_rate': 0.003084437519603887, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:11:59,682] Trial 42 finished with value: 0.24859462678432465 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 124, 'learning_rate': 0.004605169377357031, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:12:03,883] Trial 43 finished with value: 0.29252028837800026 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 124, 'num_neurons_layer_1': 28, 'learning_rate': 0.0021514513293788657, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:12:07,611] Trial 44 finished with value: 0.2611439935863018 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 128, 'learning_rate': 0.0014996665163423805, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:12:11,325] Trial 45 finished with value: 0.29254160448908806 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 120, 'learning_rate': 0.0077010411295608955, 'activation': 'tanh'}. Best is trial 26 with value: 0.22011655569076538.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:12:15,681] Trial 46 finished with value: 0.2038188874721527 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 104, 'num_neurons_layer_1': 36, 'learning_rate': 0.005237310327652666, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:12:19,898] Trial 47 finished with value: 0.3335559517145157 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 104, 'num_neurons_layer_1': 36, 'learning_rate': 0.0033305328536949847, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:12:24,237] Trial 48 finished with value: 0.2610769607126713 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 116, 'num_neurons_layer_1': 60, 'learning_rate': 0.005808302036956662, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:12:28,597] Trial 49 finished with value: 0.21919745206832886 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 96, 'num_neurons_layer_1': 32, 'learning_rate': 0.0024082294023461594, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:12:33,012] Trial 50 finished with value: 0.2506823502480984 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 92, 'num_neurons_layer_1': 28, 'learning_rate': 0.002283998988593229, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:12:37,407] Trial 51 finished with value: 0.3507284037768841 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 104, 'num_neurons_layer_1': 16, 'learning_rate': 0.00710294332180331, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:12:41,678] Trial 52 finished with value: 0.30921275168657303 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 96, 'num_neurons_layer_1': 40, 'learning_rate': 0.0013831450732274516, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:12:46,010] Trial 53 finished with value: 0.23915478959679604 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 112, 'num_neurons_layer_1': 56, 'learning_rate': 0.0024993414708187607, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:12:50,353] Trial 54 finished with value: 0.24892861396074295 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 104, 'num_neurons_layer_1': 56, 'learning_rate': 0.004798504003431676, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:12:56,737] Trial 55 finished with value: 0.3904908038675785 and parameters: {'num_layers': 5, 'num_neurons_layer_0': 96, 'num_neurons_layer_1': 48, 'num_neurons_layer_2': 68, 'num_neurons_layer_3': 56, 'num_neurons_layer_4': 124, 'learning_rate': 0.0017844042200048898, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:13:00,448] Trial 56 finished with value: 0.25730056315660477 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 112, 'learning_rate': 0.0026468907406909624, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:13:04,675] Trial 57 finished with value: 0.31971271708607674 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 100, 'num_neurons_layer_1': 28, 'learning_rate': 0.002071923199878443, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:13:09,040] Trial 58 finished with value: 0.25176529586315155 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 112, 'num_neurons_layer_1': 48, 'learning_rate': 0.003783533848718366, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:13:12,507] Trial 59 finished with value: 0.4878065139055252 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 60, 'learning_rate': 0.008946931085134549, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:13:16,642] Trial 60 finished with value: 0.29769284650683403 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 80, 'num_neurons_layer_1': 20, 'learning_rate': 0.006231258456821097, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:13:21,035] Trial 61 finished with value: 0.47042400762438774 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 120, 'num_neurons_layer_1': 36, 'learning_rate': 0.0010283422892188882, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:13:25,341] Trial 62 finished with value: 0.2683020234107971 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 104, 'num_neurons_layer_1': 32, 'learning_rate': 0.005123189302896576, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:13:29,662] Trial 63 finished with value: 0.22963036224246025 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 112, 'num_neurons_layer_1': 44, 'learning_rate': 0.003336251909505677, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:13:34,840] Trial 64 finished with value: 0.22236378863453865 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 92, 'num_neurons_layer_1': 56, 'num_neurons_layer_2': 112, 'learning_rate': 0.0034968914132286946, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:13:40,611] Trial 65 finished with value: 0.5522619187831879 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 92, 'num_neurons_layer_1': 44, 'num_neurons_layer_2': 112, 'num_neurons_layer_3': 100, 'learning_rate': 0.0033916340499854872, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:13:45,538] Trial 66 finished with value: 0.30513135716319084 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 96, 'num_neurons_layer_1': 52, 'num_neurons_layer_2': 108, 'learning_rate': 0.00432036839906114, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:13:50,279] Trial 67 finished with value: 1.547959804534912 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 68, 'num_neurons_layer_1': 44, 'num_neurons_layer_2': 96, 'learning_rate': 0.008150305837262522, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:13:53,914] Trial 68 finished with value: 0.23196037113666534 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 84, 'learning_rate': 0.0039008175974665745, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:13:58,853] Trial 69 finished with value: 1.52697379514575 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 16, 'num_neurons_layer_1': 24, 'num_neurons_layer_2': 84, 'num_neurons_layer_3': 20, 'learning_rate': 0.0037945852513637875, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:14:04,066] Trial 70 finished with value: 0.3645600974559784 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 84, 'num_neurons_layer_1': 120, 'num_neurons_layer_2': 116, 'learning_rate': 0.0016182378928229137, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:14:07,684] Trial 71 finished with value: 0.2599458135664463 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 84, 'learning_rate': 0.006582694903375505, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:14:11,319] Trial 72 finished with value: 0.28463800996541977 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 92, 'learning_rate': 0.004223593440141118, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:14:14,841] Trial 73 finished with value: 0.24802444875240326 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 76, 'learning_rate': 0.004953173674968051, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:14:18,496] Trial 74 finished with value: 0.2449307143688202 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 108, 'learning_rate': 0.0032611043553982355, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:14:22,100] Trial 75 finished with value: 0.2543186880648136 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 88, 'learning_rate': 0.005153205362336591, 'activation': 'tanh'}. Best is trial 46 with value: 0.2038188874721527.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:14:25,786] Trial 76 finished with value: 0.20234865695238113 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 100, 'learning_rate': 0.0024019178950232005, 'activation': 'tanh'}. Best is trial 76 with value: 0.20234865695238113.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:14:30,780] Trial 77 finished with value: 0.2628150209784508 and parameters: {'num_layers': 3, 'num_neurons_layer_0': 100, 'num_neurons_layer_1': 64, 'num_neurons_layer_2': 120, 'learning_rate': 0.0028137260968938388, 'activation': 'tanh'}. Best is trial 76 with value: 0.20234865695238113.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:14:34,444] Trial 78 finished with value: 0.23249981179833412 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 100, 'learning_rate': 0.0035687567513412967, 'activation': 'tanh'}. Best is trial 76 with value: 0.20234865695238113.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:14:38,136] Trial 79 finished with value: 0.2021159790456295 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 100, 'learning_rate': 0.002449807979404295, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:14:44,662] Trial 80 finished with value: 0.3844594433903694 and parameters: {'num_layers': 5, 'num_neurons_layer_0': 88, 'num_neurons_layer_1': 92, 'num_neurons_layer_2': 76, 'num_neurons_layer_3': 100, 'num_neurons_layer_4': 112, 'learning_rate': 0.0018868885960475282, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:14:48,317] Trial 81 finished with value: 0.23293515294790268 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 100, 'learning_rate': 0.0023188788591231076, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:14:52,006] Trial 82 finished with value: 0.2798144593834877 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 96, 'learning_rate': 0.003088569086332269, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:14:55,849] Trial 83 finished with value: 0.27813127636909485 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 104, 'learning_rate': 0.0036029084200804025, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:14:59,466] Trial 84 finished with value: 0.26852066069841385 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 96, 'learning_rate': 0.002033447576854394, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:15:03,599] Trial 85 finished with value: 0.36059361323714256 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 108, 'num_neurons_layer_1': 16, 'learning_rate': 0.002838323407486226, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:15:07,214] Trial 86 finished with value: 0.28512272611260414 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 100, 'learning_rate': 0.0023287009250349253, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:15:11,501] Trial 87 finished with value: 0.2608920745551586 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 92, 'num_neurons_layer_1': 56, 'learning_rate': 0.004053677463264258, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:15:15,066] Trial 88 finished with value: 0.4231329709291458 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 80, 'learning_rate': 0.0025230845095576297, 'activation': 'relu'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:15:20,022] Trial 89 finished with value: 0.3502637818455696 and parameters: {'num_layers': 4, 'num_neurons_layer_0': 28, 'num_neurons_layer_1': 52, 'num_neurons_layer_2': 64, 'num_neurons_layer_3': 60, 'learning_rate': 0.0033221346250948153, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:15:24,363] Trial 90 finished with value: 0.24140945449471474 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 104, 'num_neurons_layer_1': 68, 'learning_rate': 0.0011999980888946827, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:15:28,088] Trial 91 finished with value: 0.3073914013803005 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 108, 'learning_rate': 0.0036165207009244303, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:15:31,753] Trial 92 finished with value: 0.2434500865638256 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 112, 'learning_rate': 0.004545961212460051, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:15:35,429] Trial 93 finished with value: 0.27768706902861595 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 96, 'learning_rate': 0.0030802742499100298, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:15:39,068] Trial 94 finished with value: 0.26056304574012756 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 100, 'learning_rate': 0.003944997347714044, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:15:43,473] Trial 95 finished with value: 0.26607057079672813 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 92, 'num_neurons_layer_1': 100, 'learning_rate': 0.005746415730473474, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:15:47,147] Trial 96 finished with value: 0.3237280026078224 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 104, 'learning_rate': 0.0027471365562587824, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:15:51,153] Trial 97 finished with value: 0.22957909107208252 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 116, 'learning_rate': 0.0016192624095581996, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:15:55,324] Trial 98 finished with value: 0.3330581448972225 and parameters: {'num_layers': 2, 'num_neurons_layer_0': 108, 'num_neurons_layer_1': 32, 'learning_rate': 0.001436533771690549, 'activation': 'tanh'}. Best is trial 79 with value: 0.2021159790456295.\n",
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_6424/2695465620.py:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-10-24 20:17:08,120] Trial 99 finished with value: 0.3973546475172043 and parameters: {'num_layers': 1, 'num_neurons_layer_0': 116, 'learning_rate': 0.0017071929324178137, 'activation': 'relu'}. Best is trial 79 with value: 0.2021159790456295.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'num_layers': 1, 'num_neurons_layer_0': 100, 'learning_rate': 0.002449807979404295, 'activation': 'tanh'}\n",
      "Best trial loss:  0.2021159790456295\n",
      "Epoch 1/50\n",
      "43/43 [==============================] - 0s 413us/step - loss: 78.8495\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 0s 374us/step - loss: 66.3041\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 0s 316us/step - loss: 55.2445\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 0s 271us/step - loss: 44.3306\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 0s 313us/step - loss: 33.4913\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 0s 296us/step - loss: 22.6976\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 0s 290us/step - loss: 12.0126\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 0s 293us/step - loss: 4.6642\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 0s 280us/step - loss: 2.9505\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 0s 305us/step - loss: 2.9339\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 0s 278us/step - loss: 2.2660\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 0s 285us/step - loss: 1.0014\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 0s 275us/step - loss: 0.8591\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 0s 287us/step - loss: 0.8426\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 0s 281us/step - loss: 0.8243\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 0s 281us/step - loss: 0.7765\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 0s 288us/step - loss: 0.7410\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 0s 292us/step - loss: 0.7208\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 0s 289us/step - loss: 0.7324\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 0s 278us/step - loss: 0.7120\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 0s 302us/step - loss: 0.6452\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 0s 297us/step - loss: 0.6592\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 0s 283us/step - loss: 0.6253\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 0s 290us/step - loss: 0.5925\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 0s 288us/step - loss: 0.5966\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 0s 295us/step - loss: 0.5987\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 0s 282us/step - loss: 0.5592\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 0s 302us/step - loss: 0.5780\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 0s 280us/step - loss: 0.6164\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 0s 279us/step - loss: 0.5621\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 0s 276us/step - loss: 0.5558\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 0s 287us/step - loss: 0.5506\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 0s 288us/step - loss: 0.5351\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 0s 288us/step - loss: 0.5848\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 0s 291us/step - loss: 0.5794\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 0s 290us/step - loss: 0.5115\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 0s 281us/step - loss: 0.5330\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 0s 280us/step - loss: 0.5288\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 0s 289us/step - loss: 0.5246\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 0s 273us/step - loss: 0.5034\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 0s 286us/step - loss: 0.4966\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 0s 284us/step - loss: 0.4888\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 0s 286us/step - loss: 0.5039\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 0s 272us/step - loss: 0.5106\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 0s 282us/step - loss: 0.5296\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 0s 282us/step - loss: 0.5368\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 0s 301us/step - loss: 0.4938\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 0s 287us/step - loss: 0.4854\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 0s 288us/step - loss: 0.4739\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 0s 292us/step - loss: 0.5101\n",
      "Epoch 1/50\n",
      "43/43 [==============================] - 0s 357us/step - loss: 65.6638\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 0s 335us/step - loss: 52.9698\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 0s 292us/step - loss: 42.0960\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 0s 294us/step - loss: 31.3254\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 0s 286us/step - loss: 20.5889\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 0s 286us/step - loss: 9.8741\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 0s 285us/step - loss: 1.1565\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 0s 287us/step - loss: 0.1060\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 0s 277us/step - loss: 0.0944\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 0s 284us/step - loss: 0.0956\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 0s 280us/step - loss: 0.0957\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 0s 284us/step - loss: 0.0949\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 0s 294us/step - loss: 0.0972\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 0s 289us/step - loss: 0.0974\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 0s 278us/step - loss: 0.0947\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 0s 274us/step - loss: 0.0967\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 0s 282us/step - loss: 0.0955\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 0s 303us/step - loss: 0.0947\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 0s 288us/step - loss: 0.0988\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 0s 286us/step - loss: 0.0948\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 0s 288us/step - loss: 0.0966\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 0s 286us/step - loss: 0.0950\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 0s 301us/step - loss: 0.0956\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 0s 294us/step - loss: 0.0972\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 0s 285us/step - loss: 0.0950\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 0s 284us/step - loss: 0.0966\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 0s 294us/step - loss: 0.0942\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 0s 293us/step - loss: 0.0940\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 0s 291us/step - loss: 0.0973\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 0s 291us/step - loss: 0.0953\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 0s 281us/step - loss: 0.0986\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 0s 298us/step - loss: 0.0979\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 0s 294us/step - loss: 0.0933\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 0s 282us/step - loss: 0.0926\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 0s 293us/step - loss: 0.0935\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 0s 298us/step - loss: 0.0979\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 0s 298us/step - loss: 0.1004\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 0s 290us/step - loss: 0.0953\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 0s 282us/step - loss: 0.1000\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 0s 283us/step - loss: 0.0984\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 0s 282us/step - loss: 0.0924\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 0s 285us/step - loss: 0.1103\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 0s 286us/step - loss: 0.0945\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 0s 297us/step - loss: 0.0911\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 0s 291us/step - loss: 0.0916\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 0s 286us/step - loss: 0.0903\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 0s 282us/step - loss: 0.0906\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 0s 293us/step - loss: 0.0917\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 0s 285us/step - loss: 0.0948\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 0s 283us/step - loss: 0.0940\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2748\n",
      "Test set loss for entrance: 0.27482858300209045\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1040\n",
      "Test set loss for exit: 0.10403188318014145\n",
      "Test features Entrance: [[29.08  6.44 38.33  5.46]\n",
      " [ 7.    2.42 38.36  2.1 ]\n",
      " [ 7.64  2.76 38.77  2.32]]\n",
      "\n",
      "Test result Entrance: {'dnn_model': 0.27482858300209045}\n",
      "\n",
      "Test features Exit: [[38.8   8.45 42.5   7.19]\n",
      " [38.43  8.36 42.28  7.11]\n",
      " [38.07  8.3  42.19  7.06]]\n",
      "\n",
      "Test result Exit: {'dnn_model': 0.10403188318014145}\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Test Predictions Entrance: [75.315 90.245 90.12 ]\n",
      "\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Test Predictions Exit: [75.015 75.007 74.996]\n",
      "\n",
      "Error entrance: 5    -0.225478\n",
      "12   -0.524898\n",
      "15   -0.074096\n",
      "Name: Temperature (°F), dtype: float64\n",
      "\n",
      "Error entrance: 5    -0.225478\n",
      "12   -0.524898\n",
      "15   -0.074096\n",
      "Name: Temperature (°F), dtype: float64\n",
      "\n",
      "Average error for entrance: -0.2748242390950537\n",
      "\n",
      "Average error for exit: -0.10403381347656193\n",
      "\n",
      "Mean Squared Error for entrance: 0.11061623013037038\n",
      "Mean Squared Error for exit: 0.012014851246031579\n",
      "\n",
      "Mean Absolute Error for entrance: 0.2748242390950537\n",
      "Mean Absolute Error for exit: 0.10403381347656193\n",
      "\n",
      "MAPE for entrance: 0.32%\n",
      "MAPE for exit: 0.14%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'num_layers': 3, 'num_neurons_layer_0': 76, 'num_neurons_layer_1': 80, 'num_neurons_layer_2': 112, 'learning_rate': 0.0017124008594070982, 'activation': 'tanh'}\n",
      "Best trial loss:  0.17320891469717026\n",
      "Epoch 1/50\n",
      "43/43 [==============================] - 0s 857us/step - loss: 73.1948\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 0s 881us/step - loss: 62.4490\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 53.5721\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 44.9052\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 36.3369\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 0s 1000us/step - loss: 27.8254\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 19.3506\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 10.9932\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 5.0731\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 3.0718\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 2.9371\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 2.9334\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 2.9331\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 2.9350\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 2.9328\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 2.9331\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 2.9330\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 2.9318\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 0s 993us/step - loss: 2.9319\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 2.9325\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 0s 952us/step - loss: 2.9376\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 2.9317\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 2.9319\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 2.9313\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 2.9358\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 2.9169\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 1.8242\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 1.0216\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.8689\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.8439\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.8065\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.8577\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.7148\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.6863\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 0.6172\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 0.6702\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 0s 953us/step - loss: 0.7899\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 0.6245\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 0.6080\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 0s 1000us/step - loss: 0.5721\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.7492\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.6230\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 0.5769\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.5695\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.5536\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.5948\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 0s 1000us/step - loss: 0.5489\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.6943\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.5795\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.5080\n",
      "Epoch 1/50\n",
      "43/43 [==============================] - 0s 905us/step - loss: 61.6427\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 50.7863\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 41.9819\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 33.3583\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 24.8197\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 16.3303\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 7.8732\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.9615\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.1100\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0943\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0948\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0942\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0943\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0956\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0946\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0948\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0968\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0986\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0945\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0953\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0966\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0988\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0954\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0993\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.1001\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0968\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0979\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0967\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0977\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.1022\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0986\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 0s 1000us/step - loss: 0.0975\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 0.0969\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 0s 952us/step - loss: 0.0949\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0953\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 0.0948\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 0s 952us/step - loss: 0.0990\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 0s 953us/step - loss: 0.0987\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 0s 952us/step - loss: 0.0971\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 0s 905us/step - loss: 0.0953\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 0s 929us/step - loss: 0.0948\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 0s 952us/step - loss: 0.0976\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 0s 953us/step - loss: 0.0983\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 0s 881us/step - loss: 0.0968\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 0s 833us/step - loss: 0.1019\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 0s 976us/step - loss: 0.0977\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 0s 952us/step - loss: 0.0954\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 0s 857us/step - loss: 0.0962\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 0s 833us/step - loss: 0.0981\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 0s 952us/step - loss: 0.1029\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.2104\n",
      "Test set loss for entrance: 1.210418701171875\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2311\n",
      "Test set loss for exit: 0.2310689240694046\n",
      "Test features Entrance: [[31.29  6.52 37.49  5.53]\n",
      " [11.87  3.93 40.75  3.44]\n",
      " [14.23  4.08 38.    3.52]]\n",
      "\n",
      "Test result Entrance: {'dnn_model': 1.210418701171875}\n",
      "\n",
      "Test features Exit: [[37.82  8.26 42.16  7.03]\n",
      " [38.12  8.32 42.29  7.09]\n",
      " [37.56  8.32 41.25  6.99]]\n",
      "\n",
      "Test result Exit: {'dnn_model': 0.2310689240694046}\n",
      "\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Test Predictions Entrance: [74.689 88.619 86.211]\n",
      "\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Test Predictions Exit: [74.999 74.999 74.999]\n",
      "\n",
      "Error entrance: 5     0.929049\n",
      "12   -0.551027\n",
      "15    2.151189\n",
      "Name: Temperature (°F), dtype: float64\n",
      "\n",
      "Error entrance: 5     0.929049\n",
      "12   -0.551027\n",
      "15    2.151189\n",
      "Name: Temperature (°F), dtype: float64\n",
      "\n",
      "Average error for entrance: 0.8430702718098928\n",
      "\n",
      "Average error for exit: -0.23106048583984773\n",
      "\n",
      "Mean Squared Error for entrance: 1.9314592953701133\n",
      "Mean Squared Error for exit: 0.0536769481165465\n",
      "\n",
      "Mean Absolute Error for entrance: 1.2104217529296857\n",
      "Mean Absolute Error for exit: 0.23106048583984773\n",
      "\n",
      "MAPE for entrance: 1.48%\n",
      "MAPE for exit: 0.31%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "ori_trial_losses = []\n",
    "\n",
    "# Function to create a model with the given hyperparameters\n",
    "# Function to create a model with a specific number of neurons in each layer\n",
    "def create_model(input_shape, neurons_per_layer, activation, learning_rate):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Add the first layer with input shape\n",
    "    model.add(layers.Dense(neurons_per_layer[0], activation=activation, input_shape=(input_shape,)))\n",
    "    \n",
    "    # Add subsequent layers with specified number of neurons\n",
    "    for neurons in neurons_per_layer[1:]:\n",
    "        model.add(layers.Dense(neurons, activation=activation))\n",
    "    \n",
    "    model.add(layers.Dense(1))  # Output layer\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "                  loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# Objective function to optimize both entrance and exit models\n",
    "def objective(trial):\n",
    "    # Suggest number of layers\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 5)\n",
    "    \n",
    "    # Suggest a different number of neurons for each layer\n",
    "    neurons_per_layer = []\n",
    "    for i in range(num_layers):\n",
    "        neurons = trial.suggest_int(f'num_neurons_layer_{i}', 16, 128, step=4)  # Each layer can have 16 to 128 neurons\n",
    "        neurons_per_layer.append(neurons)\n",
    "    \n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
    "\n",
    "    # Create models for both entrance and exit\n",
    "    ori_model_entrance = create_model(train_features_entrance.shape[1], neurons_per_layer, activation, learning_rate)\n",
    "    ori_model_exit = create_model(train_features_exit.shape[1], neurons_per_layer, activation, learning_rate)\n",
    "\n",
    "    # Split data into training and validation for both entrance and exit\n",
    "    ori_x_train_entrance, ori_x_val_entrance, ori_y_train_entrance, ori_y_val_entrance = train_test_split(train_features_entrance, train_labels_entrance, test_size=0.2, random_state=42)\n",
    "    ori_x_train_exit, ori_x_val_exit, ori_y_train_exit, ori_y_val_exit = train_test_split(train_features_exit, train_labels_exit, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train both models\n",
    "    ori_model_entrance.fit(ori_x_train_entrance, ori_y_train_entrance, validation_data=(ori_x_val_entrance, ori_y_val_entrance), \n",
    "                       epochs=100, batch_size=32, verbose=0)\n",
    "    ori_model_exit.fit(ori_x_train_exit, ori_y_train_exit, validation_data=(ori_x_val_exit, ori_y_val_exit), \n",
    "                   epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate both models on validation data\n",
    "    ori_val_loss_entrance = ori_model_entrance.evaluate(ori_x_val_entrance, ori_y_val_entrance, verbose=0)\n",
    "    ori_val_loss_exit = ori_model_exit.evaluate(ori_x_val_exit, ori_y_val_exit, verbose=0)\n",
    "\n",
    "    # Combine the two objectives by returning a weighted sum\n",
    "    ori_combined_loss = 0.5 * ori_val_loss_entrance + 0.5 * ori_val_loss_exit\n",
    "    ori_trial_losses.append(ori_combined_loss)  # Append the loss to the list\n",
    "    \n",
    "    return ori_combined_loss\n",
    "\n",
    "# Create a study and optimize both models simultaneously\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ori_trial_losses, label='Loss per Trial', marker='o', color='blue')\n",
    "plt.title('Optimization Progress: Loss per Trial')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Objective Value (Loss)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "print(\"Best trial loss: \", study.best_value)\n",
    "\n",
    "# Build the final optimized models\n",
    "def build_best_model(best_params, input_shape):\n",
    "    num_layers = best_params['num_layers']\n",
    "    \n",
    "    # Reconstruct neurons per layer using the best parameters\n",
    "    neurons_per_layer = [best_params[f'num_neurons_layer_{i}'] for i in range(num_layers)]\n",
    "    activation = best_params['activation']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "\n",
    "    model = create_model(input_shape, neurons_per_layer, activation, learning_rate)\n",
    "    return model\n",
    "\n",
    "# Create the final models using the best hyperparameters\n",
    "ori_final_model_entrance = build_best_model(study.best_params, train_features_entrance.shape[1])\n",
    "ori_final_model_exit = build_best_model(study.best_params, train_features_exit.shape[1])\n",
    "\n",
    "# Train the final models on the full datasets\n",
    "ori_final_model_entrance.fit(train_features_entrance, train_labels_entrance, epochs=50, batch_size=32, verbose=1)\n",
    "ori_final_model_exit.fit(train_features_exit, train_labels_exit, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Optionally, evaluate on a test set (if you have one)\n",
    "ori_test_loss_entrance = ori_final_model_entrance.evaluate(ori_test_features_entrance, ori_test_labels_entrance, verbose=1)\n",
    "print(f\"Test set loss for entrance: {ori_test_loss_entrance}\")\n",
    "ori_test_loss_exit = ori_final_model_exit.evaluate(ori_test_features_exit, ori_test_labels_exit, verbose=1)\n",
    "print(f\"Test set loss for exit: {ori_test_loss_exit}\")\n",
    "\n",
    "\n",
    "ori_test_features_entrance = np.asarray(ori_test_features_entrance).astype(np.float32)\n",
    "print(\"Test features Entrance:\", ori_test_features_entrance)\n",
    "print()\n",
    "\n",
    "ori_test_results_entrance = {}\n",
    "ori_test_results_entrance['dnn_model'] = ori_final_model_entrance.evaluate(ori_test_features_entrance, ori_test_labels_entrance, verbose=0)\n",
    "print(\"Test result Entrance:\", ori_test_results_entrance)\n",
    "print()\n",
    "\n",
    "ori_test_features_exit = np.asarray(ori_test_features_exit).astype(np.float32)\n",
    "print(\"Test features Exit:\", ori_test_features_exit)\n",
    "print()\n",
    "\n",
    "ori_test_results_exit = {}\n",
    "ori_test_results_exit['dnn_model'] = ori_final_model_exit.evaluate(ori_test_features_exit, ori_test_labels_exit, verbose=0)\n",
    "print(\"Test result Exit:\", ori_test_results_exit)\n",
    "print()\n",
    "\n",
    "pd.DataFrame(ori_test_results_entrance, index=['Mean absolute error [Temperature]']).T\n",
    "\n",
    "pd.DataFrame(ori_test_results_exit, index=['Mean absolute error [Temperature]']).T\n",
    "\n",
    "ori_test_predictions_entrance = ori_final_model_entrance.predict(ori_test_features_entrance).flatten()\n",
    "print(\"Test Predictions Entrance:\", ori_test_predictions_entrance)\n",
    "print()\n",
    "\n",
    "ori_test_predictions_exit = ori_final_model_exit.predict(ori_test_features_exit).flatten()\n",
    "print(\"Test Predictions Exit:\", ori_test_predictions_exit)\n",
    "print()\n",
    "\n",
    "ori_error_entrance = ori_test_predictions_entrance - ori_test_labels_entrance\n",
    "print(\"Error entrance:\", ori_error_entrance)\n",
    "print()\n",
    "\n",
    "ori_error_exit = ori_test_predictions_entrance - ori_test_labels_entrance\n",
    "print(\"Error entrance:\", ori_error_exit)\n",
    "print()\n",
    "\n",
    "plt.hist(ori_error_entrance, bins=25, color='lightblue')\n",
    "plt.xlabel('Prediction Error [Temperature]')\n",
    "_ = plt.ylabel('Count')\n",
    "\n",
    "ori_error_exit = ori_test_predictions_exit - ori_test_labels_exit\n",
    "ori_error_exit\n",
    "\n",
    "plt.hist(ori_error_exit, bins=25, color='lightblue')\n",
    "plt.xlabel('Prediction Error [Temperature]')\n",
    "_ = plt.ylabel('Count')\n",
    "\n",
    "# average error\n",
    "ori_average_error_entrance = np.mean(ori_error_entrance)\n",
    "print(\"Average error for entrance:\", ori_average_error_entrance)\n",
    "print()\n",
    "\n",
    "ori_average_error_exit = np.mean(ori_error_exit)\n",
    "print(\"Average error for exit:\", ori_average_error_exit)\n",
    "print()\n",
    "\n",
    "# mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "ori_mse_entrance = mean_squared_error(ori_test_labels_entrance, ori_test_predictions_entrance)\n",
    "ori_mse_exit = mean_squared_error(ori_test_labels_exit, ori_test_predictions_exit)\n",
    "print(\"Mean Squared Error for entrance:\", ori_mse_entrance)\n",
    "print(\"Mean Squared Error for exit:\", ori_mse_exit)\n",
    "print()\n",
    "\n",
    "# mean absolute error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "ori_mae_entrance = mean_absolute_error(ori_test_labels_entrance, ori_test_predictions_entrance)\n",
    "ori_mae_exit = mean_absolute_error(ori_test_labels_exit, ori_test_predictions_exit)\n",
    "print(\"Mean Absolute Error for entrance:\", ori_mae_entrance)\n",
    "print(\"Mean Absolute Error for exit:\", ori_mae_exit)\n",
    "print()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def mean_absolute_percentage_error(y_actual, y_pred):\n",
    "    y_actual, y_pred = np.array(y_actual), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_actual - y_pred) / y_actual)) * 100\n",
    "\n",
    "ori_mape = mean_absolute_percentage_error(ori_test_labels_entrance, ori_test_predictions_entrance)\n",
    "print(f'MAPE for entrance: {ori_mape:.2f}%')\n",
    "\n",
    "ori_mape = mean_absolute_percentage_error(ori_test_labels_exit, ori_test_predictions_exit)\n",
    "print(f'MAPE for exit: {ori_mape:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "030d53a9-ecd6-4c91-8aa8-dfbcf334216b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial Losses (Objective Values) as a Column:\n",
      "1|35.21218395233154\n",
      "2|0.41268420219421387\n",
      "3|7.1848883628845215\n",
      "4|0.4403950870037079\n",
      "5|0.406866118311882\n",
      "6|39.85478973388672\n",
      "7|0.37131811678409576\n",
      "8|3.825786769390106\n",
      "9|0.41180936992168427\n",
      "10|0.6084505021572113\n",
      "11|0.26812744140625\n",
      "12|0.2620934471487999\n",
      "13|0.22476675361394882\n",
      "14|0.27170631289482117\n",
      "15|0.2520553059875965\n",
      "16|0.24775905534625053\n",
      "17|0.2459508366882801\n",
      "18|1.530828483402729\n",
      "19|1.527335699647665\n",
      "20|0.25528082996606827\n",
      "21|2.4037650898098946\n",
      "22|0.23025309666991234\n",
      "23|0.25297911465168\n",
      "24|0.2811254933476448\n",
      "25|0.20581506937742233\n",
      "26|0.307591550052166\n",
      "27|0.2552456185221672\n",
      "28|0.33648041635751724\n",
      "29|0.5128492191433907\n",
      "30|0.3604346923530102\n",
      "31|0.328628271818161\n",
      "32|0.21795573085546494\n",
      "33|0.29584889486432076\n",
      "34|0.19650382176041603\n",
      "35|0.2683088667690754\n",
      "36|0.16556208208203316\n",
      "37|1.527197178453207\n",
      "38|1.3295589685440063\n",
      "39|0.22876622527837753\n",
      "40|0.6495141983032227\n",
      "41|0.2731059268116951\n",
      "42|0.2664371356368065\n",
      "43|0.2787623256444931\n",
      "44|0.2002953141927719\n",
      "45|0.4061339795589447\n",
      "46|0.2776757925748825\n",
      "47|0.23848029226064682\n",
      "48|0.20021960139274597\n",
      "49|0.33917438983917236\n",
      "50|0.2744418866932392\n",
      "51|1.5266606956720352\n",
      "52|0.22408035025000572\n",
      "53|0.21584459021687508\n",
      "54|0.3278237283229828\n",
      "55|0.23300965130329132\n",
      "56|0.24748185276985168\n",
      "57|0.20157593488693237\n",
      "58|0.274687297642231\n",
      "59|0.3730230927467346\n",
      "60|0.19824137911200523\n",
      "61|0.19919058308005333\n",
      "62|0.3663274236023426\n",
      "63|0.2182997688651085\n",
      "64|0.2905588187277317\n",
      "65|0.20024679973721504\n",
      "66|0.34982649609446526\n",
      "67|0.24007777869701385\n",
      "68|0.19088805839419365\n",
      "69|0.2182762809097767\n",
      "70|0.2474994659423828\n",
      "71|0.20481907948851585\n",
      "72|0.1869850791990757\n",
      "73|0.25098852813243866\n",
      "74|0.1961853764951229\n",
      "75|0.2436147779226303\n",
      "76|32.07254219055176\n",
      "77|0.28973933309316635\n",
      "78|0.19763054698705673\n",
      "79|0.39756572246551514\n",
      "80|12.552001714706421\n",
      "81|0.19217174872756004\n",
      "82|0.26413894072175026\n",
      "83|0.17511086910963058\n",
      "84|0.22373748198151588\n",
      "85|0.1788523644208908\n",
      "86|0.2862778566777706\n",
      "87|0.3063046336174011\n",
      "88|0.2118481695652008\n",
      "89|0.41877366229891777\n",
      "90|0.28980210795998573\n",
      "91|1.2948951721191406\n",
      "92|0.2741427980363369\n",
      "93|0.25982605665922165\n",
      "94|0.2783374972641468\n",
      "95|0.2663700617849827\n",
      "96|0.1830802857875824\n",
      "97|0.22013089805841446\n",
      "98|0.361565500497818\n",
      "99|0.19161754474043846\n",
      "100|1.5290288999676704\n"
     ]
    }
   ],
   "source": [
    "print(\"Trial Losses (Objective Values) as a Column:\")\n",
    "for i, loss in enumerate(trial_losses):\n",
    "    print(f\"{i + 1}|{loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49034c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
